---
title: "How good are the GPT 4.1 models at writing R code?"
date: '2025-04-15'
tags:
  - ai
  - vitals
  - rstats
subtitle: "OpenAI dropped a trio of models yesterday that they claim result from \"focusing closely on real-world developer needs.\" I was curious whether this might mean stronger R coding performance."
image: featured.png
summary: ''
---

```{r setup}
#| include: false
options(ellmer_timeout_s = 1000)
Sys.setenv(INSPECT_LOG_DIR = "blog/2025-04-15-gpt-4-1/logs")
should_eval <- FALSE

task_files <- list.files(here::here("blog/2025-04-15-gpt-4-1/tasks"), full.names = TRUE)
for (file in task_files) {
  load(file)
}

options(width = 70)

knitr::opts_chunk$set(
  fig.width = 6, 
  fig.asp = 0.618, 
  out.width = "100%", 
  message = FALSE, 
  warning = FALSE
)

library(ggplot2)
theme_set(
  theme_bw() +
  theme(
    panel.border = element_blank(),
    panel.background = element_rect(fill = "#f8f8f1", color = NA),
    plot.background = element_rect(fill = "#f8f8f1", color = NA),
    legend.background = element_rect(fill = "#F3F3EE", color = NA)
  )
)

options(
  ggplot2.discrete.colour = c(
    "#ccb118", "#4A7862", "#8B4513", "#B0B0B0", "#000000"
  ),
  ggplot2.discrete.fill = c(
    "#ccb118", "#4A7862", "#8B4513", "#B0B0B0", "#000000"
  )
)

cat <- function(x, width = 0.9 * getOption("width")) {
  lines <- unlist(strsplit(x, "\n"))
  wrapped <- unlist(lapply(lines, strwrap, width = width))
  base::cat(wrapped, sep = "\n")
}

# use this to resize the thumbnail image for the blog post
optimize_image <- function(
	image_path, 
	target_width = 500, 
	target_quality = 85, 
	output_format = "png"
  ) {
  img <- magick::image_read(image_path)
  
  dims <- magick::image_info(img)
  
  if (dims$width > target_width) {
    img <- magick::image_resize(img, paste0(target_width, "x"))
  }
  
  img <- magick::image_write(
	img, image_path, format = output_format, quality = target_quality
  )
  
  invisible(img)
}
```

Yesterday, OpenAI dropped [a new series of models](https://openai.com/index/gpt-4-1/) called GPT 4.1, 4.1 mini, and GPT 4.1 nano. This line from their release post, specifically, caught my eye:

> GPT‑4.1 is a significant step forward in the practical application of AI. By focusing closely on real-world developer needs—ranging from coding to instruction-following and long context understanding—these models unlock new possibilities for building intelligent systems and sophisticated agentic applications.

It's no surprise to me that OpenAI's newest drop tops benchmark after benchmark. That said, when I see news of new models beating out Claude Sonnet by various measures, I usually wait a week before coming to any conclusions; many developers seem to feel that the Claude series of models have some secret sauce, and I'm among them. Seeing this explicit focus on real-world coding and instruction-following piqued my curiosity, so I'm bypassing my usual "wait a week" policy to see what's up.

<a href="https://simonpcouch.github.io/vtials/"><img src="vitals.png" alt="The hex sticker for the vitals package: a teddy bear in blue scrubs happily holding a stethoscope." align="right" height="240"/></a>

As it happens, I've been working on [a new tool called vitals](https://simonpcouch.github.io/vitals/) for large language model evaluation in R. The package is still pretty early on in it's development and is changing rapidly--so much so that its name has changed in the two weeks since I last [wrote about it on this blog](https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/)--but I'll use it here to evaluate these models on an R coding benchmark.

> _tl;dr_:
> 
> * This eval is a good measure for R coding problems, but doesn't aim to measure instruction-following or long context understanding.
> * The GPT 4.1 series of models does seem to improve on GPT-4o for solving R coding problems.
> * Claude Sonnet 3.7 still outperforms GPT-4o and the GPT 4.1 series of models on R coding.
> * The GPT 4.1 nano model seems to pack quite the punch for its price point; I'm curious whether it might be a good fit for a budget [chores](https://simonpcouch.github.io/chores/) and [gander](https://simonpcouch.github.io/gander/) engine.

## Introducing vitals

vitals is an R port of the widely adopted Python framework [Inspect](https://inspect.ai-safety-institute.org.uk/). While the package doesn't integrate with Inspect directly, it allows users to interface with the [Inspect log viewer](https://inspect.ai-safety-institute.org.uk/log-viewer.html) and shares much of its grammar and philosophy.

vitals describes LLM evals in three core components:

1)  **Datasets** contain a set of labelled samples. Datasets are just a tibble with columns `input` and `target`, where `input` is a prompt and `target` is either literal value(s) or grading guidance.
2)  **Solvers** evaluate the `input` in the dataset and produce a final result (hopefully) approximating `target`. In vitals, the simplest solver is just an ellmer chat (e.g. [`ellmer::chat_claude()`](https://ellmer.tidyverse.org/reference/chat_claude.html)) wrapped in `generate()`, i.e. `generate(ellmer::chat_claude())`), which will call the [Chat object's `$chat()` method](https://ellmer.tidyverse.org/reference/Chat.html#method-Chat-chat) and return whatever it returns.
3)  **Scorers** evaluate the final output of solvers. They may use text comparisons, model grading, or other custom schemes to determine how well the solver approximated the `target` based on the `input`.

In this blog post, we'll apply a solver powered by four different models to a dataset of R coding problems. Our baseline will be Claude 3.7 Sonnet, as this is my daily driver for coding assistance and a peer to GPT 4.1 in pricing. Then, I'll also add GPT-4o, as I know this has been the model of choice for many other folks. Finally, I'll include the three new models for the GPT series: 4.1, 4.1 mini, and 4.1 nano.

In ellmer, here's how we define those model connections:

```{r candidates}
library(ellmer)

sonnet_3_7 <- chat_anthropic(model = "claude-3-7-sonnet-latest")

gpt_4o <- chat_openai(model = "gpt-4o")

gpt_4_1 <- chat_openai(model = "gpt-4.1")
gpt_4_1_mini <- chat_openai(model = "gpt-4.1-mini")
gpt_4_1_nano <- chat_openai(model = "gpt-4.1-nano")
```

:::callout-note
If you're interested in how Gemini's newest 2.5 Pro release stacks up on this eval, check out [this post](https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/) from two weeks ago.
:::

Note that I needed to configure a `ANTHROPIC_API_KEY` and `OPENAI_API_KEY` to connect to these models, respectively. These new models are quite cheap compared to Claude 3.7 Sonnet and GPT-4o! Their pricing per million tokens is as follows

```{r pricing}
#| echo: false
tibble::tibble(
  Name = c("Claude 3.7 Sonnet", "GPT-4o", "GPT-4.1", "GPT-4.1 mini", "GPT-4.1 nano"),
  Input = c("$3.00", "$3.75", "$2.00", "$0.40", "$0.10"),
  Output = c("$15.00", "$15.00", "$8.00", "$1.60", "$0.40")
)
```

Altogether, the data underlying this blog post took around $3 USD to generate.

## An R Eval dataset

```{r load-pkgs}
library(vitals)
library(tidyverse)
```

We'll use a dataset that ships with vitals called `are`, or "An R Eval." From the `are` docs:

> An R Eval is a dataset of challenging R coding problems. Each `input` is a question about R code which could be solved on first-read only by human experts and, with a chance to read documentation and run some code, by fluent data scientists. Solutions are in `target` and enable a fluent data scientist to evaluate whether the solution deserves full, partial, or no credit.

```{r explore-dataset}
glimpse(are)
```

At a high level:

-   `title`: A unique identifier for the problem.
-   `input`: The question to be answered.
-   `target`: The solution, often with a description of notable features of a correct solution.
-   `domain`, `task`, and `knowledge` are pieces of metadata describing the kind of R coding challenge.
-   `source`: Where the problem came from, as a URL. Many of these coding problems are adapted "from the wild" and include the kinds of context usually available to those answering questions.

Notably, these coding problems look like a typical chat, so the eval doesn't measure instruction-following / structured output specifically.

For the purposes of actually carrying out the initial evaluation, we're specifically interested in the `input` and `target` columns. Let's print out the first entry in full so you can get a taste of a typical problem in this dataset:

```{r input-1}
cat(are$input[1])
```

Here's the suggested solution:

```{r target-1}
cat(are$target[1])
```

For now, `are` was publicly shared after the knowledge cutoff of each of these models, so the answers to these questions (likely) aren't yet incorporated into the models' weights.

## A baseline model

LLM evaluation with vitals happens in two main steps:

**First**, use `Task$new()` to situate a dataset, solver, and scorer in a `Task`. [Tasks](https://simonpcouch.github.io/vitals/reference/Task.html) are R6 objects that define important methods and data structures for LLM evaluation. Below, I use `generate()` as a solver, currently the only built-in solver supplied by the package. Think of it like Chat objects' `$chat()` method with some bells and whistles—parallel requests, some nice progress functionality, and lots of logging. `generate()` returns a function that has one argument, `solver_chat`, which takes an ellmer Chat; you can set a default Chat by supplying it to `generate()` itself. The scorer, `model_graded_qa()`, uses model grading (or "LLM-as-a-judge") to score the solver's responses. Different models exhibit different behaviors as judges, so I use the same model (Claude Sonnet 3.7) as the judge regardless of which model is being evaluated.

```{r create-task}
are_task <- Task$new(
  dataset = are,
  solver = generate(),
  scorer = model_graded_qa(
    scorer_chat = sonnet_3_7, 
    partial_credit = TRUE
  ),
  epochs = 3,
  name = "An R Eval"
)

are_task
```

**Second**, use `Task$eval()` to evaluate the solver, evaluate the scorer, and then explore a persistent log of the results in an interactive viewer.

```{r eval-sonnet-3-7, eval = should_eval}
are_sonnet_3_7 <- are_task$clone()
are_sonnet_3_7$eval(solver_chat = sonnet_3_7)
```

:::callout-note
You can also run `$eval()` step-by-step, if you want; `$eval()` just calls `$solve()`, `$score()`, `$log()`, and `$view()` in sequence.
:::


```{r save-sonnet-3-7, eval = should_eval}
#| include: false
if (should_eval) {
  save(are_sonnet_3_7, file = "blog/2025-04-15-gpt-4-1/tasks/are_sonnet_3_7.rda")
}
```

After evaluation, the task contains information from the solving and scoring steps. Here's what the model responded to that first question with:

```{r output-1}
cat(are_sonnet_3_7$samples$result[1])
```

Then, since we've supplied `model_graded_qa()` as the scorer, Claude Sonnet 3.7 will be supplied the input question, the grading guidance, and the solver's output, and asked to determine whether the solution is incorrect, partially correct, or correct. Here's Claude's grading transcript for the solver's first answer:

```{r grading-1}
cat(are_sonnet_3_7$samples$scorer_chat[[1]]$last_turn()@text)
```

vitals ships with the Inspect Log Viewer, a small .js app that allows you to interactively explore evaluation logs. Especially the first few times you run an eval, the tool is super helpful for uncovering unexpected behavior in solving and scoring. I've embedded the viewer in this post so you can check out the problems in *An R Eval* and how effectively Claude Sonnet 3.7 handled them:

```{r viewer}
#| echo: false
htmltools::tags$iframe(
  src = "/assets/2025-04-15-gpt-4-1/viewer/index.html",
  width = "100%", 
  height = "600px",
  style = "border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);"
) 
```

I'd encourage you to poke around in this app! You'll certainly see some bugs that I've still yet to work out and some surprising behavior from the scorer, but there's lots to be learned about how these models work from evaluation logs.

## Evaluating the rest

We can evaluate the remaining models by cloning the original task and running `$eval()` with a new solver chat. First, to evaluate the previous GPT (non-thinking) generation, GPT-4o:

```{r eval-gpt-4o, eval = should_eval}
are_gpt_4o <- are_task$clone()
are_gpt_4o$eval(solver_chat = gpt_4o)
save(are_gpt_4o, file = "are_gpt_4o.rda")
```

```{r save-eval-gpt-4o, eval = should_eval}
#| include: false
if (should_eval) {
  save(are_gpt_4o, file = "blog/2025-04-15-gpt-4-1/tasks/are_gpt_4o.rda")
}
```

From here, it's pretty rote. Evaluating each of GPT 4.1, 4.1 mini, and 4.1 nano on this dataset:

```{r eval-gpt-4-1, eval = should_eval}
are_gpt_4_1 <- are_task$clone()
are_gpt_4_1$eval(solver_chat = gpt_4_1)
```
```{r save-eval-gpt-4-1, eval = should_eval}
#| include: false
if (should_eval) {
  save(are_gpt_4_1, file = "blog/2025-04-15-gpt-4-1/tasks/are_gpt_4_1.rda")
}
```

```{r eval-gpt-4-1-mini, eval = should_eval}
are_gpt_4_1_mini <- are_task$clone()
are_gpt_4_1_mini$eval(solver_chat = gpt_4_1_mini)
```
```{r save-eval-gpt-4-1-mini, eval = should_eval}
#| include: false
if (should_eval) {
  save(are_gpt_4_1_mini, file = "blog/2025-04-15-gpt-4-1/tasks/are_gpt_4_1_mini.rda")
}
```

```{r eval-gpt-4-1-nano, eval = should_eval}
are_gpt_4_1_nano <- are_task$clone()
are_gpt_4_1_nano$eval(solver_chat = gpt_4_1_nano)
```
```{r save-eval-gpt-4-1-nano, eval = should_eval}
#| include: false
if (should_eval) {
  save(are_gpt_4_1_nano, file = "blog/2025-04-15-gpt-4-1/tasks/are_gpt_4_1_nano.rda")
}
```


```{r bundle, eval = should_eval}
#| include: false
vitals_bundle(
  "blog/2025-04-15-gpt-4-1/logs", 
  "assets/2025-04-15-gpt-4-1/viewer"
)
```

I've also situated the logs for the above evaluations in the above app—just click the three stacked bars in the top right of the app to check out the logs for the remaining models.

## Analysis

At evaluation time, vitals does a naive accuracy calculation that you can see displayed in the app, but in general is quite restrained in its analysis functionality. Instead, the package aims to get analysts to Happy Data Frame Land as quickly as possible using `vitals_bind()`:

```{r are-eval}
are_eval <- 
  vitals_bind(
    `Claude Sonnet 3.7` = are_sonnet_3_7,
    `GPT-4o` = are_gpt_4o,
    `GPT-4.1` = are_gpt_4_1,
    `GPT-4.1 mini` = are_gpt_4_1_mini,
    `GPT-4.1 nano` = are_gpt_4_1_nano,
  ) %>%
  rename(model = task) %>%
  mutate(
    model = factor(model, levels = c(
      "Claude Sonnet 3.7", 
      "GPT-4o",
      "GPT-4.1", 
      "GPT-4.1 mini",
      "GPT-4.1 nano"
    ))
  )

are_eval
```

In this dataset, each row represents a single time a solver is invoked to answer a question:

-   `model` gives the model used to solve a given question

-   `id` gives the question id

-   `epoch` identifies the run/resample of the given question

-   `scores` shows whether the scoring model (Claude Sonnet 3.7) identified the solver's answer as Incorrect, Partially Correct, or Correct. It's an ordinal factor with `I < P < C`.

-   `metadata` is a list column containing just about all of the information that vitals collects during the evaluation process.

We're interested in which of these three models are right more often. We have 26 unique questions, each resampled across 3 epochs for each of 5 models. For a cursory analysis, we could do the canonical Bar Chart Dodged By Model visualization:

```{r plot-are-eval}
#| fig-alt: "A ggplot2 bar plot showing the counts of correct, partially correct, and incorrect answers from various LLMs on R coding problems. Claude Sonnet 3.7, shown in orange, answer questions correctly much more often than the GPT series models, shown in shades of blue. GPT-4o answers questions correctly less often than any of the GPT 4.1 series models."
are_eval %>%
  mutate(
    score = fct_rev(score),
    score = fct_recode(
      score, 
      "Correct" = "C", "Partially Correct" = "P", "Incorrect" = "I"
    )
  ) %>%
  ggplot(aes(x = score, fill = model)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c(
    "Claude Sonnet 3.7" = "#d6941a", 
    "GPT-4o" = "#0f4c81", 
    "GPT-4.1" = "#4f86c6", 
    "GPT-4.1 mini" = "#6a9ed4",  
    "GPT-4.1 nano" = "#89b9e2" 
  )) +
  labs(
    x = "Score", y = "Count", fill = "Model",
    title = "An R Eval",
    subtitle = "While the newest GPT 4.1 series models tend to solve R coding problems\nmore effectively than GPT-4o, they still seem to lag behind Claude Sonnet 3.7."
  ) +
  theme(plot.subtitle = element_text(face = "italic"))
```

Could the differences we're seeing be attributed to random noise, though? We can use a hierarchical modeling technique called a mixed model to model the probability of each score (i.e., correct, etc.) as a function of the LLM. In this case, observations are not independent; some questions may be harder than others, and we're repeating each question multiple times since we've set `epochs = 3`. A random intercept on the question `id` can help account for this variation. Since `score` is ordinal, we use a cumulative link mixed model rather than the usual suspect `lme4::glmer()`:

```{r are-mod}
library(ordinal)

are_mod <- clmm(score ~ model + (1|id), data = are_eval)
```

```{r summary-are-mod}
summary(are_mod)
```

First, let's take a look at the `Coefficients` table. We have coefficients for each model other than Claude Sonnet 3.7, which is our "reference" model. Negative `Estimates` indicate lower odds of achieving higher rating categories, and the `Pr(>|z|)` values to their right show the p-values associated with those coefficients. We have evidence here that Claude Sonnet 3.7 is the strongest contender on this eval. While these estimates show that GPT 4.1 is closest to Claude Sonnet 3.7's performance, followed by GPT 4.1 mini, 4.1 nano, and then 4o, we haven't tested whether those pairwise differences could be attributed to random noise.

The `Threshold coefficients` describe whether ratings of Incorrect vs. Partially Correct and Partially Correct vs. Correct are meaningfully different from each other. The thresholds establish the baseline "difficulty" of achieving each category on the grading scale; more negative values for a pair of grades indicate that moving between those grades is relatively easy. If we saw that both coefficients here were quite negative, we could conclude that the rating system has a strong tendency toward higher ratings overall. In our case, those ratings seems relatively balanced.

Finally, the substantial `Random effects` value here shows that there's substantial heterogeneity in question difficulty that's being captured by the model. We can visualize these question-level effects:

```{r}
#| echo: false
#| fig-alt: "A ggplot2 plot with 26 unique questions on the y axis and the random intercept estimate associated with each on the x. The estimates seem roughly normally distributed, and the estimates range from around -5 to around 5; most are clustered near 0."
multiple_mod <- clmm(score ~ model + (1|id), data = are_eval, Hess = TRUE)

multiple_intercepts <- 
  are_eval %>% 
  distinct(id) %>% 
  arrange(id) %>% 
  mutate(
    effect = multiple_mod$ranef,
    id = factor(id),
    id = reorder(id, effect)
  )

effect_rng <- max(extendrange(abs(multiple_intercepts$effect)))

difficult <- 
  multiple_intercepts %>% 
  slice_min(effect, n = 1, with_ties = FALSE) %>% 
  inner_join(are_eval, by = "id")

difficult_xtab <- count(difficult, score)
difficult_text <- 
  cli::format_inline("{sum(difficult_xtab$n[difficult_xtab$score == 'C'])} of {difficult_xtab$n}")

p_effect <- 
  multiple_intercepts %>% 
  ggplot(aes(x = effect, y = id)) + 
  geom_point() +
  labs(y = NULL, x = "Random Intercept Estimate\n(incorrect <----------------> correct)") +
  lims(x = c(-effect_rng, effect_rng))

p_effect
```

Each of the rows here is a given question, where smaller random intercept estimates indicate that a question is more difficult. The most challenging sample was "`r difficult$id[1]`" where, across all LLMs and epochs, `r difficult_text` scores were categorized as correct. As this eval's author, this is an indication to me that I should audit these questions and determine if they're answerable at all; it's fine if these are just hard questions, but if there's not enough information in the question to actually answer it, or if the grading guidance is incorrect, this is a bug in the eval dataset rather than a measure of these models' coding ability.

:::callout-note
Keep an eye out for a vitals vignette with a more thorough model-based analysis than this one in the near future.
:::

Altogether:

* The GPT 4.1 series of models does seem to improve on GPT-4o for solving R coding problems.
* Claude Sonnet 3.7 still outperforms GPT-4o and the GPT 4.1 series of models on R coding.
* At least for this sort of problem, the GPT 4.1 nano model seems to pack quite the punch for its price point.

Given this set of releases' apparent focus on instruction-following and the relatively strong performance of the nano model here, I'm now curious if GPT 4.1 nano (or even mini) would make for a good model to underlie the [chores](https://simonpcouch.github.io/chores/) and [gander](https://simonpcouch.github.io/gander/) packages, which require a model that's very good at pattern-matching and instruction-following and don't necessarily rely on extensive coding prowess otherwise. 

-----


_Thank you to Max Kuhn for advising on the model-based analysis here._
