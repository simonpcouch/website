<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Simon P. Couch</title>
<link>https://simonpcouch.com/blog/</link>
<atom:link href="https://simonpcouch.com/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>A data science blog</description>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Thu, 14 Aug 2025 05:00:00 GMT</lastBuildDate>
<item>
  <title>R and the Model Context Protocol</title>
  <link>https://simonpcouch.com/blog/2025-08-14-mcptools/</link>
  <description><![CDATA[ This is a <a href="https://www.tidyverse.org/blog/2025/07/mcptools-0-1-0/">cross-post</a> from tidyverse.org. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-08-14-mcptools/</guid>
  <pubDate>Thu, 14 Aug 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-08-14-mcptools/featured.png" medium="image" type="image/png" height="143" width="144"/>
</item>
<item>
  <title>How I’m using Claude Code to write R code</title>
  <link>https://simonpcouch.com/blog/2025-07-17-claude-code-2/</link>
  <description><![CDATA[ A couple months ago, I <a href="https://www.simonpcouch.com/blog/2025-03-26-claude-code/">wrote a bit</a> about how I was using Claude Code to help me write R code. At the time, I mostly just shared my impressions of working with the tool and some prompting tips. In the month or two after I wrote the post, my usage waned; I was mostly back to using LLMs only for shorter, more narrowly-scoped tasks. A few weeks ago, though, we put together some tooling that has helped me get much more out of the tool and thus made me interested in using it more often again. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-07-17-claude-code-2/</guid>
  <pubDate>Thu, 17 Jul 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-07-17-claude-code-2/featured.png" medium="image" type="image/png" height="146" width="144"/>
</item>
<item>
  <title>Kimi K2 and R Coding</title>
  <link>https://simonpcouch.com/blog/2025-07-14-kimi-k2/</link>
  <description><![CDATA[ It was a hoot and a half of a weekend in the LLM world. A company I hadn’t heard of called Moonshot AI released a model called <a href="https://moonshotai.github.io/Kimi-K2/">Kimi K2</a>. From 30,000 feet: ]]></description>
  <guid>https://simonpcouch.com/blog/2025-07-14-kimi-k2/</guid>
  <pubDate>Mon, 14 Jul 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-07-14-kimi-k2/featured.png" medium="image" type="image/png" height="142" width="144"/>
</item>
<item>
  <title>Claude 4 and R Coding</title>
  <link>https://simonpcouch.com/blog/2025-05-27-claude-4/</link>
  <description><![CDATA[ <a href="https://www.anthropic.com/news/claude-4">Claude 4</a> dropped on Thursday! Given that Claude 3.7 Sonnet is my daily driver LLM for R coding, I’ve been excited to poke at it. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-05-27-claude-4/</guid>
  <pubDate>Tue, 27 May 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-05-27-claude-4/featured.png" medium="image" type="image/png" height="125" width="144"/>
</item>
<item>
  <title>Evaluating Gemini 2.5 Flash on R coding tasks</title>
  <link>https://simonpcouch.com/blog/2025-05-21-gemini-2-5-flash/</link>
  <description><![CDATA[ Google’s preview of their Gemini 2.5 Pro model has <a href="https://www.simonpcouch.com/blog/2025-05-07-gemini-2-5-pro-new/">really made a splash</a>. The model has become many folks’ daily driver, and I’ve started to see “What about Gemini?” in the comments of each of these blog posts if they don’t explicitly call out the model series in the title. Yesterday, Google announced an update of the preview for Gemini 2.5 Flash, a smaller and cheaper version of 2.5 Pro. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-05-21-gemini-2-5-flash/</guid>
  <pubDate>Wed, 21 May 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-05-21-gemini-2-5-flash/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Evaluating the new Gemini 2.5 Pro update on R coding</title>
  <link>https://simonpcouch.com/blog/2025-05-07-gemini-2-5-pro-new/</link>
  <description><![CDATA[ The title line of <a href="https://developers.googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/">Google’s release post</a> on the newest Gemini 2.5 Pro release is “even better coding performance.” Reading this, I was curious whether we’d see a notable increase in performance compared to the last generation on R coding tasks; in <a href="https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/">an earlier post</a>, I saw that the March release of Gemini 2.5 Pro was a contender with Claude 3.7 Sonnet on <em>An R Eval</em>, a dataset of challenging R coding problems. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-05-07-gemini-2-5-pro-new/</guid>
  <pubDate>Wed, 07 May 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-05-07-gemini-2-5-pro-new/featured.png" medium="image" type="image/png" height="140" width="144"/>
</item>
<item>
  <title>Evaluating o3 and o4-mini on R coding performance</title>
  <link>https://simonpcouch.com/blog/2025-04-18-o3-o4-mini/</link>
  <description><![CDATA[ 48 hours after the drop of the GPT 4.1 series of models, a trio of non-reasoning models focused on “real-world developer needs,” OpenAI dropped another set of models, o3 and o4-mini. These two models are the latest generation of thinking models from OpenAI, and they form the backbone of <a href="https://github.com/openai/codex">Codex</a>, a new Claude Code competitor from OpenAI. In short, OpenAI wants market share among developers. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-04-18-o3-o4-mini/</guid>
  <pubDate>Fri, 18 Apr 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-04-18-o3-o4-mini/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>How good are the GPT 4.1 models at writing R code?</title>
  <link>https://simonpcouch.com/blog/2025-04-15-gpt-4-1/</link>
  <description><![CDATA[ Yesterday, OpenAI dropped <a href="https://openai.com/index/gpt-4-1/">a new series of models</a> called GPT 4.1, 4.1 mini, and GPT 4.1 nano. This line from their release post, specifically, caught my eye: ]]></description>
  <guid>https://simonpcouch.com/blog/2025-04-15-gpt-4-1/</guid>
  <pubDate>Tue, 15 Apr 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-04-15-gpt-4-1/featured.png" medium="image" type="image/png" height="142" width="144"/>
</item>
<item>
  <title>Introducing chores</title>
  <link>https://simonpcouch.com/blog/2025-04-11-chores/</link>
  <description><![CDATA[ The following is a cross-post of a post I put together for the Posit Blog; you can read that post <a href="https://posit.co/blog/introducing-chores/">here</a>. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-04-11-chores/</guid>
  <pubDate>Fri, 11 Apr 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-04-11-chores/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>How Good Is Gemini 2.5 Pro at Writing R Code?</title>
  <link>https://simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/</link>
  <description><![CDATA[ Since Gemini 2.5 Pro Experimental’s <a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/">release</a> last week, I’ve been seeing a <a href="https://thezvi.substack.com/p/gemini-25-is-the-new-sota?utm_source=post-email-title&amp;publication_id=573100&amp;post_id=160014258&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=21np6y&amp;triedRedirect=true&amp;utm_medium=email">lot</a> <a href="https://simonwillison.net/2025/Mar/25/gemini/">of</a> <a href="https://www.youtube.com/watch?v=A0V4km88tFc&amp;t=700s">hype</a> claiming that the model is the new state of the art. I’ve been wondering—how good is this model at writing R code? ]]></description>
  <guid>https://simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/</guid>
  <pubDate>Wed, 02 Apr 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/featured.png" medium="image" type="image/png" height="147" width="144"/>
</item>
<item>
  <title>How I’m Using Claude Code to Develop R Packages</title>
  <link>https://simonpcouch.com/blog/2025-03-26-claude-code/</link>
  <description><![CDATA[ Since the release of <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a> a few weeks ago, I’ve been experimenting with using the tool to develop R packages. I’ve come to really appreciate it as part of my LLM toolkit, but it’s definitely taken a bit of getting used to. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-03-26-claude-code/</guid>
  <pubDate>Wed, 26 Mar 2025 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-03-26-claude-code/featured.png" medium="image" type="image/png" height="143" width="144"/>
</item>
<item>
  <title>Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements</title>
  <link>https://simonpcouch.com/blog/2025-01-30-llm-biases/</link>
  <description><![CDATA[ I’ve been spending some time recently learning about LLMs evaluating output from LLMs, or as its referred to in the literature, “LLM-as-a-judge.” That is, after asking a question to an LLM and receiving an answer, both the question and answer are provided to another language model and that model is asked to somehow judge whether the provided response was satisfactory. ]]></description>
  <guid>https://simonpcouch.com/blog/2025-01-30-llm-biases/</guid>
  <pubDate>Thu, 30 Jan 2025 06:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2025-01-30-llm-biases/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Which names that are also names of countries are most common?</title>
  <link>https://simonpcouch.com/blog/2024-12-23-names/</link>
  <description><![CDATA[ Sitting around the living room over the weekend, someone asked “Which names that are also names of countries are most common?” We all gave guesses, and then I gave in to the urge to reach for my laptop and generate an answer that was authoritative enough for our purposes. ]]></description>
  <guid>https://simonpcouch.com/blog/2024-12-23-names/</guid>
  <pubDate>Mon, 23 Dec 2024 06:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2024-12-23-names/featured.png" medium="image" type="image/png" height="134" width="144"/>
</item>
<item>
  <title>Announcing a new book</title>
  <link>https://simonpcouch.com/blog/2024-10-29-book/</link>
  <description><![CDATA[ Over the last couple years, I’ve spent quite a bit of time focused on making tidymodels code run as fast as possible. Throughout, I’ve written about this work a good bit on this blog<sup>1</sup> and the tidyverse blog<sup>2</sup>. Early this year, I had the idea that maybe I ought to compile many of those learnings together in a book, focused on helping tidymodels users reduce the computational time needed to develop machine learning models without sacrificing predictive performance. I wrote portions of a couple chapters over the course of a couple weeks, and then mostly set the book aside for many months. ]]></description>
  <guid>https://simonpcouch.com/blog/2024-10-29-book/</guid>
  <pubDate>Tue, 29 Oct 2024 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2024-10-29-book/featured.png" medium="image" type="image/png" height="140" width="144"/>
</item>
<item>
  <title>Postprocessing is coming to tidymodels</title>
  <link>https://simonpcouch.com/blog/2024-10-16-postprocessing/</link>
  <description><![CDATA[ This is a cross-post of <a href="https://www.tidyverse.org/blog/2024/10/postprocessing-preview/">a post of mine</a> on the tidyverse blog. ]]></description>
  <guid>https://simonpcouch.com/blog/2024-10-16-postprocessing/</guid>
  <pubDate>Wed, 16 Oct 2024 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2024-10-16-postprocessing/featured.png" medium="image" type="image/png" height="130" width="144"/>
</item>
<item>
  <title>A new package for profiling parallel R code</title>
  <link>https://simonpcouch.com/blog/2024-07-15-syrup/</link>
  <description><![CDATA[ I’ve found that the following pattern looms large in content about parallel processing with R (including my own): ]]></description>
  <guid>https://simonpcouch.com/blog/2024-07-15-syrup/</guid>
  <pubDate>Mon, 15 Jul 2024 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2024-07-15-syrup/featured.png" medium="image" type="image/png" height="148" width="144"/>
</item>
<item>
  <title>How to best parallelize boosted tree model fits with tidymodels</title>
  <link>https://simonpcouch.com/blog/2024-05-13-parallel/</link>
  <description><![CDATA[ The <a href="https://xgboost.readthedocs.io/">XGBoost</a> and <a href="https://lightgbm.readthedocs.io/en/stable/">LightGBM</a> modeling engines both enable distributing the computations needed to train a single boosted tree model across several CPU cores. Similarly, the <a href="https://tidymodels.org">tidymodels framework</a> enables distributing model fits across cores. The natural question, then, is whether tidymodels users ought to make use of the engine’s parallelism implementation, tidymodels’ implementation, or both at the same time. This blog post is a scrappy attempt at finding which of those approaches will lead to the smallest elapsed time when fitting many models. ]]></description>
  <guid>https://simonpcouch.com/blog/2024-05-13-parallel/</guid>
  <pubDate>Mon, 13 May 2024 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2024-05-13-parallel/featured.png" medium="image" type="image/png" height="145" width="144"/>
</item>
<item>
  <title>Measuring elapsed time to fit with tidymodels</title>
  <link>https://simonpcouch.com/blog/2024-04-08-fit-time/</link>
  <description><![CDATA[ <strong>tl;dr</strong>: The development versions of tidymodels packages include methods for a <a href="https://workflows.tidymodels.org/dev/reference/extract-workflow.html">new extract function</a>, <code>extract_fit_time()</code>, that returns the time required to train a workflow. Pass <code>extract_fit_time()</code> as a control option while tuning and run <code>collect_extracts()</code> to see training times for resampled workflows. In this example, we can identify a modeling workflow that trains more than 10x faster than the most performant model with very little decrease in predictive performance. ]]></description>
  <guid>https://simonpcouch.com/blog/2024-04-08-fit-time/</guid>
  <pubDate>Mon, 08 Apr 2024 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2024-04-08-fit-time/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Run an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS</title>
  <link>https://simonpcouch.com/blog/2024-03-14-oracle/</link>
  <description><![CDATA[ I recently sunk a few days into getting an Oracle database deployed on MacOS with an M1 chip in a docker container via Docker Desktop. The few solutions that I found recommended using the Docker Desktop alternative Colima and/or using publicly available, community-contributed images; I had various troubles getting these solutions to work, and found myself missing the bells and whistles of Docker Desktop along the way. This morning, I finally got this database deployed by building the image myself from the official Oracle source, in Docker Desktop rather than Colima, and thought it’d be worth writing up how I did so, especially given the countless GitHub issue comments, StackOverflow posts, and forum discussion I came across from others in my situation. ]]></description>
  <guid>https://simonpcouch.com/blog/2024-03-14-oracle/</guid>
  <pubDate>Thu, 14 Mar 2024 05:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2024-03-14-oracle/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Analyzing my own music listening data with R and the tidyverse (2023)</title>
  <link>https://simonpcouch.com/blog/2023-11-30-listening-2023/</link>
  <description><![CDATA[ Aside from exchanging playlists with my partner every once in a while, I’m not much of a Spotify user. Around this time every year, though, all of my friends start posting their Spotify Wrapped, and I get jealous, as the platform that I listen to music on doesn’t have anything like it. Of course, though, it collects data about me (it’s 2023!); <a href="https://www.simonpcouch.com/blog/2022-12-01-listening-2022/">last year</a>, I got to wondering whether I could make a lo-fi knockoff of wrapped using R, the tidyverse, and the data that I have access to. You already know: ]]></description>
  <guid>https://simonpcouch.com/blog/2023-11-30-listening-2023/</guid>
  <pubDate>Thu, 30 Nov 2023 06:00:00 GMT</pubDate>
  <media:content url="https://simonpcouch.com/blog/2023-11-30-listening-2023/featured.png" medium="image" type="image/png" height="156" width="144"/>
</item>
</channel>
</rss>
