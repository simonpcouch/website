<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="dcterms.date" content="2025-04-15">
<title>How good are the GPT 4.1 models at writing R code? | Simon P. Couch – Simon P. Couch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/cabin.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a2d2da6447bc21d3e680c795c75d6b9d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-DDB8R0B1ZW"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-DDB8R0B1ZW', { 'anonymize_ip': true});
</script>
</head>
<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../assets/cabin.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Simon P. Couch</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://www.github.com/simonpcouch/website" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">How good are the GPT 4.1 models at writing R code?</h1>
            <p class="subtitle lead">OpenAI dropped a trio of models yesterday that they claim result from “focusing closely on real-world developer needs.” I was curious whether this might mean stronger R coding performance.</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 15, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><p>Yesterday, OpenAI dropped <a href="https://openai.com/index/gpt-4-1/">a new series of models</a> called GPT 4.1, 4.1 mini, and GPT 4.1 nano. This line from their release post, specifically, caught my eye:</p>
<blockquote class="blockquote">
<p>GPT‑4.1 is a significant step forward in the practical application of AI. By focusing closely on real-world developer needs—ranging from coding to instruction-following and long context understanding—these models unlock new possibilities for building intelligent systems and sophisticated agentic applications.</p>
</blockquote>
<p>It’s no surprise to me that OpenAI’s newest drop tops benchmark after benchmark. That said, when I see news of new models beating out Claude Sonnet by various measures, I usually wait a week before coming to any conclusions; many developers seem to feel that the Claude series of models have some secret sauce, and I’m among them. Seeing this explicit focus on real-world coding and instruction-following piqued my curiosity, so I’m bypassing my usual “wait a week” policy to see what’s up.</p>
<p><a href="https://simonpcouch.github.io/vtials/"><img src="vitals.png" alt="The hex sticker for the vitals package: a teddy bear in blue scrubs happily holding a stethoscope." align="right" height="240"></a></p>
<p>As it happens, I’ve been working on <a href="https://simonpcouch.github.io/vitals/">a new tool called vitals</a> for large language model evaluation in R. The package is still pretty early on in it’s development and is changing rapidly–so much so that its name has changed in the two weeks since I last <a href="https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/">wrote about it on this blog</a>–but I’ll use it here to evaluate these models on an R coding benchmark.</p>
<blockquote class="blockquote">
<p><em>tl;dr</em>:</p>
<ul>
<li>This eval is a good measure for R coding problems, but doesn’t aim to measure instruction-following or long context understanding.</li>
<li>The GPT 4.1 series of models does seem to improve on GPT-4o for solving R coding problems.</li>
<li>Claude Sonnet 3.7 still outperforms GPT-4o and the GPT 4.1 series of models on R coding.</li>
<li>The GPT 4.1 nano model seems to pack quite the punch for its price point; I’m curious whether it might be a good fit for a budget <a href="https://simonpcouch.github.io/chores/">chores</a> and <a href="https://simonpcouch.github.io/gander/">gander</a> engine.</li>
</ul>
</blockquote>
<section id="introducing-vitals" class="level2"><h2 class="anchored" data-anchor-id="introducing-vitals">Introducing vitals</h2>
<p>vitals is an R port of the widely adopted Python framework <a href="https://inspect.ai-safety-institute.org.uk/">Inspect</a>. While the package doesn’t integrate with Inspect directly, it allows users to interface with the <a href="https://inspect.ai-safety-institute.org.uk/log-viewer.html">Inspect log viewer</a> and shares much of its grammar and philosophy.</p>
<p>vitals describes LLM evals in three core components:</p>
<ol type="1">
<li>
<strong>Datasets</strong> contain a set of labelled samples. Datasets are just a tibble with columns <code>input</code> and <code>target</code>, where <code>input</code> is a prompt and <code>target</code> is either literal value(s) or grading guidance.</li>
<li>
<strong>Solvers</strong> evaluate the <code>input</code> in the dataset and produce a final result (hopefully) approximating <code>target</code>. In vitals, the simplest solver is just an ellmer chat (e.g.&nbsp;<a href="https://ellmer.tidyverse.org/reference/chat_claude.html"><code>ellmer::chat_claude()</code></a>) wrapped in <code><a href="https://vitals.tidyverse.org/reference/generate.html">generate()</a></code>, i.e.&nbsp;<code>generate(ellmer::chat_claude())</code>), which will call the <a href="https://ellmer.tidyverse.org/reference/Chat.html#method-Chat-chat">Chat object’s <code>$chat()</code> method</a> and return whatever it returns.</li>
<li>
<strong>Scorers</strong> evaluate the final output of solvers. They may use text comparisons, model grading, or other custom schemes to determine how well the solver approximated the <code>target</code> based on the <code>input</code>.</li>
</ol>
<p>In this blog post, we’ll apply a solver powered by four different models to a dataset of R coding problems. Our baseline will be Claude 3.7 Sonnet, as this is my daily driver for coding assistance and a peer to GPT 4.1 in pricing. Then, I’ll also add GPT-4o, as I know this has been the model of choice for many other folks. Finally, I’ll include the three new models for the GPT series: 4.1, 4.1 mini, and 4.1 nano.</p>
<p>In ellmer, here’s how we define those model connections:</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ellmer.tidyverse.org">ellmer</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">sonnet_3_7</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_anthropic.html">chat_anthropic</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"claude-3-7-sonnet-latest"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">gpt_4o</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html">chat_openai</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">gpt_4_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html">chat_openai</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4.1"</span><span class="op">)</span></span>
<span><span class="va">gpt_4_1_mini</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html">chat_openai</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4.1-mini"</span><span class="op">)</span></span>
<span><span class="va">gpt_4_1_nano</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html">chat_openai</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4.1-nano"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you’re interested in how Gemini’s newest 2.5 Pro release stacks up on this eval, check out <a href="https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/">this post</a> from two weeks ago.</p>
</div>
</div>
<p>Note that I needed to configure a <code>ANTHROPIC_API_KEY</code> and <code>OPENAI_API_KEY</code> to connect to these models, respectively. These new models are quite cheap compared to Claude 3.7 Sonnet and GPT-4o! Their pricing per million tokens is as follows</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 3
  Name              Input Output
  &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; 
1 Claude 3.7 Sonnet $3.00 $15.00
2 GPT-4o            $3.75 $15.00
3 GPT-4.1           $2.00 $8.00 
4 GPT-4.1 mini      $0.40 $1.60 
5 GPT-4.1 nano      $0.10 $0.40 </code></pre>
</div>
</div>
<p>Altogether, the data underlying this blog post took around $3 USD to generate.</p>
</section><section id="an-r-eval-dataset" class="level2"><h2 class="anchored" data-anchor-id="an-r-eval-dataset">An R Eval dataset</h2>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidyverse/vitals">vitals</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll use a dataset that ships with vitals called <code>are</code>, or “An R Eval.” From the <code>are</code> docs:</p>
<blockquote class="blockquote">
<p>An R Eval is a dataset of challenging R coding problems. Each <code>input</code> is a question about R code which could be solved on first-read only by human experts and, with a chance to read documentation and run some code, by fluent data scientists. Solutions are in <code>target</code> and enable a fluent data scientist to evaluate whether the solution deserves full, partial, or no credit.</p>
</blockquote>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">are</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 26
Columns: 7
$ id        &lt;chr&gt; "after-stat-bar-heights", "conditional-grouped-sum…
$ input     &lt;chr&gt; "This bar chart shows the count of different cuts …
$ target    &lt;chr&gt; "Preferably: \n\n```\nggplot(data = diamonds) + \n…
$ domain    &lt;chr&gt; "Data analysis", "Data analysis", "Data analysis",…
$ task      &lt;chr&gt; "New code", "New code", "New code", "Debugging", "…
$ source    &lt;chr&gt; "https://jrnold.github.io/r4ds-exercise-solutions/…
$ knowledge &lt;list&gt; "tidyverse", "tidyverse", "tidyverse", "r-lib", "…</code></pre>
</div>
</div>
<p>At a high level:</p>
<ul>
<li>
<code>title</code>: A unique identifier for the problem.</li>
<li>
<code>input</code>: The question to be answered.</li>
<li>
<code>target</code>: The solution, often with a description of notable features of a correct solution.</li>
<li>
<code>domain</code>, <code>task</code>, and <code>knowledge</code> are pieces of metadata describing the kind of R coding challenge.</li>
<li>
<code>source</code>: Where the problem came from, as a URL. Many of these coding problems are adapted “from the wild” and include the kinds of context usually available to those answering questions.</li>
</ul>
<p>Notably, these coding problems look like a typical chat, so the eval doesn’t measure instruction-following / structured output specifically.</p>
<p>For the purposes of actually carrying out the initial evaluation, we’re specifically interested in the <code>input</code> and <code>target</code> columns. Let’s print out the first entry in full so you can get a taste of a typical problem in this dataset:</p>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">are</span><span class="op">$</span><span class="va">input</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>This bar chart shows the count of different cuts of diamonds,
and each bar is stacked and filled according to clarity:

```
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut, fill = clarity))
```

Could you change this code so that the proportion of diamonds
with a given cut corresponds to the bar height and not the
count? Each bar should still be filled according to clarity.</code></pre>
</div>
</div>
<p>Here’s the suggested solution:</p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">are</span><span class="op">$</span><span class="va">target</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Preferably:

```
ggplot(data = diamonds) +
geom_bar(aes(x = cut, y = after_stat(count) /
sum(after_stat(count)), fill = clarity))
```

The dot-dot notation (`..count..`) was deprecated in ggplot2
3.4.0, but it still works:

```
ggplot(data = diamonds) +
geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill =
clarity))
```

Simply setting `position = "fill" will result in each bar
having a height of 1 and is not correct.</code></pre>
</div>
</div>
<p>For now, <code>are</code> was publicly shared after the knowledge cutoff of each of these models, so the answers to these questions (likely) aren’t yet incorporated into the models’ weights.</p>
</section><section id="a-baseline-model" class="level2"><h2 class="anchored" data-anchor-id="a-baseline-model">A baseline model</h2>
<p>LLM evaluation with vitals happens in two main steps:</p>
<p><strong>First</strong>, use <code>Task$new()</code> to situate a dataset, solver, and scorer in a <code>Task</code>. <a href="https://simonpcouch.github.io/vitals/reference/Task.html">Tasks</a> are R6 objects that define important methods and data structures for LLM evaluation. Below, I use <code><a href="https://vitals.tidyverse.org/reference/generate.html">generate()</a></code> as a solver, currently the only built-in solver supplied by the package. Think of it like Chat objects’ <code>$chat()</code> method with some bells and whistles—parallel requests, some nice progress functionality, and lots of logging. <code><a href="https://vitals.tidyverse.org/reference/generate.html">generate()</a></code> returns a function that has one argument, <code>solver_chat</code>, which takes an ellmer Chat; you can set a default Chat by supplying it to <code><a href="https://vitals.tidyverse.org/reference/generate.html">generate()</a></code> itself. The scorer, <code><a href="https://vitals.tidyverse.org/reference/scorer_model.html">model_graded_qa()</a></code>, uses model grading (or “LLM-as-a-judge”) to score the solver’s responses. Different models exhibit different behaviors as judges, so I use the same model (Claude Sonnet 3.7) as the judge regardless of which model is being evaluated.</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_task</span> <span class="op">&lt;-</span> <span class="va"><a href="https://vitals.tidyverse.org/reference/Task.html">Task</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  dataset <span class="op">=</span> <span class="va">are</span>,</span>
<span>  solver <span class="op">=</span> <span class="fu"><a href="https://vitals.tidyverse.org/reference/generate.html">generate</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  scorer <span class="op">=</span> <span class="fu"><a href="https://vitals.tidyverse.org/reference/scorer_model.html">model_graded_qa</a></span><span class="op">(</span></span>
<span>    scorer_chat <span class="op">=</span> <span class="va">sonnet_3_7</span>, </span>
<span>    partial_credit <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  name <span class="op">=</span> <span class="st">"An R Eval"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_task</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>An evaluation task AnREval.</code></pre>
</div>
</div>
<p><strong>Second</strong>, use <code>Task$eval()</code> to evaluate the solver, evaluate the scorer, and then explore a persistent log of the results in an interactive viewer.</p>
<div class="cell">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_sonnet_3_7</span> <span class="op">&lt;-</span> <span class="va">are_task</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">are_sonnet_3_7</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span>solver_chat <span class="op">=</span> <span class="va">sonnet_3_7</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can also run <code>$eval()</code> step-by-step, if you want; <code>$eval()</code> just calls <code>$solve()</code>, <code>$score()</code>, <code>$log()</code>, and <code>$view()</code> in sequence.</p>
</div>
</div>
<p>After evaluation, the task contains information from the solving and scoring steps. Here’s what the model responded to that first question with:</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">are_sonnet_3_7</span><span class="op">$</span><span class="va">samples</span><span class="op">$</span><span class="va">result</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>To change the bar chart from showing counts to showing
proportions of diamonds with each cut, I'll modify the code to
use proportions instead of raw counts while keeping the
clarity breakdown within each bar.

Here's the modified code:

```r
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut, fill = clarity),
position = "fill") +
labs(y = "Proportion")
```

This change:

1. Adds `position = "fill"` to the `geom_bar()` function,
which normalizes each bar to have the same height (1.0 or
100%) and shows the proportional breakdown of clarity
categories within each cut.

2. Adds `labs(y = "Proportion")` to relabel the y-axis, since
it now represents proportions rather than counts.

Each bar will now have the same height, representing 100% of
diamonds with that cut, and the colored segments within each
bar will show the proportional distribution of clarity
categories for that particular cut.</code></pre>
</div>
</div>
<p>Then, since we’ve supplied <code><a href="https://vitals.tidyverse.org/reference/scorer_model.html">model_graded_qa()</a></code> as the scorer, Claude Sonnet 3.7 will be supplied the input question, the grading guidance, and the solver’s output, and asked to determine whether the solution is incorrect, partially correct, or correct. Here’s Claude’s grading transcript for the solver’s first answer:</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">are_sonnet_3_7</span><span class="op">$</span><span class="va">samples</span><span class="op">$</span><span class="va">scorer_chat</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="fu">last_turn</span><span class="op">(</span><span class="op">)</span><span class="op">@</span><span class="va">text</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>I need to analyze whether the submitted answer correctly
addresses the task according to the criterion.

The submission suggests using `position = "fill"` in
`geom_bar()`, which normalizes each bar to have the same
height (1.0) and shows the proportional distribution of
clarity categories within each cut.

However, according to the criterion, this approach is
explicitly stated as incorrect. The criterion specifies that
we need to use either:
1. `after_stat(count) / sum(after_stat(count))` in newer
versions of ggplot2, or
2. `..count.. / sum(..count..)` in older versions of ggplot2

The reason this is different from `position = "fill"` is that
`position = "fill"` normalizes each cut category individually,
showing the proportion of different clarity values within each
cut. In contrast, the requested solution shows the proportion
of each cut relative to the total number of diamonds, while
still maintaining the clarity breakdown within each bar.

The submitted solution does not match what was specifically
requested. The criterion explicitly states that using
`position = "fill"` is not correct for this task.

GRADE: I</code></pre>
</div>
</div>
<p>vitals ships with the Inspect Log Viewer, a small .js app that allows you to interactively explore evaluation logs. Especially the first few times you run an eval, the tool is super helpful for uncovering unexpected behavior in solving and scoring. I’ve embedded the viewer in this post so you can check out the problems in <em>An R Eval</em> and how effectively Claude Sonnet 3.7 handled them:</p>
<div class="cell">
<div class="cell-output-display">
<iframe src="../../assets/2025-04-15-gpt-4-1/viewer/index.html" width="100%" height="600px" style="border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);"></iframe>
</div>
</div>
<p>I’d encourage you to poke around in this app! You’ll certainly see some bugs that I’ve still yet to work out and some surprising behavior from the scorer, but there’s lots to be learned about how these models work from evaluation logs.</p>
</section><section id="evaluating-the-rest" class="level2"><h2 class="anchored" data-anchor-id="evaluating-the-rest">Evaluating the rest</h2>
<p>We can evaluate the remaining models by cloning the original task and running <code>$eval()</code> with a new solver chat. First, to evaluate the previous GPT (non-thinking) generation, GPT-4o:</p>
<div class="cell">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_gpt_4o</span> <span class="op">&lt;-</span> <span class="va">are_task</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">are_gpt_4o</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span>solver_chat <span class="op">=</span> <span class="va">gpt_4o</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/save.html">save</a></span><span class="op">(</span><span class="va">are_gpt_4o</span>, file <span class="op">=</span> <span class="st">"are_gpt_4o.rda"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>From here, it’s pretty rote. Evaluating each of GPT 4.1, 4.1 mini, and 4.1 nano on this dataset:</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_gpt_4_1</span> <span class="op">&lt;-</span> <span class="va">are_task</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">are_gpt_4_1</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span>solver_chat <span class="op">=</span> <span class="va">gpt_4_1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_gpt_4_1_mini</span> <span class="op">&lt;-</span> <span class="va">are_task</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">are_gpt_4_1_mini</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span>solver_chat <span class="op">=</span> <span class="va">gpt_4_1_mini</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_gpt_4_1_nano</span> <span class="op">&lt;-</span> <span class="va">are_task</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">are_gpt_4_1_nano</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span>solver_chat <span class="op">=</span> <span class="va">gpt_4_1_nano</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I’ve also situated the logs for the above evaluations in the above app—just click the three stacked bars in the top right of the app to check out the logs for the remaining models.</p>
</section><section id="analysis" class="level2"><h2 class="anchored" data-anchor-id="analysis">Analysis</h2>
<p>At evaluation time, vitals does a naive accuracy calculation that you can see displayed in the app, but in general is quite restrained in its analysis functionality. Instead, the package aims to get analysts to Happy Data Frame Land as quickly as possible using <code><a href="https://vitals.tidyverse.org/reference/vitals_bind.html">vitals_bind()</a></code>:</p>
<div class="cell">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_eval</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu"><a href="https://vitals.tidyverse.org/reference/vitals_bind.html">vitals_bind</a></span><span class="op">(</span></span>
<span>    `Claude Sonnet 3.7` <span class="op">=</span> <span class="va">are_sonnet_3_7</span>,</span>
<span>    `GPT-4o` <span class="op">=</span> <span class="va">are_gpt_4o</span>,</span>
<span>    `GPT-4.1` <span class="op">=</span> <span class="va">are_gpt_4_1</span>,</span>
<span>    `GPT-4.1 mini` <span class="op">=</span> <span class="va">are_gpt_4_1_mini</span>,</span>
<span>    `GPT-4.1 nano` <span class="op">=</span> <span class="va">are_gpt_4_1_nano</span>,</span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">task</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>    model <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">model</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>      <span class="st">"Claude Sonnet 3.7"</span>, </span>
<span>      <span class="st">"GPT-4o"</span>,</span>
<span>      <span class="st">"GPT-4.1"</span>, </span>
<span>      <span class="st">"GPT-4.1 mini"</span>,</span>
<span>      <span class="st">"GPT-4.1 nano"</span></span>
<span>    <span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">are_eval</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 390 × 5
   model             id                          epoch score metadata
   &lt;fct&gt;             &lt;chr&gt;                       &lt;int&gt; &lt;ord&gt; &lt;list&gt;  
 1 Claude Sonnet 3.7 after-stat-bar-heights          1 I     &lt;tibble&gt;
 2 Claude Sonnet 3.7 after-stat-bar-heights          2 I     &lt;tibble&gt;
 3 Claude Sonnet 3.7 after-stat-bar-heights          3 I     &lt;tibble&gt;
 4 Claude Sonnet 3.7 conditional-grouped-summary     1 P     &lt;tibble&gt;
 5 Claude Sonnet 3.7 conditional-grouped-summary     2 P     &lt;tibble&gt;
 6 Claude Sonnet 3.7 conditional-grouped-summary     3 C     &lt;tibble&gt;
 7 Claude Sonnet 3.7 correlated-delays-reasoning     1 P     &lt;tibble&gt;
 8 Claude Sonnet 3.7 correlated-delays-reasoning     2 C     &lt;tibble&gt;
 9 Claude Sonnet 3.7 correlated-delays-reasoning     3 P     &lt;tibble&gt;
10 Claude Sonnet 3.7 curl-http-get                   1 I     &lt;tibble&gt;
# ℹ 380 more rows</code></pre>
</div>
</div>
<p>In this dataset, each row represents a single time a solver is invoked to answer a question:</p>
<ul>
<li><p><code>model</code> gives the model used to solve a given question</p></li>
<li><p><code>id</code> gives the question id</p></li>
<li><p><code>epoch</code> identifies the run/resample of the given question</p></li>
<li><p><code>scores</code> shows whether the scoring model (Claude Sonnet 3.7) identified the solver’s answer as Incorrect, Partially Correct, or Correct. It’s an ordinal factor with <code>I &lt; P &lt; C</code>.</p></li>
<li><p><code>metadata</code> is a list column containing just about all of the information that vitals collects during the evaluation process.</p></li>
</ul>
<p>We’re interested in which of these three models are right more often. We have 26 unique questions, each resampled across 3 epochs for each of 5 models. For a cursory analysis, we could do the canonical Bar Chart Dodged By Model visualization:</p>
<div class="cell">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">are_eval</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>    score <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_rev.html">fct_rev</a></span><span class="op">(</span><span class="va">score</span><span class="op">)</span>,</span>
<span>    score <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_recode.html">fct_recode</a></span><span class="op">(</span></span>
<span>      <span class="va">score</span>, </span>
<span>      <span class="st">"Correct"</span> <span class="op">=</span> <span class="st">"C"</span>, <span class="st">"Partially Correct"</span> <span class="op">=</span> <span class="st">"P"</span>, <span class="st">"Incorrect"</span> <span class="op">=</span> <span class="st">"I"</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">score</span>, fill <span class="op">=</span> <span class="va">model</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html">geom_bar</a></span><span class="op">(</span>position <span class="op">=</span> <span class="st">"dodge"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_fill_manual</a></span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"Claude Sonnet 3.7"</span> <span class="op">=</span> <span class="st">"#d6941a"</span>, </span>
<span>    <span class="st">"GPT-4o"</span> <span class="op">=</span> <span class="st">"#0f4c81"</span>, </span>
<span>    <span class="st">"GPT-4.1"</span> <span class="op">=</span> <span class="st">"#4f86c6"</span>, </span>
<span>    <span class="st">"GPT-4.1 mini"</span> <span class="op">=</span> <span class="st">"#6a9ed4"</span>,  </span>
<span>    <span class="st">"GPT-4.1 nano"</span> <span class="op">=</span> <span class="st">"#89b9e2"</span> </span>
<span>  <span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"Score"</span>, y <span class="op">=</span> <span class="st">"Count"</span>, fill <span class="op">=</span> <span class="st">"Model"</span>,</span>
<span>    title <span class="op">=</span> <span class="st">"An R Eval"</span>,</span>
<span>    subtitle <span class="op">=</span> <span class="st">"While the newest GPT 4.1 series models tend to solve R coding problems\nmore effectively than GPT-4o, they still seem to lag behind Claude Sonnet 3.7."</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>plot.subtitle <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>face <span class="op">=</span> <span class="st">"italic"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="index_files/figure-html/plot-are-eval-1.png" class="img-fluid figure-img" style="width:100.0%" alt="A ggplot2 bar plot showing the counts of correct, partially correct, and incorrect answers from various LLMs on R coding problems. Claude Sonnet 3.7, shown in orange, answer questions correctly much more often than the GPT series models, shown in shades of blue. GPT-4o answers questions correctly less often than any of the GPT 4.1 series models."></p>
</figure>
</div>
</div>
</div>
<p>Could the differences we’re seeing be attributed to random noise, though? We can use a hierarchical modeling technique called a mixed model to model the probability of each score (i.e., correct, etc.) as a function of the LLM. In this case, observations are not independent; some questions may be harder than others, and we’re repeating each question multiple times since we’ve set <code>epochs = 3</code>. A random intercept on the question <code>id</code> can help account for this variation. Since <code>score</code> is ordinal, we use a cumulative link mixed model rather than the usual suspect <code><a href="https://rdrr.io/pkg/lme4/man/glmer.html">lme4::glmer()</a></code>:</p>
<div class="cell">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/runehaubo/ordinal">ordinal</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ordinal/man/clmm.html">clmm</a></span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">model</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span><span class="op">|</span><span class="va">id</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">are_eval</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">are_mod</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cumulative Link Mixed Model fitted with the Laplace approximation

formula: score ~ model + (1 | id)
data:    are_eval

 link  threshold nobs logLik  AIC    niter     max.grad cond.H 
 logit flexible  390  -268.62 551.24 272(1363) 1.69e-05 5.8e+01

Random effects:
 Groups Name        Variance Std.Dev.
 id     (Intercept) 13       3.605   
Number of groups:  id 26 

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
modelGPT-4o        -2.3531     0.4359  -5.398 6.72e-08 ***
modelGPT-4.1       -1.2544     0.4116  -3.048 0.002307 ** 
modelGPT-4.1 mini  -1.6051     0.4135  -3.882 0.000104 ***
modelGPT-4.1 nano  -1.6902     0.4195  -4.029 5.60e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Threshold coefficients:
    Estimate Std. Error z value
I|P  -2.3073     0.7963  -2.897
P|C   0.1779     0.7782   0.229</code></pre>
</div>
</div>
<p>First, let’s take a look at the <code>Coefficients</code> table. We have coefficients for each model other than Claude Sonnet 3.7, which is our “reference” model. Negative <code>Estimates</code> indicate lower odds of achieving higher rating categories, and the <code>Pr(&gt;|z|)</code> values to their right show the p-values associated with those coefficients. We have evidence here that Claude Sonnet 3.7 is the strongest contender on this eval. While these estimates show that GPT 4.1 is closest to Claude Sonnet 3.7’s performance, followed by GPT 4.1 mini, 4.1 nano, and then 4o, we haven’t tested whether those pairwise differences could be attributed to random noise.</p>
<p>The <code>Threshold coefficients</code> describe whether ratings of Incorrect vs.&nbsp;Partially Correct and Partially Correct vs.&nbsp;Correct are meaningfully different from each other. The thresholds establish the baseline “difficulty” of achieving each category on the grading scale; more negative values for a pair of grades indicate that moving between those grades is relatively easy. If we saw that both coefficients here were quite negative, we could conclude that the rating system has a strong tendency toward higher ratings overall. In our case, those ratings seems relatively balanced.</p>
<p>Finally, the substantial <code>Random effects</code> value here shows that there’s substantial heterogeneity in question difficulty that’s being captured by the model. We can visualize these question-level effects:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="index_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" style="width:100.0%" alt="A ggplot2 plot with 26 unique questions on the y axis and the random intercept estimate associated with each on the x. The estimates seem roughly normally distributed, and the estimates range from around -5 to around 5; most are clustered near 0."></p>
</figure>
</div>
</div>
</div>
<p>Each of the rows here is a given question, where smaller random intercept estimates indicate that a question is more difficult. The most challenging sample was “after-stat-bar-heights” where, across all LLMs and epochs, 0 of 15 scores were categorized as correct. As this eval’s author, this is an indication to me that I should audit these questions and determine if they’re answerable at all; it’s fine if these are just hard questions, but if there’s not enough information in the question to actually answer it, or if the grading guidance is incorrect, this is a bug in the eval dataset rather than a measure of these models’ coding ability.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Keep an eye out for a vitals vignette with a more thorough model-based analysis than this one in the near future.</p>
</div>
</div>
<p>Altogether:</p>
<ul>
<li>The GPT 4.1 series of models does seem to improve on GPT-4o for solving R coding problems.</li>
<li>Claude Sonnet 3.7 still outperforms GPT-4o and the GPT 4.1 series of models on R coding.</li>
<li>At least for this sort of problem, the GPT 4.1 nano model seems to pack quite the punch for its price point.</li>
</ul>
<p>Given this set of releases’ apparent focus on instruction-following and the relatively strong performance of the nano model here, I’m now curious if GPT 4.1 nano (or even mini) would make for a good model to underlie the <a href="https://simonpcouch.github.io/chores/">chores</a> and <a href="https://simonpcouch.github.io/gander/">gander</a> packages, which require a model that’s very good at pattern-matching and instruction-following and don’t necessarily rely on extensive coding prowess otherwise.</p>
<hr>
<p><em>Thank you to Max Kuhn for advising on the model-based analysis here.</em></p>


</section><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section></div></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/simonpcouch\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://utteranc.es/client.js" repo="simonpcouch/website" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Simon P. Couch ∙ Made with <a href="https://quarto.org">Quarto</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


</body></html>