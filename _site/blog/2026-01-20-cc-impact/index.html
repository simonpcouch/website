<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="dcterms.date" content="2026-01-20">
<title>Electricity use of AI coding agents | Simon P. Couch – Simon P. Couch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/cabin.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c1032820e43c09d93aab91433483bf4f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-DDB8R0B1ZW"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-DDB8R0B1ZW', { 'anonymize_ip': true});
</script>
</head>
<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../assets/cabin.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Simon P. Couch</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://www.github.com/simonpcouch/website" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Electricity use of AI coding agents</h1>
            <p class="subtitle lead">Most of the discourse about the environmental impact of LLM use focuses on a ‘median query.’ What about a Claude Code session?</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 20, 2026</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><p>Throughout 2025, we got better estimates of electricity and water use of AI chatbots. There are all sorts of posts I could cite on this topic, but a favorite is <a href="https://www.sustainabilitybynumbers.com/p/ai-footprint-august-2025">this blog post</a> from Our World in Data’s Hannah Ritchie. On the electricity front:</p>
<p><img src="images/hannah-ritchie-electricity-relative.png" class="img-fluid"></p>
<p>In short, “unless you’re an extreme power user, asking AI questions every day is still a rounding error on your total electricity footprint.”</p>
<p>A similar story applies to water usage. <a href="https://benjamintodd.substack.com/p/the-environment-is-a-terrible-reason">This one from Benjamin Todd</a>:</p>
<blockquote class="blockquote">
<p>The average American <a href="https://hess.copernicus.org/articles/22/3007/2018/">uses 1600 liters of water per day</a>, so even if you make 100 prompts per day, at 2ml per prompt, that’s only 0.01% of your total water consumption. Using a <a href="https://www.epa.gov/watersense/showerheads#:~:text=Specification-,Shower%20With%20Power,no%20more%20than%202.0%20gpm.">shower for one second</a> would use far more.</p>
</blockquote>
<p>Generally, these analyses guide my own thinking about the environmental impacts of my individual usage of LLMs; if I’m interested in reducing my personal carbon footprint, I’m much better off driving a couple miles less a week or avoiding one flight each year. This is indeed the right conclusion for users of chat interfaces like chatgpt.com or claude.ai.</p>
<p>That said, 1 or 10 or 100 median prompts a day is many orders of magnitude off from my own personal use of LLMs; I likely am, in Hannah Ritchie’s words, an “extreme power user.” I work in software and spend much of my workday driving 2 or 3 coding agents, like Claude Code, at a time. Thus, a much more relevant question for me is <strong>how much energy does a typical Claude Code session consume?</strong> (I’m not going to dicuss water use in this post.)</p>
<p>tl;dr, much more:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="index_files/figure-html/summary-plot-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>There are so many considerations and assumptions and pieces of shorthand one must use along the way to answer this sort of question. I’ll do my best to call those out throughout this post, but please do understand this is still just Sunday afternoon napkin math from Some Guy.</p>
<section id="coding-agents-and-the-median-query" class="level2"><h2 class="anchored" data-anchor-id="coding-agents-and-the-median-query">Coding agents and the ‘median query’</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>tl;dr: In this section, I point out that a Claude Code session should use orders of magnitude more energy than a ‘median query.’</p>
</div>
</div>
<p>The <a href="https://arxiv.org/pdf/2508.15734">Gemini paper</a>, <a href="https://blog.samaltman.com/the-gentle-singularity">Sam Altman blog post</a>, and <a href="https://epoch.ai/gradient-updates/how-much-energy-does-chatgpt-use">Epoch AI analysis</a> of ChatGPT 4o that Hannah Ritchie cites all refer to something like a “median query” or “typical prompt.” Those various sources all ballpark the electricity usage of such a prompt around 0.3 Wh—0.24, 0.34, and 0.3, respectively. What is a median query?</p>
<p>From the Gemini team:</p>
<blockquote class="blockquote">
<p>To calculate the energy consumption for the median Gemini Apps text prompt on a given day, we first determine the average energy/prompt for each model, and then rank these models by their energy/prompt values. We then construct a cumulative distribution of text prompts along this energy-ranked list to identify the model that serves the 50-th percentile prompt.</p>
</blockquote>
<p>So, they took the 50th percentile to get the median. Yup. From Sam Altman, no details at all.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>What does the Gemini team’s paper and Sam Altman blog post–the two sources (hypothetically) with access to the real data–have in common? Very little detail. “Median” is doing a <em>lot</em> of work here. How many input tokens? Are we counting the system prompt? The core system prompt? How many output tokens? Which model? Does this include systems like Web Search or Deep Research? Is this Web-only, API-only, both? To give a number like 0.24 Wh or 0.34 Wh and no other details on the factors that lead to that number feels like a deliberate attempt to “have given an answer” while answering as little as possible of the actual question.</p>
<p>I’m ranting.</p>
<p>Anyways, here’s an example impression of a “median query”:</p>
<blockquote class="blockquote">
<p>USER: Tell me a joke about statisticians</p>
<p>ASSISTANT: A statistician is someone who, when their head is in the oven and their feet are in the freezer, says “on average, I feel fine.”</p>
</blockquote>
<p>The user types something, the model responds. Presumably, the system prompt under the hood, on the order of thousands of tokens, is cached and is included in this “median query.”</p>
<p>How is Claude Code different from this?</p>
<p>First, there’s the system prompt and descriptions of the tools that are available to Claude Code. In response to <code>/context</code> in Claude Code just now, this is what I see:</p>
<blockquote class="blockquote">
<p>⛀ ⛁ ⛁ ⛁ ⛁ ⛁ System prompt:<br>
3.1k tokens (1.6%)</p>
<p>⛁ ⛁ ⛁ ⛁ ⛶ ⛁ System tools:<br>
16.4k tokens (8.2%)</p>
</blockquote>
<p>So, before the user even types a query, our context length is almost 20,000 tokens. These are presumably also cached/prefilled on Anthropic’s end.</p>
<p>Then, there’s also user-defined tools and prompts. I have 5 tools registered via the Model Context Protocol, in addition to ~200 tokens in my CLAUDE.md. Those 5 tools’ tool descriptions also consume a couple thousand tokens:</p>
<blockquote class="blockquote">
<p>⛶ ⛶ ⛶ ⛶ ⛶ ⛁ MCP tools: 2.6k tokens (1.3%)</p>
</blockquote>
<p>Okay, you get it–with Claude Code, by the time we’re kicking off the first request, we’re looking at a relatively long query.</p>
<p>Now, how does Claude Code do stuff? It <a href="https://ellmer.tidyverse.org/articles/tool-calling.html">calls tools</a>. For example, if I say “Familiarize yourself, please!”, that message gets sent to Anthropic’s API, along with the system prompt and all of the tool descriptions, and then the model will say “Yes okay I’ll do that” and call a couple tools to gather information:</p>
<blockquote class="blockquote">
<p>❯ Familiarize yourself, please!</p>
<p>⏺ I’ll take a look at the project structure to get oriented.</p>
<p>⏺ Bash(ls -la /Users/simoncouch/Documents/rrr/website)</p>
<p>⏺ Search(pattern: “**/*.qmd”)</p>
</blockquote>
<p>In reply, my laptop will automatically process the requests and send this message back on my behalf with the tool call results:</p>
<blockquote class="blockquote">
<p>⏺ Bash(ls -la /Users/simoncouch/Documents/rrr/website)<br>
⎿ total 240<br>
drwx——@ 3 simoncouch staff 96 Nov 11 13:59<br>
_extensions<br>
… +60 lines (ctrl+o to expand)</p>
<p>⏺ Search(pattern: “**/*.qmd”)<br>
⎿ Found 35 files (ctrl+o to expand)</p>
</blockquote>
<p>The model will then see those results (in addition to my first message and the system prompt and the tool descriptions) and then kick off another couple tool requests. Those requests will return results, which is another user message (and all of the conversation history), and so on, until the model believes it has seen enough of the repository and gives me a summary of what it found. So, <strong>in sending one message, I’ve actually kicked off a series of 5 or 10 very large queries</strong>.</p>
<p>Whenever I read ‘median query’, I think about this. In my sessions with Claude Code, I’m sending maybe a dozen messages, and each of those messages are accompanied by maybe 5 tool calls (which are also queries that require just as much compute as a query that I type in myself), and all of those messages are super long. It seems that <strong>a Claude Code session should be orders of magnitude more compute-intensive than a ‘median query.’</strong></p>
<p>To try and estimate the energy cost of a Claude Code session, I need to somehow scale the watt-hour figure for the median query to the scale of a Claude Code session’s hundreds of longer-than-median queries. I can attempt this by first scaling <em>back</em>, estimating the energy consumption per-token for the various kinds of tokens involved. Then, I’ll analyze my real Claude Code session data (which has the real token usage for each type of tokens) and scale those per-token figures up to coding agent scale.</p>
</section><section id="estimating-electricity-per-token" class="level2"><h2 class="anchored" data-anchor-id="estimating-electricity-per-token">Estimating electricity per token</h2>
<p>From Claude Code, I have a bunch of data on millions of input, cached input, and output tokens. I want to “scale” those median query estimates to how many tokens I actually consume using the tool. To do so, we’ll use <a href="https://epoch.ai/gradient-updates/how-much-energy-does-chatgpt-use">a blog post from Epoch AI</a> and <a href="https://platform.claude.com/docs/en/about-claude/pricing#model-pricing">Anthropic’s API pricing data</a> to put together a guess.</p>
<p>Here’s the graph:</p>
<p><img src="images/epoch-ai-relative-electricity-usage.png" class="img-fluid"></p>
<p><br></p>
<p>Here’s Anthropic’s pricing data, priced per million tokens (MTok):</p>
<table class="caption-top table">
<thead><tr class="header">
<th>Model</th>
<th>Base Input</th>
<th>Cache Reads</th>
<th>Output</th>
<th>Output/Input Ratio</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Opus 4.5</td>
<td>$5/MTok</td>
<td>$0.50/MTok</td>
<td>$25/MTok</td>
<td>5:1</td>
</tr>
<tr class="even">
<td>Sonnet 4.5</td>
<td>$3/MTok</td>
<td>$0.30/MTok</td>
<td>$15/MTok</td>
<td>5:1</td>
</tr>
<tr class="odd">
<td>Haiku 4.5</td>
<td>$1/MTok</td>
<td>$0.10/MTok</td>
<td>$5/MTok</td>
<td>5:1</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Let’s focus on the medium context length case from Epoch AI, at 7,500 input words, which is something like half the full Claude Code system prompt and tool descriptions. They estimate that a long query to ChatGPT 4o (~7,500 words, or ~10,000 tokens at .75 words per token) with typical output length (400 words, or ~530 tokens) consumes 2.5 Wh. So, blending input and output tokens, that’s 10,530 tokens, or 0.01053 MTok. 2.5 Wh / 0.01053 MTok is ~240 Wh/MTok, blended.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>These estimates are for GPT-4o. In the last month, most all of my Claude Code usage has used Opus 4.5, Anthropic’s largest (and presumably most energy-intensive) model. At launch, <a href="https://community.openai.com/t/announcing-gpt-4o-in-the-api/744700">GPT-4o was priced at</a> $5 MTok input, $15 MTok output. Opus 4.5 is $5 and $25, respectively. Opus is likely a larger model, but also likely served much more efficiently than was possible in May 2024. I’m going to assume they’re in a ballpark of each other in terms of energy consumption per token.</p>
</div>
</div>
<p>Now, we actually want to split that up into Wh/MTok for input tokens and output tokens individually. Anthropic’s API prices output tokens at 5x the cost of input tokens. The assumption I’m going to make here is that energy usage for different kinds of tokens can be reasonably estimated by scaling according to the billing rates of those tokens.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> We have <code>(input_tokens × input_rate) + (output_tokens × output_rate) = total_Wh</code> and <code>output_rate = 5 × input_rate</code>, and can substitute the second into the first to get:</p>
<pre><code>input_rate = 2.5 / (0.01 + 5 × 0.00053)
input_rate = 2.5 / (0.01 + 0.00265)
input_rate = 2.5 / 0.01265
input_rate ≈ 198 Wh/MTok</code></pre>
<p>So, ~200 Wh/MTok for input, ~990 Wh/MTok for output.</p>
<p>Doing that for the short and maximum context cases, we get:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 6%">
<col style="width: 11%">
<col style="width: 27%">
<col style="width: 27%">
</colgroup>
<thead><tr class="header">
<th>Query type</th>
<th>Input tokens</th>
<th>Output tokens</th>
<th>Total Wh</th>
<th>Blended Wh/MTok</th>
<th>Back-solved input Wh/MTok (5× assumption)</th>
<th>Back-solved output Wh/MTok (5× assumption)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Typical short</td>
<td>~130</td>
<td>~530</td>
<td>0.3</td>
<td>~450</td>
<td>~110</td>
<td>~540</td>
</tr>
<tr class="even">
<td>Medium context</td>
<td>~10,000</td>
<td>~530</td>
<td>2.5</td>
<td>~240</td>
<td>~200</td>
<td>~990</td>
</tr>
<tr class="odd">
<td>“Maximum” context</td>
<td>~100,000</td>
<td>~530</td>
<td>40</td>
<td>~400</td>
<td>~390</td>
<td>~1950</td>
</tr>
</tbody>
</table>
<p>Now, note that <strong>it doesn’t make sense for the output-vs-input energy ratio to be constant across input lengths.</strong> Generating output tokens does get more expensive as the context grows (roughly proportional to context length per output token), but processing the prompt grows more steeply (roughly quadratic in context length). That means the “true” output/input Wh-per-token ratio should shrink as inputs get longer. This is our 5x assumption, based on Anthropic’s pricing, at work.</p>
<p>For the purposes of this post, I’ll use the figures from the 100,000 “maximum”–Claude Sonnet and Opus 4.5 both have context windows of 200,000 tokens, and I run up against them regularly–to generate pessimistic estimates. So, ~390 Wh/MTok input, ~1950 Wh/MTok output.</p>
<p>Now, a large portion of the tokens consumed with Claude Code are cache hits and refreshes, which <a href="https://platform.claude.com/docs/en/about-claude/pricing#model-pricing">Anthropic prices at 1/10th the cost of input tokens</a>. So, we’ll ballpark cached inputs at ~39 Wh/MTok. Same napkin math for cache writes; they price cache writes at a 25% premium to input tokens, so ~490 Wh/MTok. I have no idea if these are reasonable guesses.</p>
</section><section id="my-claude-code-data" class="level2"><h2 class="anchored" data-anchor-id="my-claude-code-data">My Claude Code data</h2>
<p>Claude Code stores session logs in <code>~/.claude/projects/</code>, with each session saved as a JSONL file containing usage data for every API call. Each log entry includes <code>input_tokens</code>, <code>output_tokens</code>, <code>cache_creation_input_tokens</code>, <code>cache_read_input_tokens</code>, and a <code>requestId</code> identifying the HTTP request. Multiple streaming events are logged per request with identical token counts, so we deduplicate by <code>requestId</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">usage_raw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span><span class="st">"sessions/calls.csv"</span>, show_col_types <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">usage</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">usage_raw</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/distinct.html">distinct</a></span><span class="op">(</span><span class="va">request_id</span>, .keep_all <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_n</a></span><span class="op">(</span><span class="va">usage</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">usage</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 8,825
Columns: 10
$ project               &lt;chr&gt; "-Users-simoncouch-Documents-rrr-hypot…
$ session_id            &lt;chr&gt; "f50ac864-06c2-4d94-907b-f5bd447f9312"…
$ request_id            &lt;chr&gt; "req_011CX5WSCwYKkpgtw15qcjW5", "req_0…
$ timestamp             &lt;dttm&gt; 2026-01-13 14:08:09, 2026-01-09 19:06…
$ model                 &lt;chr&gt; "claude-opus-4-5-20251101", "claude-op…
$ input_tokens          &lt;dbl&gt; 1, 8, 1, 1, 1979, 8, 1, 8, 1, 8, 1, 1,…
$ output_tokens         &lt;dbl&gt; 89, 1, 24, 24, 2, 3, 90, 3, 84, 1, 88,…
$ cache_creation_tokens &lt;dbl&gt; 147, 1022, 148, 146, 0, 368, 148, 3586…
$ cache_read_tokens     &lt;dbl&gt; 36851, 104199, 173604, 33251, 0, 72682…
$ cost_usd              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…</code></pre>
</div>
</div>
<p>Plotting the distributions of each token type per session:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="index_files/figure-html/token-distributions-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>The median session had 24 requests and consumed 592,439 total tokens—5 user messages and 19 tool call responses. Applying our napkin math energy estimates:</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">wh_per_mtok</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  input <span class="op">=</span> <span class="fl">390</span>,</span>
<span>  output <span class="op">=</span> <span class="fl">1950</span>,</span>
<span>  cache_creation <span class="op">=</span> <span class="fl">490</span>,</span>
<span>  cache_read <span class="op">=</span> <span class="fl">39</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">median_energy_wh</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">median_session</span><span class="op">$</span><span class="va">input_tokens</span> <span class="op">*</span> <span class="va">wh_per_mtok</span><span class="op">[[</span><span class="st">"input"</span><span class="op">]</span><span class="op">]</span> <span class="op">/</span> <span class="fl">1e6</span> <span class="op">+</span></span>
<span>  <span class="va">median_session</span><span class="op">$</span><span class="va">output_tokens</span> <span class="op">*</span> <span class="va">wh_per_mtok</span><span class="op">[[</span><span class="st">"output"</span><span class="op">]</span><span class="op">]</span> <span class="op">/</span> <span class="fl">1e6</span> <span class="op">+</span></span>
<span>  <span class="va">median_session</span><span class="op">$</span><span class="va">cache_creation_tokens</span> <span class="op">*</span> <span class="va">wh_per_mtok</span><span class="op">[[</span><span class="st">"cache_creation"</span><span class="op">]</span><span class="op">]</span> <span class="op">/</span> <span class="fl">1e6</span> <span class="op">+</span></span>
<span>  <span class="va">median_session</span><span class="op">$</span><span class="va">cache_read_tokens</span> <span class="op">*</span> <span class="va">wh_per_mtok</span><span class="op">[[</span><span class="st">"cache_read"</span><span class="op">]</span><span class="op">]</span> <span class="op">/</span> <span class="fl">1e6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>My estimate is that my median Claude Code session consumes 41 Wh, 138x more than the “typical query.”</p>
<p>In the course of a typical day at work, I’m usually programming for a few hours, and driving 2 or 3 Claude Code instances at a time throughout those hours. What’s the distribution of my energy usage daily?</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="index_files/figure-html/daily-energy-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>On a median day, I estimate I consume 1,300 Wh through Claude Code—4,400 “typical queries” worth. (For those wondering, this is $15-20 of token spend on a typical day.)</p>
</section><section id="in-context" class="level2"><h2 class="anchored" data-anchor-id="in-context">In context</h2>
<p>So, compared to a “typical query” of 0.3 Wh, my median Claude Code session is something like 41 Wh, and in a typical day of coding with Claude Code, I consume something like 1,300 Wh. How does this compare to other things I do every day?</p>
<p>Using <a href="https://docs.google.com/spreadsheets/d/1Xe1WXNaZ0IZuuRAJP62xf_ufmwUhgRhtp446bTHnUOM/edit?gid=1214080165#gid=1214080165">the same data from Epoch AI</a>:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="index_files/figure-html/energy-comparisons-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>It’s a bit unfair to show the “per-day” usage from Claude Code without the same time scale for other usages. Here’s a similar plot with a few more “per day” numbers included:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="index_files/figure-html/energy-comparisons-2-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>So, if I wanted to analogize the energy usage of my use of coding agents, it’s something like running the dishwasher an extra time each day, keeping an extra refrigerator, or skipping one drive to the grocery store in favor of biking there. To me, this is <em>very</em> different than, in Benjamin Todd’s words, “a terrible reason to avoid” this level of AI use. These are the sorts of things that would make me think twice.</p>
<p>There are a few very important caveats:</p>
<ol type="1">
<li>This is all napkin math based on estimates from other researchers because the frontier labs don’t release comprehensive data. They ought to.</li>
<li>The mix of energy sources that are used to power that compute largely determines how problematic this is. If that compute is largely driven by fossil fuels, that’s a problem, but if the compute is largely driven by renewables, I don’t mind much.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Hank Green has a <a href="https://www.youtube.com/watch?v=H_c6MWk7PQc&amp;t=1s">great video about this</a>.</li>
<li>
<a href="https://benjamintodd.substack.com/p/the-environment-is-a-terrible-reason">As Benjamin Todd puts it</a>, “Cutting individual emissions is an inefficient way to fight climate change in the first place.” If we want to reduce the climate impact of our LLM usage, we are likely better off e.g.&nbsp;supporting advocacy for the green energy transition.
<ul>
<li>I do have a weird relationship with this point specifically, as I write software that makes it easier for <em>other people</em> to use LLMs and I give talks about it and write blog posts about it and stuff. For most readers of this post, though, this likely doesn’t apply.</li>
</ul>
</li>
</ol>
<p>Personally, I don’t know that this scale of energy (and, ostensibly, water) use is significant enough to make me decrease my use of coding agents.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> That said, it <em>is</em> significant enough to make me donate to organizations that are working to accelerate the green energy transition, especially related to AI, like <a href="https://climateaction.tech/">ClimateAction.tech</a>, <a href="https://www.thegreenwebfoundation.org/">Green Web Foundation</a>, <a href="https://cleangridalliance.org/">Clean Grid Alliance</a>, <a href="https://www.sierraclub.org/">Sierra Club</a>, <a href="https://www.nrdc.org/">Natural Resources Defense Council (NRDC)</a>, <a href="https://earthjustice.org/">Earthjustice</a>, <a href="https://rmi.org/">RMI (Rocky Mountain Institute)</a>, <a href="https://350.org/">350.org</a>, and <a href="https://www.cesa.org/">Clean Energy States Alliance</a>.</p>
</section><section id="appendix-cost-gut-check" class="level2"><h2 class="anchored" data-anchor-id="appendix-cost-gut-check">Appendix: Cost gut-check</h2>
<p>Just to confirm that my parsing of the Claude session logs was reasonable, I decided to also estimate the dollar cost from our token data and compare the results to what <a href="https://ccusage.com/">ccusage</a> reports.</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pricing</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tribble.html">tribble</a></span><span class="op">(</span></span>
<span>  <span class="op">~</span><span class="va">model</span>,                        <span class="op">~</span><span class="va">input</span>, <span class="op">~</span><span class="va">output</span>,</span>
<span>  <span class="st">"claude-opus-4-5-20251101"</span>,    <span class="fl">5</span>,      <span class="fl">25</span>,</span>
<span>  <span class="st">"claude-sonnet-4-5-20250929"</span>,  <span class="fl">3</span>,      <span class="fl">15</span>,</span>
<span>  <span class="st">"claude-haiku-4-5-20251001"</span>,   <span class="fl">1</span>,      <span class="fl">5</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>    cache_creation <span class="op">=</span> <span class="va">input</span> <span class="op">*</span> <span class="fl">1.25</span>,</span>
<span>    cache_read <span class="op">=</span> <span class="va">input</span> <span class="op">*</span> <span class="fl">0.10</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">estimated_cost</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">usage</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="fu"><a href="https://lubridate.tidyverse.org/reference/as_date.html">as_date</a></span><span class="op">(</span><span class="va">timestamp</span><span class="op">)</span> <span class="op">&gt;=</span> <span class="st">"2025-12-19"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html">left_join</a></span><span class="op">(</span><span class="va">pricing</span>, by <span class="op">=</span> <span class="st">"model"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span></span>
<span>    cost_usd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span></span>
<span>      <span class="va">input_tokens</span> <span class="op">*</span> <span class="va">input</span> <span class="op">/</span> <span class="fl">1e6</span> <span class="op">+</span></span>
<span>      <span class="va">output_tokens</span> <span class="op">*</span> <span class="va">output</span> <span class="op">/</span> <span class="fl">1e6</span> <span class="op">+</span></span>
<span>      <span class="va">cache_creation_tokens</span> <span class="op">*</span> <span class="va">cache_creation</span> <span class="op">/</span> <span class="fl">1e6</span> <span class="op">+</span></span>
<span>      <span class="va">cache_read_tokens</span> <span class="op">*</span> <span class="va">cache_read</span> <span class="op">/</span> <span class="fl">1e6</span>,</span>
<span>      na.rm <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">estimated_cost</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 1
  cost_usd
     &lt;dbl&gt;
1     481.</code></pre>
</div>
</div>
<p>I estimate the cost over the last month at $480.64, within a dollar of what ccusage reported. Close enough for me.</p>


</section><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>
<ol>
<li id="fn1"><p>From Epoch AI, they assume 500 output tokens and &lt;100 input tokens and say everything else out loud, too. Thank you, Josh You.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is pretty silly. For one, the “true” ratio of energy consumption for output vs.&nbsp;input tokens does not actually exist, as the former grows linearly while the latter has quadratic growth relative to the context length. Also, the ratios in each company’s pricing are a reflection of business strategy rather than the true ratios. An <a href="https://muxup.com/2026q1/per-query-energy-consumption-of-llms">analysis of data from SemiAnalysis</a> estimated that, at the few-thousand-tokens scale, output tokens are ~15x more compute-intensive than input. This makes me think that, at the tens-of-thousands scale, 5x is reasonable.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I especially don’t care about this energy usage if it is both driven by renewables <em>and</em> by on-premises power rather than drawn exclusively from the local grid, where improvements to infrastructure are much more often funded by local residents than the AI companies.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For many, the size of the bill you’ll get from the Anthropic platform for exceeding a Max plan’s usage is probably a stronger mediator than these figures.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section></div></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/simonpcouch\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><script src="https://utteranc.es/client.js" repo="simonpcouch/website" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Simon P. Couch ∙ Made with <a href="https://quarto.org">Quarto</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


</body></html>