[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Some Pitters and Patters with R",
    "section": "",
    "text": "chores 0.3.0 and local LLMs\n\n\nThere are now small models capable of powering chores helpers.\n\n\n\n\n\nDec 10, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nLocal models are not there (yet)\n\n\nLLMs that can run on your laptop are not yet capable enough to drive coding agents.\n\n\n\n\n\nDec 4, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my music listening data with Databot\n\n\nIt‚Äôs Spotify Wrapped season.\n\n\n\n\n\nDec 3, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhen plotting, LLMs see what they expect to see\n\n\nData science agents need to accurately read plots even when the content contradicts their expectations.\n\n\n\n\n\nNov 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nside::kick(), a coding agent for RStudio\n\n\nOpen-source AI for RStudio, built entirely in R.\n\n\n\n\n\nNov 11, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nI‚Äôm‚Ä¶ writing a newsletter?\n\n\nSara Altman and I have been working on a newsletter focused on AI.\n\n\n\n\n\nOct 8, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nI was wrong about tidymodels and LLMs\n\n\nToday‚Äôs frontier LLMs know much more about tidymodels than I thought.\n\n\n\n\n\nAug 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nR and the Model Context Protocol\n\n\nThe newly released mcptools package makes coding assistants better at writing R code and applications built with ellmer more powerful.\n\n\n\n\n\nAug 14, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow I‚Äôm using Claude Code to write R code\n\n\nGiving Claude Code the ability to peruse R package documentation via MCP is so, so helpful.\n\n\n\n\n\nJul 17, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nKimi K2 and R Coding\n\n\nAn open-weights model released over the weekend by a little-known company has drawn quite a bit of attention. Is it any good?\n\n\n\n\n\nJul 14, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nClaude 4 and R Coding\n\n\nEvaluating the most recent releases of Claude Sonnet and Opus on challenging R coding problems.\n\n\n\n\n\nMay 27, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Gemini 2.5 Flash on R coding tasks\n\n\nGoogle‚Äôs Gemini 2.5 Pro release really made a splash last month. They‚Äôve just announced an update to 2.5 Flash, a faster and cheaper model.\n\n\n\n\n\nMay 21, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the new Gemini 2.5 Pro update on R coding\n\n\nThe initial Gemini 2.5 Pro release a month ago was surprisingly strong, so I was excited to benchmark the update announced yesterday on R coding problems.\n\n\n\n\n\nMay 7, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating o3 and o4-mini on R coding performance\n\n\nWith a pair of coding-focused model drops, it‚Äôs clear OpenAI is aiming for the developer market. How well do the newest o3 and o4-mini perform on R coding tasks?\n\n\n\n\n\nApr 18, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow good are the GPT 4.1 models at writing R code?\n\n\nOpenAI dropped a trio of models yesterday that they claim result from ‚Äúfocusing closely on real-world developer needs.‚Äù I was curious whether this might mean stronger R coding performance.\n\n\n\n\n\nApr 15, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing chores\n\n\nA package to help with repetitive but hard-to-automate data science tasks.\n\n\n\n\n\nApr 11, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow Good Is Gemini 2.5 Pro at Writing R Code?\n\n\nSince Gemini 2.5 Pro‚Äôs release last week, I‚Äôve been seeing a lot of hype claiming that the model is the new state of the art. How well does it know R?\n\n\n\n\n\nApr 2, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow I‚Äôm Using Claude Code to Develop R Packages\n\n\nI‚Äôve come to really appreciate having Claude Code as part of my LLM toolkit, but it‚Äôs taken a bit of getting used to.\n\n\n\n\n\nMar 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements\n\n\nWhat happens when you ask language models to evaluate their own output?\n\n\n\n\n\nJan 30, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhich names that are also names of countries are most common?\n\n\nSitting around the living room over the weekend, someone asked this question. Let‚Äôs poke at an answer.\n\n\n\n\n\nDec 23, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing a new book\n\n\nToday, during R/Pharma 2024, I open-sourced the draft of a book I‚Äôve been working on.\n\n\n\n\n\nOct 29, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPostprocessing is coming to tidymodels\n\n\nThe tidymodels team has been hard at work on postprocessing, a set of features to adjust model predictions. The functionality includes a new package as well as changes across the framework.\n\n\n\n\n\nOct 16, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nA new package for profiling parallel R code\n\n\nPeriodic snapshots of CPU and memory usage consumed by R sessions enable better analyses of parallel computation.\n\n\n\n\n\nJul 15, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow to best parallelize boosted tree model fits with tidymodels\n\n\nShould tidymodels users use the parallelism implementations from XGBoost and LightGBM?\n\n\n\n\n\nMay 13, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring elapsed time to fit with tidymodels\n\n\nThe development versions of tidymodels packages now include tools to benchmark training time.\n\n\n\n\n\nApr 8, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRun an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS\n\n\nStarting a containerized Oracle database on an Apple Silicon Mac.\n\n\n\n\n\nMar 14, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse (2023)\n\n\nAll of my friends are posting their Spotify Wrapped, and I‚Äôm jealous.\n\n\n\n\n\nNov 30, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting flight delays with tidymodelsüõ©\n\n\nTrying out a variety of machine learning models to predict flight delays out of Madison, WI.\n\n\n\n\n\nNov 28, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto!\n\n\nI recently rebuilt this site with Quarto.\n\n\n\n\n\nOct 24, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nDown the submodels rabbit hole with tidymodels\n\n\nDebugging something that turned out not to be a bug at all.\n\n\n\n\n\nOct 11, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHiking the 2023 John Muir Trail / N√º√ºm√º Poyo\n\n\nA group of buddies and I walked 242 miles from Mt. Whitney to Yosemite Valley.\n\n\n\n\n\nSep 1, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing model parameters faster with tidymodels\n\n\nA couple small changes can greatly speed up the hyperparameter tuning process with tidymodels.\n\n\n\n\n\nAug 4, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nMoving On From Baltimore\n\n\nAfter 2 years in Baltimore, I recently made the move to Chicago, IL.\n\n\n\n\n\nJul 28, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nThe tidymodels is getting a whole lot faster\n\n\nRecent optimizations have made fits on small datasets much, much snappier.\n\n\n\n\n\nMar 24, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse\n\n\nAll of my friends are posting their Spotify Wrapped, and I‚Äôm jealous.\n\n\n\n\n\nDec 1, 2022\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRedirecting from sub-domains with Netlify\n\n\nAfter merging two websites that deployed from different branches, I had to figure out how to redirect my old blog posts to their new home.\n\n\n\n\n\nJul 20, 2022\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\n{infer} v1.0.0 is on CRAN\n\n\nSome short reflections on working on the {infer} R package.\n\n\n\n\n\nAug 21, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nApplying to the NSF Graduate Research Fellowship (GRFP)\n\n\nA compilation of the advice that made me feel most informed and confident while applying for the NSF Graduate Research Fellowship.\n\n\n\n\n\nAug 2, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPipe-esque Programming with {ggplot2}‚Äôs Plus Operator\n\n\nWriting iterative code with ‚Äò+‚Äô rather than ‚Äò%&gt;%‚Äô was a tough transition my first time around.\n\n\n\n\n\nJun 10, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nBig Things (Developer Documentation pt.¬†4)\n\n\nOn the tension between documenting R packages exhaustively and maintainably.\n\n\n\n\n\nMay 13, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nNaming the Things (Developer Documentation pt.¬†3)\n\n\nWeighing the pros and cons of several possible schemas for naming the core functions in {stacks}.\n\n\n\n\n\nMay 12, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting Things Up (Developer Documentation pt.¬†2)\n\n\nWhy {stacks} requires (at least) four separate functions to build an ensemble model rather than wrapping them all up into one.\n\n\n\n\n\nMay 11, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow {stacks} Came To Be (Developer Documentation pt.¬†1)\n\n\nIntroducing a set of blog posts reflecting on the development process of the {stacks} package.\n\n\n\n\n\nMay 10, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs Next For Me\n\n\n\n\n\n\n\n\nMar 23, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nApplying to Graduate School in Statistics and Biostatistics\n\n\nThere were all sorts of tips and tricks about applying to grad school that I was only able to learn about via office hours, personal meetings, and Twitter DMs, and I thought it would be worth publicly compiling some lessons learned.\n\n\n\n\n\nMar 15, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRunning R Scripts on a Schedule with GitHub Actions\n\n\nSome pointers on running R scripts and committing their results to a GitHub repository on a regular interval using Actions.\n\n\n\n\n\nDec 27, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\n{stacks} v0.1.0 is on CRAN!\n\n\nIntroducing ensemble learning to the tidymodels.\n\n\n\n\n\nNov 30, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Generating PDF Reports with the Tidyverse\n\n\nIn a few different roles over the past few years, I‚Äôve come across the problem of programatically generating some kind of PDF reports from data. Here are some tips/tricks I‚Äôve come across while making that happen.\n\n\n\n\n\nNov 12, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to Tidy Model Stacking\n\n\nModel stacking is an ensembling technique that involves training a model to combine the outputs of many diverse statistical models. The stacks package implements a grammar for tidymodels-aligned model stacking.\n\n\n\n\n\nOct 27, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nThe Blog Thing! We‚Äôre Doing It!\n\n\nFor a while now, I‚Äôve wished I had a place to share some short posts on what I‚Äôm up to related to R, data science, and statistics. I decided I‚Äôd take a Sunday to get this subdomain up and running!\n\n\n\n\n\nOct 11, 2020\n\n\n¬†\n\n\n\n\n\nNo matching items\n\n  \n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html",
    "href": "blog/2025-12-04-local-agents/index.html",
    "title": "Local models are not there (yet)",
    "section": "",
    "text": "I understand the appeal of local models. Using coding agents like Claude Code or Codex, it‚Äôs not difficult to rack up a hundred dollars of usage in the course of a work week. Besides the price, if you‚Äôre working with sensitive IP or confidential data, you need to really believe that providers like Anthropic and OpenAI can be trusted with your data. And then, there‚Äôs evil billionaires. What if you could run models that were nearly as good on your own laptop?\nBecause of this, one of the most commonly asked questions we get about coding agents like Positron Assistant, Databot, or side::kick() is whether they can be used with local models. (For the purposes of this post, when I say ‚Äúlocal model‚Äù, I mean an LLM that can be served on a high-end laptop with decent tokens per second. Think GPT-OSS 20B.) The answer is yes, but also that we don‚Äôt recommend doing so; while you can connect to a local model through an OpenAI API-compatible endpoint with either of those systems, you likely won‚Äôt have a good experience if you do so.\nThis is an argument I‚Äôve tried to make in a paragraph or two many times, but I thought it would be worth fleshing out in more detail. To do so, I put together an LLM evaluation that demonstrate the issues we‚Äôve seen when trying to get these models working."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#the-problem",
    "href": "blog/2025-12-04-local-agents/index.html#the-problem",
    "title": "Local models are not there (yet)",
    "section": "The problem",
    "text": "The problem\nLet‚Äôs consider a table-stakes capability for a coding agent; a simple refactor. Here, when I say ‚Äúcoding agent,‚Äù a mean an LLM that‚Äôs equipped with tools to explore and make changes to your working directory. That could be Claude Code, Codex, Positron Assistant, or side::kick().\nSo, say I have some file tool-fetch_skill.R pulled up that looks like this:\n\nfetch_skill_impl &lt;- function(skill_name, `_intent` = NULL) {\n  skill_path &lt;- find_skill(skill_name)\n\n  if (is.null(skill_path)) {\n    available &lt;- list_available_skills()\n    skill_names &lt;- vapply(available, function(x) x$name, character(1))\n    cli::cli_abort(\n      c(\n        \"Skill {.val {skill_name}} not found.\",\n        \"i\" = \"Available skills: {.val {skill_names}}\"\n      ),\n      call = rlang::caller_env()\n    )\n  }\n\n  skill_content &lt;- readLines(skill_path, warn = FALSE)\n\n  # Remove YAML frontmatter if present\n  content_start &lt;- 1\n  if (length(skill_content) &gt; 0 && skill_content[1] == \"---\") {\n    yaml_end &lt;- which(skill_content == \"---\")\n    if (length(yaml_end) &gt;= 2) {\n      content_start &lt;- yaml_end[2] + 1\n    }\n  }\n\n  skill_text &lt;- paste(\n    skill_content[content_start:length(skill_content)],\n    collapse = \"\\n\"\n  )\n  \n  # ...\n\nI might provide a prompt like the following:\n\nRefactor this code from my project into a helper:\n# Remove YAML frontmatter if present\ncontent_start &lt;- 1\nif (length(skill_content) &gt; 0 && skill_content[1] == \"---\") {\n  yaml_end &lt;- which(skill_content == \"---\")\n  if (length(yaml_end) &gt;= 2) {\n    content_start &lt;- yaml_end[2] + 1\n  }\n}\n\nAt this point, the coding agent needs to do a few things:\n\nFind which file in the project those lines are from by using a text search tool, probably for ‚ÄúRemove YAML frontmatter‚Äù or a different snippet of code.\nRead the file that it discovers.\nApply edits to the file, first by adding a helper function (probably to the end of the file or to a new file in the directory) and then second by replacing the original usage with a call to the helper.\n\nIn practice, that might look something like this:\n\n\n\n\n\n\n\nNote\n\n\n\nIn case this interface is unfamiliar, this is the RStudio IDE running an open source coding agent side::kick(). You‚Äôll need a daily version of the RStudio IDE to get that full-height sidebar.\n\n\nThis is something that a coding agent should get perfect almost always; my initial prompt is a bit more detailed than I‚Äôd usually provide for such a simple task, there‚Äôs no context rot that could affect the agent‚Äôs effectiveness, and the task is a simple one."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#evaluation",
    "href": "blog/2025-12-04-local-agents/index.html#evaluation",
    "title": "Local models are not there (yet)",
    "section": "Evaluation",
    "text": "Evaluation\nUsing the vitals package, I implemented an LLM evaluation where various models are tasked with that refactor multiple times. You can install it with pak::pak(\"simonpcouch/helperbench\").\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(helperbench)\n\nFor a given run, the setup goes like this:\n\nThe side::kick() GitHub repository is copied into a temporary directory.\nAn ellmer chat is provided with the side::kick() prompt and tools that tell it to basically act like a Claude Code- or Codex-flavored agent and is situated in that directory.\nThe agent is provided with the user prompt shown above (‚ÄúCan you refactor this‚Ä¶‚Äù) and allowed to work until completion.\nOnce the agent finishes,\n\nThe eval checks whether the agent actually made any edits to (at least) the relevant file. If it didn‚Äôt, that run is a failure.\nThen, the contents of the resulting R/ directory is copied in place of the R/ directory in another ephemeral copy of the side::kick() GitHub repository, and the package unit tests run. If the unit tests pass, the run is a pass, otherwise it‚Äôs a failure.\n\n\n\nFor each of 9 models, I ran the eval 10 times. The results are combined in this data frame:\n\nhelper_results %&gt;%\n  sample_n(10)\n\n# A tibble: 10 √ó 6\n   model             score type         cost   input output\n   &lt;chr&gt;             &lt;ord&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 GPT-4.1 Mini      C     Budget   0.488    1204578   3944\n 2 Qwen 3 14B        I     Local    0.00207    13533   5025\n 3 GPT OSS 20B       I     Local    0.00386   119159   2069\n 4 GPT-4.1           C     Frontier 1.32      640982   4151\n 5 Claude Haiku 4.5  C     Budget   0.161     151944   1764\n 6 Claude Sonnet 4.5 C     Frontier 0.444     139778   1626\n 7 Gemini Flash 2.5  I     Budget   0.000438     219    149\n 8 Claude Haiku 4.5  C     Budget   0.138     128187   1981\n 9 GPT OSS 20B       I     Local    0.00158    48270    971\n10 GPT OSS 20B       I     Local    0.000354    8811    641\n\n\nEach row is a given run, where score == \"C\" means the run succeeded."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#performance-by-model",
    "href": "blog/2025-12-04-local-agents/index.html#performance-by-model",
    "title": "Local models are not there (yet)",
    "section": "Performance by model",
    "text": "Performance by model\nWe can first plot the proportion of runs that succeeded by model:\n\nhelper_results %&gt;%\n  mutate(\n    correct = score == \"C\",\n    lab = case_when(\n      model %in% c(\"Claude Haiku 4.5\", \"Claude Sonnet 4.5\") ~ \"Anthropic\",\n      model %in% c(\"GPT-4.1 Mini\", \"GPT-4.1\", \"GPT OSS 20B\") ~ \"OpenAI\",\n      model %in% c(\"Gemini Flash 2.5\", \"Gemini Pro 2.5\") ~ \"Google\",\n      TRUE ~ \"Other\"\n    ),\n    color = case_when(\n      lab == \"Anthropic\" ~ \"#CC5500\",\n      lab == \"OpenAI\" ~ \"#ADD8E6\",\n      lab == \"Google\" ~ \"#22B14C\",\n      TRUE ~ \"#CCCCCC\"\n    )\n  ) %&gt;%\n  group_by(model, type, lab, color) %&gt;%\n  summarise(pct_correct = mean(correct) * 100, .groups = \"drop\") %&gt;%\n  mutate(pct_correct = pmax(pct_correct, .5)) %&gt;%\n  arrange(type, pct_correct) %&gt;%\n  mutate(\n    type = factor(type, levels = c(\"Frontier\", \"Budget\", \"Local\")),\n    model = fct_inorder(model)\n  ) %&gt;%\n  ggplot(aes(x = model, y = pct_correct, fill = color)) +\n  geom_col() +\n  scale_fill_identity() +\n  scale_y_continuous(n.breaks = 10, minor_breaks = NULL) +\n  facet_wrap(~type, nrow = 3, dir = \"v\", scales = \"free_y\") +\n  coord_flip() +\n  labs(\n    y = \"Percent Correct\",\n    x = NULL,\n    title = \"Agentic Coding Reliability\",\n    subtitle = \"When choosing models to power coding tools, you get what you pay for.\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = -0.343),\n    plot.subtitle = element_text(hjust = 10, face = \"italic\"),\n    axis.text.y = element_text(angle = 15, hjust = 1)\n  )\n\n\n\n\n\n\n\nThe models labeled ‚ÄúFrontier‚Äù are models that I would actually use as a daily-driver coding assistant in an application like Positron Assistant or side::kick(), one from each of the big three labs. The models labeled ‚ÄúBudget‚Äù are ‚Äúone step below‚Äù from each of the big three AI labs, but do note that there are some significant price differences within that band. These are models that I might temporarily switch to when speed is a priority and I feel confident the quality won‚Äôt be an issue. Finally, the models marked ‚ÄúLocal‚Äù are models that are small enough that I could run them at a reasonable tokens per second on my laptop. To make sure the playing field was otherwise as level as possible, I used chat_openrouter() rather than each of the labs‚Äô APIs directly. In practice, I‚Äôd usually prefer to use the labs‚Äô APIs directly and the local models‚Ä¶ locally.\nThe graph shows that the frontier models complete the refactor successfully almost always. The budget models are spotty. Never once did a model I could run on my laptop complete the refactor successfully.\n\n\n\n\n\n\nNote\n\n\n\nThere are a couple notable model omissions here.\n\nI had trouble getting Gemini 3 Pro working with ellmer through OpenRouter and didn‚Äôt bother to debug.\nI use GPT 4.1 rather than 5, 5.1, or any of the Codex variants of the GPT family. side::kick() has ‚Äúbuilt-in‚Äù thinking and is this most pleasant with non-thinking models; OpenAI (currently, as of December 2025) describes GPT 4.1 as their ‚Äúsmartest non-reasoning model.‚Äù\n\nOn the Local side:\n\nGemma 3 27B was one of my first thoughts, but OpenRouter doesn‚Äôt support tool calling with that model.\nI also had initially ran this against Qwen 3 Coder 30B, and it did surprisingly well (7/10!), but then I realized it‚Äôs too large to run on my M4 Macbook Pro with 48GB memory unless quantized aggressively, which ruins the performance.\n\n\n\nSurprisingly, the failure modes for each of the local models tested here are pretty consistent within-model:\n\nMistral 3.1 24B just (correctly) writes out the steps to make the refactor without calling the needed tools to make it happen.\nGPT OSS 20B attempts to call a few tools and struggles to get the argument formats right before giving up.\nQwen 3 14B tends to declare success too early, writing the helper function correctly but not incorporating it back into the original function.\n\nYou can explore the logs from each of these runs in more detail in the log viewer:"
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#factoring-in-cost",
    "href": "blog/2025-12-04-local-agents/index.html#factoring-in-cost",
    "title": "Local models are not there (yet)",
    "section": "Factoring in cost",
    "text": "Factoring in cost\nAgain, my categorizations of models here aren‚Äôt a given. Within those categories, there are pretty substantial differences in the average cost per run:\n\nhelper_results %&gt;%\n  mutate(\n    correct = score == \"C\",\n    lab = case_when(\n      model %in% c(\"Claude Haiku 4.5\", \"Claude Sonnet 4.5\") ~ \"Anthropic\",\n      model %in% c(\"GPT-4.1 Mini\", \"GPT-4.1\", \"GPT OSS 20B\") ~ \"OpenAI\",\n      model %in% c(\"Gemini Flash 2.5\", \"Gemini Pro 2.5\") ~ \"Google\",\n      TRUE ~ \"Other\"\n    ),\n    color = case_when(\n      lab == \"Anthropic\" ~ \"#CC5500\",\n      lab == \"OpenAI\" ~ \"#ADD8E6\",\n      lab == \"Google\" ~ \"#22B14C\",\n      TRUE ~ \"#b4b4b4ff\"\n    ),\n    color = factor(color, levels = c(\"#CC5500\", \"#22B14C\", \"#ADD8E6\", \"#b4b4b4ff\"))\n  ) %&gt;%\n  group_by(model, type, lab, color) %&gt;%\n  summarise(avg_cost = mean(cost / n(), na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(avg_cost = pmax(avg_cost, .001)) %&gt;%\n  arrange(type, desc(color), avg_cost) %&gt;%\n  mutate(\n    type = factor(type, levels = c(\"Frontier\", \"Budget\", \"Local\")),\n    model = fct_inorder(model)\n  ) %&gt;%\n  ggplot(aes(x = model, y = avg_cost, fill = color)) +\n  geom_col() +\n  scale_fill_identity() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  facet_wrap(~type, nrow = 3, dir = \"v\", scales = \"free_y\") +\n  coord_flip() +\n  labs(\n    y = \"Average Cost Per Run\", \n    x = NULL,\n    title = \"Cost by model\",\n    subtitle = \"There's more to LLM pricing than cost per token.\"\n  ) +\n  theme(\n    legend.position = \"none\", \n    plot.title = element_text(hjust = -0.1),\n    plot.subtitle = element_text(hjust = -0.25, face = \"italic\"),\n    axis.text.y = element_text(angle = 15, hjust = 1)\n  )\n\n\n\n\n\n\n\nThe bar heights represent the average cost per refactor. So, it costed about 4 cents for Claude Sonnet 4.5 to ‚Äúread‚Äù the side::kick() prompt and tool descriptions, call tools to check out its surroundings, and apply the edits once. Altogether, this eval costed me about $22 to run.\nSince we have a proxy for performance and the cost, we might as well do the performance vs.¬†cost plot:\n\nhelper_results %&gt;%\n  mutate(\n    correct = score == \"C\",\n    lab = case_when(\n      model %in% c(\"Claude Haiku 4.5\", \"Claude Sonnet 4.5\") ~ \"Anthropic\",\n      model %in% c(\"GPT-4.1 Mini\", \"GPT-4.1\", \"GPT OSS 20B\") ~ \"OpenAI\",\n      model %in% c(\"Gemini Flash 2.5\", \"Gemini Pro 2.5\") ~ \"Google\",\n      TRUE ~ \"Other\"\n    ),\n    color = case_when(\n      lab == \"Anthropic\" ~ \"#CC5500\",\n      lab == \"OpenAI\" ~ \"#ADD8E6\",\n      lab == \"Google\" ~ \"#22B14C\",\n      TRUE ~ \"#b4b4b4ff\"\n    )\n  ) %&gt;%\n  group_by(model, type, lab, color) %&gt;%\n  summarise(\n    pct_correct = mean(correct) * 100,\n    avg_cost = mean(cost / n(), na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    type = factor(type, levels = c(\"Frontier\", \"Budget\", \"Local\"))\n  ) %&gt;%\n  ggplot(aes(x = avg_cost, y = pct_correct, color = color)) +\n  geom_point(alpha = 0.8) +\n  geom_text_repel(aes(label = model), size = 3.5, show.legend = FALSE) +\n  scale_color_identity() +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_size_identity() +\n  labs(\n    x = \"Average Cost Per Run\",\n    y = \"Percent Correct\",\n    title = \"Refactoring reliability by price\",\n    subtitle = \"Cost per token is very different than cost per success.\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.subtitle = element_text(face = \"italic\")\n  )\n\n\n\n\n\n\n\nNote that the proportions in pricing don‚Äôt map directly to the cost per token. The cost per token is different than the cost per attempt, and very different than the cost per success. Models that ‚Äúthink‚Äù for a while before acting will use up more tokens than models that don‚Äôt, and models that struggle to use tools fluently can use many more tokens in their attempts to resolve tool errors."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#conclusion",
    "href": "blog/2025-12-04-local-agents/index.html#conclusion",
    "title": "Local models are not there (yet)",
    "section": "Conclusion",
    "text": "Conclusion\nI don‚Äôt really enjoy telling folks that they ought not to use local models. Again, the advantages are clear; free (as in, the cost of electricity to charge your laptop), perfect privacy, no mega-corporations involved. In the medium run (1-2 years?), I‚Äôd love for it to be the case that you can run a Claude Sonnet 4-ish model on a base Macbook Pro, and I think that‚Äôs a plausible future. For now, though, if you‚Äôd like to get value out of coding agents, you‚Äôll need to opt for the most capable models out there."
  },
  {
    "objectID": "blog/2025-12-10-chores/index.html",
    "href": "blog/2025-12-10-chores/index.html",
    "title": "chores 0.3.0 and local LLMs",
    "section": "",
    "text": "The tl;dr:"
  },
  {
    "objectID": "blog/2025-12-10-chores/index.html#the-chores-package",
    "href": "blog/2025-12-10-chores/index.html#the-chores-package",
    "title": "chores 0.3.0 and local LLMs",
    "section": "The chores package",
    "text": "The chores package\nThe chores package provides a library of ergonomic LLM assistants designed to help you complete repetitive, hard-to-automate tasks quickly. After selecting some code, you can press a keyboard shortcut, select a helper that corresponds to a system prompt, and watch your code be rewritten.\n\nWhen you select some code and a chore helper, what‚Äôs happening under the hood is that the package first retrieves the prompt corresponding the system prompt you chose. For example, the prompt for templating out roxygen2 function documentation looks like this:\n\n\nTemplating function documentation\nYou are a terse assistant designed to help R package developers quickly template out their function documentation using roxygen2. Given some highlighted function code, return minimal documentation on the function‚Äôs parameters and return type. Beyond those two elements, be sparing so as not to describe things you don‚Äôt have context for. Respond with only R #' roxygen2 comments‚Äîno backticks or newlines around the response, no further commentary.\nFor function parameters in @params, describe each according to their type (e.g.¬†‚ÄúA numeric vector‚Äù or ‚ÄúA single string‚Äù) and note if the parameter isn‚Äôt required by writing ‚ÄúOptional‚Äù if it has a default value. If the parameters have a default enum (e.g.¬†arg = c(\"a\", \"b\", \"c\")), write them out as ‚Äòone of \"a\", \"b\", or \"c\".‚Äô If there are ellipses in the function signature, note what happens to them. If they‚Äôre checked with rlang::check_dots_empty() or otherwise, document them as ‚ÄúCurrently unused; must be empty.‚Äù If the ellipses are passed along to another function, note which function they‚Äôre passed to.\nFor the return type in @returns, note any important errors or warnings that might occur and under what conditions. If the output is returned with invisible(output), note that it‚Äôs returned ‚Äúinvisibly.‚Äù\nHere are some examples:\nGiven:\nkey_get &lt;- function(name, error_call = caller_env()) {\n  val &lt;- Sys.getenv(name)\n  if (!identical(val, \"\")) {\n    val\n  } else {\n    if (is_testing()) {\n      testthat::skip(sprintf(\"%s env var is not configured\", name))\n    } else {\n      cli::cli_abort(\"Can't find env var {.code {name}}.\", call = error_call)\n    }\n  }\n}\nReply with:\n#' Get key\n#'\n#' @description\n#' A short description...\n#' \n#' @param name A single string.\n#' @param error_call A call to mention in error messages. Optional.\n#'\n#' @returns \n#' If found, the value corresponding to the provided `name`. Otherwise,\n#' the function will error.\n#'\n#' @export\nGiven:\nchat_perform &lt;- function(provider,\n                         mode = c(\"value\", \"stream\", \"async-stream\", \"async-value\"),\n                         turns,\n                         tools = list(),\n                         extra_args = list()) {\n\n  mode &lt;- arg_match(mode)\n  stream &lt;- mode %in% c(\"stream\", \"async-stream\")\n\n  req &lt;- chat_request(\n    provider = provider,\n    turns = turns,\n    tools = tools,\n    stream = stream,\n    extra_args = extra_args\n  )\n\n  switch(mode,\n    \"value\" = chat_perform_value(provider, req),\n    \"stream\" = chat_perform_stream(provider, req),\n    \"async-value\" = chat_perform_async_value(provider, req),\n    \"async-stream\" = chat_perform_async_stream(provider, req)\n  )\n}\nReply with:\n#' Perform chat\n#'\n#' @description\n#' A short description...\n#' \n#' @param provider A provider.\n#' @param mode One of `\"value\"`, `\"stream\"`, `\"async-stream\"`, or `\"async-value\"`.\n#' @param turns Turns.\n#' @param tools Optional. A list of tools.\n#' @param extra_args Optional. A list of extra arguments.\n#'\n#' @returns \n#' A result.\n#'\n#' @export\nGiven:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    cli::cli_abort(\"Function must have at least two arguments.\", .internal = TRUE)\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\nReply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\nWhen two functions are supplied, only provide documentation for the first function, only making use of later functions as additional context. For example:\nGiven:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    error_less_than_two_args()\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\n\nerror_less_than_two_args &lt;- function(call = caller_env()) {\n  cli::cli_abort(\"Function must have at least two arguments.\", call = call, .internal = TRUE)\n}\nReply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\n\n\nThen, the selected helper prompt is set as the system prompt and the code you selected is set as the user prompt in a call to ellmer::Chat(). It looks something like this:\n\nlibrary(ellmer)\n\nch &lt;- chat_anthropic(system_prompt = the_prompt_from_above)\n \nch$chat(\"&lt;the code you selected&gt;\")\n#&gt; #' The documentation for the selected code.\n#&gt; #' \n#&gt; #' Yada yada yada."
  },
  {
    "objectID": "blog/2025-12-10-chores/index.html#choosing-a-model",
    "href": "blog/2025-12-10-chores/index.html#choosing-a-model",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Choosing a model",
    "text": "Choosing a model\nThe chores package allows you to use any model you can connect to with ellmer. So, how do you choose which one to use?\nThe model powering chores needs the following characteristics:\n\nStrict instruction-following: Looking back at that roxygen prompt, those instructions make two asks of LLMs that are pretty difficult for models that have been trained so strictly into the ‚Äúhelpful assistant‚Äù role: no exposition or explanatory text before or after the roxygen comments, and no (triple) backticks around the response. The chores package writes the LLMs‚Äô output directly to the source file, so it‚Äôs really frustrating when models provide any text other than what‚Äôs requested.\nMinimally- or non-thinking: Thinking adds latency and shouldn‚Äôt be necessary to complete these tasks. There are many interfaces where thinking is nice, but this isn‚Äôt one of them.\n\nNotably, the model does not need the ability to call tools, carry out long-horizon tasks, or be a pleasant conversationalist. It‚Äôs fine if the model used with chores is bad at pretty much everything besides writing syntactically valid code in compliance with the instructions in the provided prompt.\nIn the package documentation, I recommend Claude 3.7 Sonnet and GPT 4.1 (optionally, -mini). Up to this point, though, I‚Äôd thought you really had needed to use a frontier-ish model to get any value out of chores. It‚Äôs seemed to me that many of the models that I can currently run on my laptop (up to late 2025) had been trained into the ‚ÄúI‚Äôm a helpful assistant‚Äù persona so strictly‚Äìeven those that are advertised as instruction-tuned‚Äìthat they‚Äôll ramble on and on before and after providing the requested code, even if the provided code is reasonable.\nIn some ways, some time just needed to pass, but I also had overlooked a critical issue (and seemingly everyone else that‚Äôs tried to use chores with local models). In working on another problem, I learned that:\n\nollama‚Äôs and LM Studio‚Äôs default context length is 4,096 tokens, even for models that support much longer context windows, and\nif you provide a prompt that‚Äôs greater than the size of the context length, it will be truncated to fit inside the length.\n\nI think this probably contributed to my misconception that models small enough to run on my laptop weren‚Äôt capable of powering chores. Once I trimmed the cli helper prompt to fit inside the context window (and/or increased the size of the context window in ollama), I saw much more promising results on these tasks than I had seen from local models before."
  },
  {
    "objectID": "blog/2025-12-10-chores/index.html#a-new-kid-on-the-block",
    "href": "blog/2025-12-10-chores/index.html#a-new-kid-on-the-block",
    "title": "chores 0.3.0 and local LLMs",
    "section": "A new kid on the block",
    "text": "A new kid on the block\nSo, even with the models that I had ollama pulled a few months ago, I realized we were already closer to local models powering chores than I had thought. At that point, I wondered what the newest releases were that might show stronger performance. The Qwen team is cracked, so I started there.\nAfter some pokings-around, I think that Qwen3 4B Instruct 2507 is a great model for local use with chores. Here‚Äôs a real-time (as in, not sped up) demo of that model in action on my M4 Macbook:\n\nThe cli refactor is a little bit wonky; the model chose to use backticks around the env var markup rather than curly braces, which won‚Äôt render correctly. The templating for the roxygen2 documentation is totally reasonable. Performance isn‚Äôt quite Claude 3.7 Sonnet, but I‚Äôm pretty blown away by how good it is.\nOn Apple Silicon (Mac M-series), I recommend LM Studio; LM Studio supports MLX, an array framework for Apple Silicon, which helps the model run much more quickly than with ollama. Click ‚ÄúDiscover‚Äù, search ‚ÄúQwen3 4B Instruct 2507‚Äù, and click ‚ÄúDownload.‚Äù Once downloaded, click the ‚ÄúDeveloper‚Äù tab and change the Status from Stopped to Running. Then, in R, configure chores with:\nqwen3_4b &lt;- ellmer::chat_openai_compatible(\n  base_url = \"http://127.0.0.1:1234/v1\",\n  model = \"qwen/qwen3-4b-2507\"\n)\n\noptions(chores.chat = qwen3_4b)\nNote the /v1 in the base URL; this will hit LM Studio‚Äôs OpenAI API v1 endpoint.\nOn other systems, use Ollama. Run ollama pull qwen3:4b at the terminal, then set options(chores.chat = ellmer::chat_ollama(model = \"qwen3:4b\")).\nAt least with MLX on LM Studio, the model takes up 2.5GB of disk space and requires 2.5GB of RAM to run."
  },
  {
    "objectID": "blog/2025-12-10-chores/index.html#oh-and-chores",
    "href": "blog/2025-12-10-chores/index.html#oh-and-chores",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Oh, and chores",
    "text": "Oh, and chores\nFor a good experience ‚Äúby default‚Äù (i.e.¬†without the need to change the context length in LM Studio to use the default helpers), install the new release of chores with install.packages(\"chores\")!"
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html",
    "href": "blog/2025-12-10-chores-0-3-0/index.html",
    "title": "chores 0.3.0 and local LLMs",
    "section": "",
    "text": "The tl;dr:"
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#the-chores-package",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#the-chores-package",
    "title": "chores 0.3.0 and local LLMs",
    "section": "The chores package",
    "text": "The chores package\nThe chores package provides a library of ergonomic LLM assistants designed to help you complete repetitive, hard-to-automate tasks quickly. After selecting some code, you can press a keyboard shortcut, select a helper that corresponds to a system prompt, and watch your code be rewritten.\n\nWhen you select some code and a chore helper, what‚Äôs happening under the hood is that the package first retrieves the prompt corresponding the system prompt you chose. For example, the prompt for templating out roxygen2 function documentation looks like this:\n\n\nTemplating function documentation\nYou are a terse assistant designed to help R package developers quickly template out their function documentation using roxygen2. Given some highlighted function code, return minimal documentation on the function‚Äôs parameters and return type. Beyond those two elements, be sparing so as not to describe things you don‚Äôt have context for. Respond with only R #' roxygen2 comments‚Äîno backticks or newlines around the response, no further commentary.\nFor function parameters in @params, describe each according to their type (e.g.¬†‚ÄúA numeric vector‚Äù or ‚ÄúA single string‚Äù) and note if the parameter isn‚Äôt required by writing ‚ÄúOptional‚Äù if it has a default value. If the parameters have a default enum (e.g.¬†arg = c(\"a\", \"b\", \"c\")), write them out as ‚Äòone of \"a\", \"b\", or \"c\".‚Äô If there are ellipses in the function signature, note what happens to them. If they‚Äôre checked with rlang::check_dots_empty() or otherwise, document them as ‚ÄúCurrently unused; must be empty.‚Äù If the ellipses are passed along to another function, note which function they‚Äôre passed to.\nFor the return type in @returns, note any important errors or warnings that might occur and under what conditions. If the output is returned with invisible(output), note that it‚Äôs returned ‚Äúinvisibly.‚Äù\nHere are some examples:\nGiven:\nkey_get &lt;- function(name, error_call = caller_env()) {\n  val &lt;- Sys.getenv(name)\n  if (!identical(val, \"\")) {\n    val\n  } else {\n    if (is_testing()) {\n      testthat::skip(sprintf(\"%s env var is not configured\", name))\n    } else {\n      cli::cli_abort(\"Can't find env var {.code {name}}.\", call = error_call)\n    }\n  }\n}\nReply with:\n#' Get key\n#'\n#' @description\n#' A short description...\n#' \n#' @param name A single string.\n#' @param error_call A call to mention in error messages. Optional.\n#'\n#' @returns \n#' If found, the value corresponding to the provided `name`. Otherwise,\n#' the function will error.\n#'\n#' @export\nGiven:\nchat_perform &lt;- function(provider,\n                         mode = c(\"value\", \"stream\", \"async-stream\", \"async-value\"),\n                         turns,\n                         tools = list(),\n                         extra_args = list()) {\n\n  mode &lt;- arg_match(mode)\n  stream &lt;- mode %in% c(\"stream\", \"async-stream\")\n\n  req &lt;- chat_request(\n    provider = provider,\n    turns = turns,\n    tools = tools,\n    stream = stream,\n    extra_args = extra_args\n  )\n\n  switch(mode,\n    \"value\" = chat_perform_value(provider, req),\n    \"stream\" = chat_perform_stream(provider, req),\n    \"async-value\" = chat_perform_async_value(provider, req),\n    \"async-stream\" = chat_perform_async_stream(provider, req)\n  )\n}\nReply with:\n#' Perform chat\n#'\n#' @description\n#' A short description...\n#' \n#' @param provider A provider.\n#' @param mode One of `\"value\"`, `\"stream\"`, `\"async-stream\"`, or `\"async-value\"`.\n#' @param turns Turns.\n#' @param tools Optional. A list of tools.\n#' @param extra_args Optional. A list of extra arguments.\n#'\n#' @returns \n#' A result.\n#'\n#' @export\nGiven:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    cli::cli_abort(\"Function must have at least two arguments.\", .internal = TRUE)\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\nReply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\nWhen two functions are supplied, only provide documentation for the first function, only making use of later functions as additional context. For example:\nGiven:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    error_less_than_two_args()\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\n\nerror_less_than_two_args &lt;- function(call = caller_env()) {\n  cli::cli_abort(\"Function must have at least two arguments.\", call = call, .internal = TRUE)\n}\nReply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\n\n\nThen, the selected helper prompt is set as the system prompt and the code you selected is set as the user prompt in a call to ellmer::Chat(). It looks something like this:\n\nlibrary(ellmer)\n\nch &lt;- chat_anthropic(system_prompt = the_prompt_from_above)\n \nch$chat(\"&lt;the code you selected&gt;\")\n#&gt; #' The documentation for the selected code.\n#&gt; #' \n#&gt; #' Yada yada yada."
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#choosing-a-model",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#choosing-a-model",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Choosing a model",
    "text": "Choosing a model\nThe chores package allows you to use any model you can connect to with ellmer. So, how do you choose which one to use?\nThe model powering chores needs the following characteristics:\n\nStrict instruction-following: Looking back at that roxygen prompt, those instructions make two asks of LLMs that are pretty difficult for models that have been trained so strictly into the ‚Äúhelpful assistant‚Äù role: no exposition or explanatory text before or after the roxygen comments, and no (triple) backticks around the response. The chores package writes the LLMs‚Äô output directly to the source file, so it‚Äôs really frustrating when models provide any text other than what‚Äôs requested.\nMinimally- or non-thinking: Thinking adds latency and shouldn‚Äôt be necessary to complete these tasks. There are many interfaces where thinking is nice, but this isn‚Äôt one of them.\n\nNotably, the model does not need the ability to call tools, carry out long-horizon tasks, or be a pleasant conversationalist. It‚Äôs fine if the model used with chores is bad at pretty much everything besides writing syntactically valid code in compliance with the instructions in the provided prompt.\nIn the package documentation, I recommend Claude 3.7 Sonnet and GPT 4.1 (optionally, -mini). Up to this point, though, I‚Äôd thought you really had needed to use a frontier-ish model to get any value out of chores. It‚Äôs seemed to me that many of the models that I can currently run on my laptop (up to late 2025) had been trained into the ‚ÄúI‚Äôm a helpful assistant‚Äù persona so strictly‚Äìeven those that are advertised as instruction-tuned‚Äìthat they‚Äôll ramble on and on before and after providing the requested code, even if the provided code is reasonable.\nIn some ways, some time just needed to pass, but I also had overlooked a critical issue (and seemingly everyone else that‚Äôs tried to use chores with local models). In working on another problem, I learned that:\n\nollama‚Äôs and LM Studio‚Äôs default context length is 4,096 tokens, even for models that support much longer context windows, and\nIf you provide a prompt that‚Äôs greater than the size of the context length, it will be truncated to fit inside the length rather than erroring.\n\nI think this probably contributed to my misconception that models small enough to run on my laptop weren‚Äôt capable of powering chores. Once I trimmed the cli helper prompt to fit inside the context window (and/or increased the size of the context window in ollama), I saw much more promising results on these tasks than I had seen from local models before."
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#a-new-kid-on-the-block",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#a-new-kid-on-the-block",
    "title": "chores 0.3.0 and local LLMs",
    "section": "A new kid on the block",
    "text": "A new kid on the block\nSo, even with the models that I had ollama pulled a few months ago, I realized we were already closer to local models powering chores than I had thought. At that point, I wondered what the newest releases were that might show stronger performance. The Qwen team is cracked, so I started there.\nAfter some pokings-around, I think that Qwen3 4B Instruct 2507 is a great model for local use with chores. Here‚Äôs a real-time (as in, not sped up) demo of that model in action on my M4 Macbook:\n\nThe cli refactor is a little bit wonky; the model chose to use backticks around the env var markup rather than curly braces, which won‚Äôt render correctly. The templating for the roxygen2 documentation is totally reasonable. Performance isn‚Äôt quite Claude 3.7 Sonnet, but I‚Äôm pretty blown away by how good it is.\nOn Apple Silicon (Mac M-series), I recommend LM Studio; LM Studio supports MLX, an array framework for Apple Silicon, which helps the model run much more quickly than with ollama. Click ‚ÄúDiscover‚Äù, search ‚ÄúQwen3 4B Instruct 2507‚Äù, and click ‚ÄúDownload.‚Äù Once downloaded, click the ‚ÄúDeveloper‚Äù tab and change the Status from Stopped to Running. Then, in R, configure chores with:\nqwen3_4b &lt;- ellmer::chat_openai_compatible(\n  base_url = \"http://127.0.0.1:1234/v1\",\n  model = \"qwen/qwen3-4b-2507\"\n)\n\noptions(chores.chat = qwen3_4b)\nNote the /v1 in the base URL; this will hit LM Studio‚Äôs OpenAI API v1 endpoint.\nOn other systems, use Ollama. Run ollama pull qwen3:4b at the terminal, then set options(chores.chat = ellmer::chat_ollama(model = \"qwen3:4b\")).\nAt least with MLX on LM Studio, the model takes up 2.5GB of disk space and requires 2.5GB of RAM to run."
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#oh-and-chores",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#oh-and-chores",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Oh, and chores",
    "text": "Oh, and chores\nFor a good experience ‚Äúby default‚Äù (i.e.¬†without the need to change the context length in LM Studio to use the default helpers), install the new release of chores with install.packages(\"chores\")!"
  }
]