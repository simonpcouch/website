[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Some Pitters and Patters with R",
    "section": "",
    "text": "Local models are not there (yet)\n\n\nLLMs that can run on your laptop are not yet capable enough to drive coding agents.\n\n\n\n\n\nDec 4, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my music listening data with Databot\n\n\nIt‚Äôs Spotify Wrapped season.\n\n\n\n\n\nDec 3, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhen plotting, LLMs see what they expect to see\n\n\nData science agents need to accurately read plots even when the content contradicts their expectations.\n\n\n\n\n\nNov 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nside::kick(), a coding agent for RStudio\n\n\nOpen-source AI for RStudio, built entirely in R.\n\n\n\n\n\nNov 11, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nI‚Äôm‚Ä¶ writing a newsletter?\n\n\nSara Altman and I have been working on a newsletter focused on AI.\n\n\n\n\n\nOct 8, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nI was wrong about tidymodels and LLMs\n\n\nToday‚Äôs frontier LLMs know much more about tidymodels than I thought.\n\n\n\n\n\nAug 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nR and the Model Context Protocol\n\n\nThe newly released mcptools package makes coding assistants better at writing R code and applications built with ellmer more powerful.\n\n\n\n\n\nAug 14, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow I‚Äôm using Claude Code to write R code\n\n\nGiving Claude Code the ability to peruse R package documentation via MCP is so, so helpful.\n\n\n\n\n\nJul 17, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nKimi K2 and R Coding\n\n\nAn open-weights model released over the weekend by a little-known company has drawn quite a bit of attention. Is it any good?\n\n\n\n\n\nJul 14, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nClaude 4 and R Coding\n\n\nEvaluating the most recent releases of Claude Sonnet and Opus on challenging R coding problems.\n\n\n\n\n\nMay 27, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Gemini 2.5 Flash on R coding tasks\n\n\nGoogle‚Äôs Gemini 2.5 Pro release really made a splash last month. They‚Äôve just announced an update to 2.5 Flash, a faster and cheaper model.\n\n\n\n\n\nMay 21, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the new Gemini 2.5 Pro update on R coding\n\n\nThe initial Gemini 2.5 Pro release a month ago was surprisingly strong, so I was excited to benchmark the update announced yesterday on R coding problems.\n\n\n\n\n\nMay 7, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating o3 and o4-mini on R coding performance\n\n\nWith a pair of coding-focused model drops, it‚Äôs clear OpenAI is aiming for the developer market. How well do the newest o3 and o4-mini perform on R coding tasks?\n\n\n\n\n\nApr 18, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow good are the GPT 4.1 models at writing R code?\n\n\nOpenAI dropped a trio of models yesterday that they claim result from ‚Äúfocusing closely on real-world developer needs.‚Äù I was curious whether this might mean stronger R coding performance.\n\n\n\n\n\nApr 15, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing chores\n\n\nA package to help with repetitive but hard-to-automate data science tasks.\n\n\n\n\n\nApr 11, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow Good Is Gemini 2.5 Pro at Writing R Code?\n\n\nSince Gemini 2.5 Pro‚Äôs release last week, I‚Äôve been seeing a lot of hype claiming that the model is the new state of the art. How well does it know R?\n\n\n\n\n\nApr 2, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow I‚Äôm Using Claude Code to Develop R Packages\n\n\nI‚Äôve come to really appreciate having Claude Code as part of my LLM toolkit, but it‚Äôs taken a bit of getting used to.\n\n\n\n\n\nMar 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements\n\n\nWhat happens when you ask language models to evaluate their own output?\n\n\n\n\n\nJan 30, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhich names that are also names of countries are most common?\n\n\nSitting around the living room over the weekend, someone asked this question. Let‚Äôs poke at an answer.\n\n\n\n\n\nDec 23, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing a new book\n\n\nToday, during R/Pharma 2024, I open-sourced the draft of a book I‚Äôve been working on.\n\n\n\n\n\nOct 29, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPostprocessing is coming to tidymodels\n\n\nThe tidymodels team has been hard at work on postprocessing, a set of features to adjust model predictions. The functionality includes a new package as well as changes across the framework.\n\n\n\n\n\nOct 16, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nA new package for profiling parallel R code\n\n\nPeriodic snapshots of CPU and memory usage consumed by R sessions enable better analyses of parallel computation.\n\n\n\n\n\nJul 15, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow to best parallelize boosted tree model fits with tidymodels\n\n\nShould tidymodels users use the parallelism implementations from XGBoost and LightGBM?\n\n\n\n\n\nMay 13, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring elapsed time to fit with tidymodels\n\n\nThe development versions of tidymodels packages now include tools to benchmark training time.\n\n\n\n\n\nApr 8, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRun an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS\n\n\nStarting a containerized Oracle database on an Apple Silicon Mac.\n\n\n\n\n\nMar 14, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse (2023)\n\n\nAll of my friends are posting their Spotify Wrapped, and I‚Äôm jealous.\n\n\n\n\n\nNov 30, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting flight delays with tidymodelsüõ©\n\n\nTrying out a variety of machine learning models to predict flight delays out of Madison, WI.\n\n\n\n\n\nNov 28, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto!\n\n\nI recently rebuilt this site with Quarto.\n\n\n\n\n\nOct 24, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nDown the submodels rabbit hole with tidymodels\n\n\nDebugging something that turned out not to be a bug at all.\n\n\n\n\n\nOct 11, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHiking the 2023 John Muir Trail / N√º√ºm√º Poyo\n\n\nA group of buddies and I walked 242 miles from Mt. Whitney to Yosemite Valley.\n\n\n\n\n\nSep 1, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing model parameters faster with tidymodels\n\n\nA couple small changes can greatly speed up the hyperparameter tuning process with tidymodels.\n\n\n\n\n\nAug 4, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nMoving On From Baltimore\n\n\nAfter 2 years in Baltimore, I recently made the move to Chicago, IL.\n\n\n\n\n\nJul 28, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nThe tidymodels is getting a whole lot faster\n\n\nRecent optimizations have made fits on small datasets much, much snappier.\n\n\n\n\n\nMar 24, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse\n\n\nAll of my friends are posting their Spotify Wrapped, and I‚Äôm jealous.\n\n\n\n\n\nDec 1, 2022\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRedirecting from sub-domains with Netlify\n\n\nAfter merging two websites that deployed from different branches, I had to figure out how to redirect my old blog posts to their new home.\n\n\n\n\n\nJul 20, 2022\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\n{infer} v1.0.0 is on CRAN\n\n\nSome short reflections on working on the {infer} R package.\n\n\n\n\n\nAug 21, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nApplying to the NSF Graduate Research Fellowship (GRFP)\n\n\nA compilation of the advice that made me feel most informed and confident while applying for the NSF Graduate Research Fellowship.\n\n\n\n\n\nAug 2, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPipe-esque Programming with {ggplot2}‚Äôs Plus Operator\n\n\nWriting iterative code with ‚Äò+‚Äô rather than ‚Äò%&gt;%‚Äô was a tough transition my first time around.\n\n\n\n\n\nJun 10, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nBig Things (Developer Documentation pt.¬†4)\n\n\nOn the tension between documenting R packages exhaustively and maintainably.\n\n\n\n\n\nMay 13, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nNaming the Things (Developer Documentation pt.¬†3)\n\n\nWeighing the pros and cons of several possible schemas for naming the core functions in {stacks}.\n\n\n\n\n\nMay 12, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting Things Up (Developer Documentation pt.¬†2)\n\n\nWhy {stacks} requires (at least) four separate functions to build an ensemble model rather than wrapping them all up into one.\n\n\n\n\n\nMay 11, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow {stacks} Came To Be (Developer Documentation pt.¬†1)\n\n\nIntroducing a set of blog posts reflecting on the development process of the {stacks} package.\n\n\n\n\n\nMay 10, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs Next For Me\n\n\n\n\n\n\n\n\nMar 23, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nApplying to Graduate School in Statistics and Biostatistics\n\n\nThere were all sorts of tips and tricks about applying to grad school that I was only able to learn about via office hours, personal meetings, and Twitter DMs, and I thought it would be worth publicly compiling some lessons learned.\n\n\n\n\n\nMar 15, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRunning R Scripts on a Schedule with GitHub Actions\n\n\nSome pointers on running R scripts and committing their results to a GitHub repository on a regular interval using Actions.\n\n\n\n\n\nDec 27, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\n{stacks} v0.1.0 is on CRAN!\n\n\nIntroducing ensemble learning to the tidymodels.\n\n\n\n\n\nNov 30, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Generating PDF Reports with the Tidyverse\n\n\nIn a few different roles over the past few years, I‚Äôve come across the problem of programatically generating some kind of PDF reports from data. Here are some tips/tricks I‚Äôve come across while making that happen.\n\n\n\n\n\nNov 12, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to Tidy Model Stacking\n\n\nModel stacking is an ensembling technique that involves training a model to combine the outputs of many diverse statistical models. The stacks package implements a grammar for tidymodels-aligned model stacking.\n\n\n\n\n\nOct 27, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nThe Blog Thing! We‚Äôre Doing It!\n\n\nFor a while now, I‚Äôve wished I had a place to share some short posts on what I‚Äôm up to related to R, data science, and statistics. I decided I‚Äôd take a Sunday to get this subdomain up and running!\n\n\n\n\n\nOct 11, 2020\n\n\n¬†\n\n\n\n\n\nNo matching items\n\n  \n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html",
    "href": "blog/2025-12-04-local-agents/index.html",
    "title": "Local models are not there (yet)",
    "section": "",
    "text": "I understand the appeal of local models. Using coding agents like Claude Code or Codex, it‚Äôs not difficult to rack up a hundred dollars of usage in the course of a work week. Besides the price, if you‚Äôre working with sensitive IP or confidential data, you need to really believe that providers like Anthropic and OpenAI can be trusted with your data. And then, there‚Äôs evil billionaires. What if you could run models that were nearly as good on your own laptop?\nBecause of this, one of the most commonly asked questions we get about coding agents like Positron Assistant, Databot, or side::kick() is whether they can be used with local models. (For the purposes of this post, when I say ‚Äúlocal model‚Äù, I mean an LLM that can be served on a high-end laptop with decent tokens per second. Think GPT-OSS 20B.) The answer is yes, but also that we don‚Äôt recommend doing so; while you can connect to a local model through an OpenAI API-compatible endpoint with either of those systems, you likely won‚Äôt have a good experience if you do so.\nThis is an argument I‚Äôve tried to make in a paragraph or two many times, but I thought it would be worth fleshing out in more detail. To do so, I put together an LLM evaluation that demonstrate the issues we‚Äôve seen when trying to get these models working."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#the-problem",
    "href": "blog/2025-12-04-local-agents/index.html#the-problem",
    "title": "Local models are not there (yet)",
    "section": "The problem",
    "text": "The problem\nLet‚Äôs consider a table-stakes capability for a coding agent; a simple refactor. Here, when I say ‚Äúcoding agent,‚Äù a mean an LLM that‚Äôs equipped with tools to explore and make changes to your working directory. That could be Claude Code, Codex, Positron Assistant, or side::kick().\nSo, say I have some file tool-fetch_skill.R pulled up that looks like this:\n\nfetch_skill_impl &lt;- function(skill_name, `_intent` = NULL) {\n  skill_path &lt;- find_skill(skill_name)\n\n  if (is.null(skill_path)) {\n    available &lt;- list_available_skills()\n    skill_names &lt;- vapply(available, function(x) x$name, character(1))\n    cli::cli_abort(\n      c(\n        \"Skill {.val {skill_name}} not found.\",\n        \"i\" = \"Available skills: {.val {skill_names}}\"\n      ),\n      call = rlang::caller_env()\n    )\n  }\n\n  skill_content &lt;- readLines(skill_path, warn = FALSE)\n\n  # Remove YAML frontmatter if present\n  content_start &lt;- 1\n  if (length(skill_content) &gt; 0 && skill_content[1] == \"---\") {\n    yaml_end &lt;- which(skill_content == \"---\")\n    if (length(yaml_end) &gt;= 2) {\n      content_start &lt;- yaml_end[2] + 1\n    }\n  }\n\n  skill_text &lt;- paste(\n    skill_content[content_start:length(skill_content)],\n    collapse = \"\\n\"\n  )\n  \n  # ...\n\nI might provide a prompt like the following:\n\nRefactor this code from my project into a helper:\n# Remove YAML frontmatter if present\ncontent_start &lt;- 1\nif (length(skill_content) &gt; 0 && skill_content[1] == \"---\") {\n  yaml_end &lt;- which(skill_content == \"---\")\n  if (length(yaml_end) &gt;= 2) {\n    content_start &lt;- yaml_end[2] + 1\n  }\n}\n\nAt this point, the coding agent needs to do a few things:\n\nFind which file in the project those lines are from by using a text search tool, probably for ‚ÄúRemove YAML frontmatter‚Äù or a different snippet of code.\nRead the file that it discovers.\nApply edits to the file, first by adding a helper function (probably to the end of the file or to a new file in the directory) and then second by replacing the original usage with a call to the helper.\n\nIn practice, that might look something like this:\n\n\n\n\n\n\n\nNote\n\n\n\nIn case this interface is unfamiliar, this is the RStudio IDE running an open source coding agent side::kick(). You‚Äôll need a daily version of the RStudio IDE to get that full-height sidebar.\n\n\nThis is something that a coding agent should get perfect almost always; my initial prompt is a bit more detailed than I‚Äôd usually provide for such a simple task, there‚Äôs no context rot that could affect the agent‚Äôs effectiveness, and the task is a simple one."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#evaluation",
    "href": "blog/2025-12-04-local-agents/index.html#evaluation",
    "title": "Local models are not there (yet)",
    "section": "Evaluation",
    "text": "Evaluation\nUsing the vitals package, I implemented an LLM evaluation where various models are tasked with that refactor multiple times. You can install it with pak::pak(\"simonpcouch/helperbench\").\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(helperbench)\n\nFor a given run, the setup goes like this:\n\nThe side::kick() GitHub repository is copied into a temporary directory.\nAn ellmer chat is provided with the side::kick() prompt and tools that tell it to basically act like a Claude Code- or Codex-flavored agent and is situated in that directory.\nThe agent is provided with the user prompt shown above (‚ÄúCan you refactor this‚Ä¶‚Äù) and allowed to work until completion.\nOnce the agent finishes,\n\nThe eval checks whether the agent actually made any edits to (at least) the relevant file. If it didn‚Äôt, that run is a failure.\nThen, the contents of the resulting R/ directory is copied in place of the R/ directory in another ephemeral copy of the side::kick() GitHub repository, and the package unit tests run. If the unit tests pass, the run is a pass, otherwise it‚Äôs a failure.\n\n\n\nFor each of 9 models, I ran the eval 10 times. The results are combined in this data frame:\n\nhelper_results %&gt;%\n  sample_n(10)\n\n# A tibble: 10 √ó 6\n   model             score type         cost   input output\n   &lt;chr&gt;             &lt;ord&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 GPT-4.1 Mini      C     Budget   0.488    1204578   3944\n 2 Qwen 3 14B        I     Local    0.00207    13533   5025\n 3 GPT OSS 20B       I     Local    0.00386   119159   2069\n 4 GPT-4.1           C     Frontier 1.32      640982   4151\n 5 Claude Haiku 4.5  C     Budget   0.161     151944   1764\n 6 Claude Sonnet 4.5 C     Frontier 0.444     139778   1626\n 7 Gemini Flash 2.5  I     Budget   0.000438     219    149\n 8 Claude Haiku 4.5  C     Budget   0.138     128187   1981\n 9 GPT OSS 20B       I     Local    0.00158    48270    971\n10 GPT OSS 20B       I     Local    0.000354    8811    641\n\n\nEach row is a given run, where score == \"C\" means the run succeeded."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#performance-by-model",
    "href": "blog/2025-12-04-local-agents/index.html#performance-by-model",
    "title": "Local models are not there (yet)",
    "section": "Performance by model",
    "text": "Performance by model\nWe can first plot the proportion of runs that succeeded by model:\n\nhelper_results %&gt;%\n  mutate(\n    correct = score == \"C\",\n    lab = case_when(\n      model %in% c(\"Claude Haiku 4.5\", \"Claude Sonnet 4.5\") ~ \"Anthropic\",\n      model %in% c(\"GPT-4.1 Mini\", \"GPT-4.1\", \"GPT OSS 20B\") ~ \"OpenAI\",\n      model %in% c(\"Gemini Flash 2.5\", \"Gemini Pro 2.5\") ~ \"Google\",\n      TRUE ~ \"Other\"\n    ),\n    color = case_when(\n      lab == \"Anthropic\" ~ \"#CC5500\",\n      lab == \"OpenAI\" ~ \"#ADD8E6\",\n      lab == \"Google\" ~ \"#22B14C\",\n      TRUE ~ \"#CCCCCC\"\n    )\n  ) %&gt;%\n  group_by(model, type, lab, color) %&gt;%\n  summarise(pct_correct = mean(correct) * 100, .groups = \"drop\") %&gt;%\n  mutate(pct_correct = pmax(pct_correct, .5)) %&gt;%\n  arrange(type, pct_correct) %&gt;%\n  mutate(\n    type = factor(type, levels = c(\"Frontier\", \"Budget\", \"Local\")),\n    model = fct_inorder(model)\n  ) %&gt;%\n  ggplot(aes(x = model, y = pct_correct, fill = color)) +\n  geom_col() +\n  scale_fill_identity() +\n  scale_y_continuous(n.breaks = 10, minor_breaks = NULL) +\n  facet_wrap(~type, nrow = 3, dir = \"v\", scales = \"free_y\") +\n  coord_flip() +\n  labs(\n    y = \"Percent Correct\",\n    x = NULL,\n    title = \"Agentic Coding Reliability\",\n    subtitle = \"When choosing models to power coding tools, you get what you pay for.\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = -0.343),\n    plot.subtitle = element_text(hjust = 10, face = \"italic\"),\n    axis.text.y = element_text(angle = 15, hjust = 1)\n  )\n\n\n\n\n\n\n\nThe models labeled ‚ÄúFrontier‚Äù are models that I would actually use as a daily-driver coding assistant in an application like Positron Assistant or side::kick(), one from each of the big three labs. The models labeled ‚ÄúBudget‚Äù are ‚Äúone step below‚Äù from each of the big three AI labs, but do note that there are some significant price differences within that band. These are models that I might temporarily switch to when speed is a priority and I feel confident the quality won‚Äôt be an issue. Finally, the models marked ‚ÄúLocal‚Äù are models that are small enough that I could run them at a reasonable tokens per second on my laptop. To make sure the playing field was otherwise as level as possible, I used chat_openrouter() rather than each of the labs‚Äô APIs directly. In practice, I‚Äôd usually prefer to use the labs‚Äô APIs directly and the local models‚Ä¶ locally.\nThe graph shows that the frontier models complete the refactor successfully almost always. The budget models are spotty. Never once did a model I could run on my laptop complete the refactor successfully.\n\n\n\n\n\n\nNote\n\n\n\nThere are a couple notable model omissions here.\n\nI had trouble getting Gemini 3 Pro working with ellmer through OpenRouter and didn‚Äôt bother to debug.\nI use GPT 4.1 rather than 5, 5.1, or any of the Codex variants of the GPT family. side::kick() has ‚Äúbuilt-in‚Äù thinking and is this most pleasant with non-thinking models; OpenAI (currently, as of December 2025) describes GPT 4.1 as their ‚Äúsmartest non-reasoning model.‚Äù\n\nOn the Local side:\n\nGemma 3 27B was one of my first thoughts, but OpenRouter doesn‚Äôt support tool calling with that model.\nI also had initially ran this against Qwen 3 Coder 30B, and it did surprisingly well (7/10!), but then I realized it‚Äôs too large to run on my M4 Macbook Pro with 48GB memory unless quantized aggressively, which ruins the performance.\n\n\n\nSurprisingly, the failure modes for each of the local models tested here are pretty consistent within-model:\n\nMistral 3.1 24B just (correctly) writes out the steps to make the refactor without calling the needed tools to make it happen.\nGPT OSS 20B attempts to call a few tools and struggles to get the argument formats right before giving up.\nQwen 3 14B tends to declare success too early, writing the helper function correctly but not incorporating it back into the original function.\n\nYou can explore the logs from each of these runs in more detail in the log viewer:"
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#factoring-in-cost",
    "href": "blog/2025-12-04-local-agents/index.html#factoring-in-cost",
    "title": "Local models are not there (yet)",
    "section": "Factoring in cost",
    "text": "Factoring in cost\nAgain, my categorizations of models here aren‚Äôt a given. Within those categories, there are pretty substantial differences in the average cost per run:\n\nhelper_results %&gt;%\n  mutate(\n    correct = score == \"C\",\n    lab = case_when(\n      model %in% c(\"Claude Haiku 4.5\", \"Claude Sonnet 4.5\") ~ \"Anthropic\",\n      model %in% c(\"GPT-4.1 Mini\", \"GPT-4.1\", \"GPT OSS 20B\") ~ \"OpenAI\",\n      model %in% c(\"Gemini Flash 2.5\", \"Gemini Pro 2.5\") ~ \"Google\",\n      TRUE ~ \"Other\"\n    ),\n    color = case_when(\n      lab == \"Anthropic\" ~ \"#CC5500\",\n      lab == \"OpenAI\" ~ \"#ADD8E6\",\n      lab == \"Google\" ~ \"#22B14C\",\n      TRUE ~ \"#b4b4b4ff\"\n    ),\n    color = factor(color, levels = c(\"#CC5500\", \"#22B14C\", \"#ADD8E6\", \"#b4b4b4ff\"))\n  ) %&gt;%\n  group_by(model, type, lab, color) %&gt;%\n  summarise(avg_cost = mean(cost / n(), na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(avg_cost = pmax(avg_cost, .001)) %&gt;%\n  arrange(type, desc(color), avg_cost) %&gt;%\n  mutate(\n    type = factor(type, levels = c(\"Frontier\", \"Budget\", \"Local\")),\n    model = fct_inorder(model)\n  ) %&gt;%\n  ggplot(aes(x = model, y = avg_cost, fill = color)) +\n  geom_col() +\n  scale_fill_identity() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  facet_wrap(~type, nrow = 3, dir = \"v\", scales = \"free_y\") +\n  coord_flip() +\n  labs(\n    y = \"Average Cost Per Run\", \n    x = NULL,\n    title = \"Cost by model\",\n    subtitle = \"There's more to LLM pricing than cost per token.\"\n  ) +\n  theme(\n    legend.position = \"none\", \n    plot.title = element_text(hjust = -0.1),\n    plot.subtitle = element_text(hjust = -0.25, face = \"italic\"),\n    axis.text.y = element_text(angle = 15, hjust = 1)\n  )\n\n\n\n\n\n\n\nThe bar heights represent the average cost per refactor. So, it costed about 4 cents for Claude Sonnet 4.5 to ‚Äúread‚Äù the side::kick() prompt and tool descriptions, call tools to check out its surroundings, and apply the edits once. Altogether, this eval costed me about $22 to run.\nSince we have a proxy for performance and the cost, we might as well do the performance vs.¬†cost plot:\n\nhelper_results %&gt;%\n  mutate(\n    correct = score == \"C\",\n    lab = case_when(\n      model %in% c(\"Claude Haiku 4.5\", \"Claude Sonnet 4.5\") ~ \"Anthropic\",\n      model %in% c(\"GPT-4.1 Mini\", \"GPT-4.1\", \"GPT OSS 20B\") ~ \"OpenAI\",\n      model %in% c(\"Gemini Flash 2.5\", \"Gemini Pro 2.5\") ~ \"Google\",\n      TRUE ~ \"Other\"\n    ),\n    color = case_when(\n      lab == \"Anthropic\" ~ \"#CC5500\",\n      lab == \"OpenAI\" ~ \"#ADD8E6\",\n      lab == \"Google\" ~ \"#22B14C\",\n      TRUE ~ \"#b4b4b4ff\"\n    )\n  ) %&gt;%\n  group_by(model, type, lab, color) %&gt;%\n  summarise(\n    pct_correct = mean(correct) * 100,\n    avg_cost = mean(cost / n(), na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    type = factor(type, levels = c(\"Frontier\", \"Budget\", \"Local\"))\n  ) %&gt;%\n  ggplot(aes(x = avg_cost, y = pct_correct, color = color)) +\n  geom_point(alpha = 0.8) +\n  geom_text_repel(aes(label = model), size = 3.5, show.legend = FALSE) +\n  scale_color_identity() +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_size_identity() +\n  labs(\n    x = \"Average Cost Per Run\",\n    y = \"Percent Correct\",\n    title = \"Refactoring reliability by price\",\n    subtitle = \"Cost per token is very different than cost per success.\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.subtitle = element_text(face = \"italic\")\n  )\n\n\n\n\n\n\n\nNote that the proportions in pricing don‚Äôt map directly to the cost per token. The cost per token is different than the cost per attempt, and very different than the cost per success. Models that ‚Äúthink‚Äù for a while before acting will use up more tokens than models that don‚Äôt, and models that struggle to use tools fluently can use many more tokens in their attempts to resolve tool errors."
  },
  {
    "objectID": "blog/2025-12-04-local-agents/index.html#conclusion",
    "href": "blog/2025-12-04-local-agents/index.html#conclusion",
    "title": "Local models are not there (yet)",
    "section": "Conclusion",
    "text": "Conclusion\nI don‚Äôt really enjoy telling folks that they ought not to use local models. Again, the advantages are clear; free (as in, the cost of electricity to charge your laptop), perfect privacy, no mega-corporations involved. In the medium run (1-2 years?), I‚Äôd love for it to be the case that you can run a Claude Sonnet 4-ish model on a base Macbook Pro, and I think that‚Äôs a plausible future. For now, though, if you‚Äôd like to get value out of coding agents, you‚Äôll need to opt for the most capable models out there."
  }
]