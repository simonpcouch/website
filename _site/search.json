[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Some Pitters and Patters with R",
    "section": "",
    "text": "chores 0.3.0 and local LLMs\n\n\nThere are now small models capable of powering chores helpers.\n\n\n\n\n\nDec 10, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nLocal models are not there (yet)\n\n\nLLMs that can run on your laptop are not yet capable enough to drive coding agents.\n\n\n\n\n\nDec 4, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my music listening data with Databot\n\n\nIt‚Äôs Spotify Wrapped season.\n\n\n\n\n\nDec 3, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhen plotting, LLMs see what they expect to see\n\n\nData science agents need to accurately read plots even when the content contradicts their expectations.\n\n\n\n\n\nNov 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nside::kick(), a coding agent for RStudio\n\n\nOpen-source AI for RStudio, built entirely in R.\n\n\n\n\n\nNov 11, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nI‚Äôm‚Ä¶ writing a newsletter?\n\n\nSara Altman and I have been working on a newsletter focused on AI.\n\n\n\n\n\nOct 8, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nI was wrong about tidymodels and LLMs\n\n\nToday‚Äôs frontier LLMs know much more about tidymodels than I thought.\n\n\n\n\n\nAug 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nR and the Model Context Protocol\n\n\nThe newly released mcptools package makes coding assistants better at writing R code and applications built with ellmer more powerful.\n\n\n\n\n\nAug 14, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow I‚Äôm using Claude Code to write R code\n\n\nGiving Claude Code the ability to peruse R package documentation via MCP is so, so helpful.\n\n\n\n\n\nJul 17, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nKimi K2 and R Coding\n\n\nAn open-weights model released over the weekend by a little-known company has drawn quite a bit of attention. Is it any good?\n\n\n\n\n\nJul 14, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nClaude 4 and R Coding\n\n\nEvaluating the most recent releases of Claude Sonnet and Opus on challenging R coding problems.\n\n\n\n\n\nMay 27, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Gemini 2.5 Flash on R coding tasks\n\n\nGoogle‚Äôs Gemini 2.5 Pro release really made a splash last month. They‚Äôve just announced an update to 2.5 Flash, a faster and cheaper model.\n\n\n\n\n\nMay 21, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the new Gemini 2.5 Pro update on R coding\n\n\nThe initial Gemini 2.5 Pro release a month ago was surprisingly strong, so I was excited to benchmark the update announced yesterday on R coding problems.\n\n\n\n\n\nMay 7, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating o3 and o4-mini on R coding performance\n\n\nWith a pair of coding-focused model drops, it‚Äôs clear OpenAI is aiming for the developer market. How well do the newest o3 and o4-mini perform on R coding tasks?\n\n\n\n\n\nApr 18, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow good are the GPT 4.1 models at writing R code?\n\n\nOpenAI dropped a trio of models yesterday that they claim result from ‚Äúfocusing closely on real-world developer needs.‚Äù I was curious whether this might mean stronger R coding performance.\n\n\n\n\n\nApr 15, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing chores\n\n\nA package to help with repetitive but hard-to-automate data science tasks.\n\n\n\n\n\nApr 11, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow Good Is Gemini 2.5 Pro at Writing R Code?\n\n\nSince Gemini 2.5 Pro‚Äôs release last week, I‚Äôve been seeing a lot of hype claiming that the model is the new state of the art. How well does it know R?\n\n\n\n\n\nApr 2, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow I‚Äôm Using Claude Code to Develop R Packages\n\n\nI‚Äôve come to really appreciate having Claude Code as part of my LLM toolkit, but it‚Äôs taken a bit of getting used to.\n\n\n\n\n\nMar 26, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements\n\n\nWhat happens when you ask language models to evaluate their own output?\n\n\n\n\n\nJan 30, 2025\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhich names that are also names of countries are most common?\n\n\nSitting around the living room over the weekend, someone asked this question. Let‚Äôs poke at an answer.\n\n\n\n\n\nDec 23, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing a new book\n\n\nToday, during R/Pharma 2024, I open-sourced the draft of a book I‚Äôve been working on.\n\n\n\n\n\nOct 29, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPostprocessing is coming to tidymodels\n\n\nThe tidymodels team has been hard at work on postprocessing, a set of features to adjust model predictions. The functionality includes a new package as well as changes across the framework.\n\n\n\n\n\nOct 16, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nA new package for profiling parallel R code\n\n\nPeriodic snapshots of CPU and memory usage consumed by R sessions enable better analyses of parallel computation.\n\n\n\n\n\nJul 15, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow to best parallelize boosted tree model fits with tidymodels\n\n\nShould tidymodels users use the parallelism implementations from XGBoost and LightGBM?\n\n\n\n\n\nMay 13, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring elapsed time to fit with tidymodels\n\n\nThe development versions of tidymodels packages now include tools to benchmark training time.\n\n\n\n\n\nApr 8, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRun an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS\n\n\nStarting a containerized Oracle database on an Apple Silicon Mac.\n\n\n\n\n\nMar 14, 2024\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse (2023)\n\n\nAll of my friends are posting their Spotify Wrapped, and I‚Äôm jealous.\n\n\n\n\n\nNov 30, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting flight delays with tidymodelsüõ©\n\n\nTrying out a variety of machine learning models to predict flight delays out of Madison, WI.\n\n\n\n\n\nNov 28, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto!\n\n\nI recently rebuilt this site with Quarto.\n\n\n\n\n\nOct 24, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nDown the submodels rabbit hole with tidymodels\n\n\nDebugging something that turned out not to be a bug at all.\n\n\n\n\n\nOct 11, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHiking the 2023 John Muir Trail / N√º√ºm√º Poyo\n\n\nA group of buddies and I walked 242 miles from Mt. Whitney to Yosemite Valley.\n\n\n\n\n\nSep 1, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing model parameters faster with tidymodels\n\n\nA couple small changes can greatly speed up the hyperparameter tuning process with tidymodels.\n\n\n\n\n\nAug 4, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nMoving On From Baltimore\n\n\nAfter 2 years in Baltimore, I recently made the move to Chicago, IL.\n\n\n\n\n\nJul 28, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nThe tidymodels is getting a whole lot faster\n\n\nRecent optimizations have made fits on small datasets much, much snappier.\n\n\n\n\n\nMar 24, 2023\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse\n\n\nAll of my friends are posting their Spotify Wrapped, and I‚Äôm jealous.\n\n\n\n\n\nDec 1, 2022\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRedirecting from sub-domains with Netlify\n\n\nAfter merging two websites that deployed from different branches, I had to figure out how to redirect my old blog posts to their new home.\n\n\n\n\n\nJul 20, 2022\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\n{infer} v1.0.0 is on CRAN\n\n\nSome short reflections on working on the {infer} R package.\n\n\n\n\n\nAug 21, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nApplying to the NSF Graduate Research Fellowship (GRFP)\n\n\nA compilation of the advice that made me feel most informed and confident while applying for the NSF Graduate Research Fellowship.\n\n\n\n\n\nAug 2, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nPipe-esque Programming with {ggplot2}‚Äôs Plus Operator\n\n\nWriting iterative code with ‚Äò+‚Äô rather than ‚Äò%&gt;%‚Äô was a tough transition my first time around.\n\n\n\n\n\nJun 10, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nBig Things (Developer Documentation pt.¬†4)\n\n\nOn the tension between documenting R packages exhaustively and maintainably.\n\n\n\n\n\nMay 13, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nNaming the Things (Developer Documentation pt.¬†3)\n\n\nWeighing the pros and cons of several possible schemas for naming the core functions in {stacks}.\n\n\n\n\n\nMay 12, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting Things Up (Developer Documentation pt.¬†2)\n\n\nWhy {stacks} requires (at least) four separate functions to build an ensemble model rather than wrapping them all up into one.\n\n\n\n\n\nMay 11, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nHow {stacks} Came To Be (Developer Documentation pt.¬†1)\n\n\nIntroducing a set of blog posts reflecting on the development process of the {stacks} package.\n\n\n\n\n\nMay 10, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs Next For Me\n\n\n\n\n\n\n\n\nMar 23, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nApplying to Graduate School in Statistics and Biostatistics\n\n\nThere were all sorts of tips and tricks about applying to grad school that I was only able to learn about via office hours, personal meetings, and Twitter DMs, and I thought it would be worth publicly compiling some lessons learned.\n\n\n\n\n\nMar 15, 2021\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nRunning R Scripts on a Schedule with GitHub Actions\n\n\nSome pointers on running R scripts and committing their results to a GitHub repository on a regular interval using Actions.\n\n\n\n\n\nDec 27, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\n{stacks} v0.1.0 is on CRAN!\n\n\nIntroducing ensemble learning to the tidymodels.\n\n\n\n\n\nNov 30, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Generating PDF Reports with the Tidyverse\n\n\nIn a few different roles over the past few years, I‚Äôve come across the problem of programatically generating some kind of PDF reports from data. Here are some tips/tricks I‚Äôve come across while making that happen.\n\n\n\n\n\nNov 12, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to Tidy Model Stacking\n\n\nModel stacking is an ensembling technique that involves training a model to combine the outputs of many diverse statistical models. The stacks package implements a grammar for tidymodels-aligned model stacking.\n\n\n\n\n\nOct 27, 2020\n\n\n¬†\n\n\n\n\n\n\n\n\n\n\n\n\nThe Blog Thing! We‚Äôre Doing It!\n\n\nFor a while now, I‚Äôve wished I had a place to share some short posts on what I‚Äôm up to related to R, data science, and statistics. I decided I‚Äôd take a Sunday to get this subdomain up and running!\n\n\n\n\n\nOct 11, 2020\n\n\n¬†\n\n\n\n\n\nNo matching items\n\n  \n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Simon P. Couch",
    "section": "",
    "text": "I appreciate you dropping by.\n\n\nMy name‚Äôs Simon. I build tools for data scientists at Posit (formerly called RStudio).\nThroughout my career so far, I‚Äôve had a deep and enduring interest in statistical software development‚ÄîI think, done well, statistical software has an incredible impact on our ability to think intuitively about statistics and data science.\nI worked on open source R packages for data analysis and machine for a number of years before transitioning to focusing on LLMs in late 2024. Since then, I‚Äôve pushed the boundaries of LLM tooling for data scientists and developed a number of coding agents. My GitHub profile links out to some of the projects I spend my time with.\nI‚Äôm currently based out of Chicago, IL. When I‚Äôm not working, I enjoy cooking, hanging with my dog Millie, and playing American folk music.\nThis site is built with Quarto, and I‚Äôve repo-dived on Silvia Canel√≥n, Julia Silge, and Emil Hvitfeldt‚Äôs personal websites while tweaking and troubleshooting. The site is deployed using Netlify. My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html",
    "href": "blog/2025-12-10-chores-0-3-0/index.html",
    "title": "chores 0.3.0 and local LLMs",
    "section": "",
    "text": "The tl;dr:"
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#the-chores-package",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#the-chores-package",
    "title": "chores 0.3.0 and local LLMs",
    "section": "The chores package",
    "text": "The chores package\nThe chores package provides a library of ergonomic LLM assistants designed to help you complete repetitive, hard-to-automate tasks quickly. After selecting some code, you can press a keyboard shortcut, select a helper that corresponds to a system prompt, and watch your code be rewritten.\n\nWhen you select some code and a chore helper, what‚Äôs happening under the hood is that the package first retrieves the prompt corresponding the system prompt you chose. For example, the prompt for templating out roxygen2 function documentation looks like this:\n\n\nTemplating function documentation\nYou are a terse assistant designed to help R package developers quickly template out their function documentation using roxygen2. Given some highlighted function code, return minimal documentation on the function‚Äôs parameters and return type. Beyond those two elements, be sparing so as not to describe things you don‚Äôt have context for. Respond with only R #' roxygen2 comments‚Äîno backticks or newlines around the response, no further commentary.\nFor function parameters in @params, describe each according to their type (e.g.¬†‚ÄúA numeric vector‚Äù or ‚ÄúA single string‚Äù) and note if the parameter isn‚Äôt required by writing ‚ÄúOptional‚Äù if it has a default value. If the parameters have a default enum (e.g.¬†arg = c(\"a\", \"b\", \"c\")), write them out as ‚Äòone of \"a\", \"b\", or \"c\".‚Äô If there are ellipses in the function signature, note what happens to them. If they‚Äôre checked with rlang::check_dots_empty() or otherwise, document them as ‚ÄúCurrently unused; must be empty.‚Äù If the ellipses are passed along to another function, note which function they‚Äôre passed to.\nFor the return type in @returns, note any important errors or warnings that might occur and under what conditions. If the output is returned with invisible(output), note that it‚Äôs returned ‚Äúinvisibly.‚Äù\nHere are some examples:\nGiven:\nkey_get &lt;- function(name, error_call = caller_env()) {\n  val &lt;- Sys.getenv(name)\n  if (!identical(val, \"\")) {\n    val\n  } else {\n    if (is_testing()) {\n      testthat::skip(sprintf(\"%s env var is not configured\", name))\n    } else {\n      cli::cli_abort(\"Can't find env var {.code {name}}.\", call = error_call)\n    }\n  }\n}\nReply with:\n#' Get key\n#'\n#' @description\n#' A short description...\n#' \n#' @param name A single string.\n#' @param error_call A call to mention in error messages. Optional.\n#'\n#' @returns \n#' If found, the value corresponding to the provided `name`. Otherwise,\n#' the function will error.\n#'\n#' @export\nGiven:\nchat_perform &lt;- function(provider,\n                         mode = c(\"value\", \"stream\", \"async-stream\", \"async-value\"),\n                         turns,\n                         tools = list(),\n                         extra_args = list()) {\n\n  mode &lt;- arg_match(mode)\n  stream &lt;- mode %in% c(\"stream\", \"async-stream\")\n\n  req &lt;- chat_request(\n    provider = provider,\n    turns = turns,\n    tools = tools,\n    stream = stream,\n    extra_args = extra_args\n  )\n\n  switch(mode,\n    \"value\" = chat_perform_value(provider, req),\n    \"stream\" = chat_perform_stream(provider, req),\n    \"async-value\" = chat_perform_async_value(provider, req),\n    \"async-stream\" = chat_perform_async_stream(provider, req)\n  )\n}\nReply with:\n#' Perform chat\n#'\n#' @description\n#' A short description...\n#' \n#' @param provider A provider.\n#' @param mode One of `\"value\"`, `\"stream\"`, `\"async-stream\"`, or `\"async-value\"`.\n#' @param turns Turns.\n#' @param tools Optional. A list of tools.\n#' @param extra_args Optional. A list of extra arguments.\n#'\n#' @returns \n#' A result.\n#'\n#' @export\nGiven:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    cli::cli_abort(\"Function must have at least two arguments.\", .internal = TRUE)\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\nReply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\nWhen two functions are supplied, only provide documentation for the first function, only making use of later functions as additional context. For example:\nGiven:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    error_less_than_two_args()\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\n\nerror_less_than_two_args &lt;- function(call = caller_env()) {\n  cli::cli_abort(\"Function must have at least two arguments.\", call = call, .internal = TRUE)\n}\nReply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\n\n\nThen, the selected helper prompt is set as the system prompt and the code you selected is set as the user prompt in a call to ellmer::Chat(). It looks something like this:\n\nlibrary(ellmer)\n\nch &lt;- chat_anthropic(system_prompt = the_prompt_from_above)\n \nch$chat(\"&lt;the code you selected&gt;\")\n#&gt; #' The documentation for the selected code.\n#&gt; #' \n#&gt; #' Yada yada yada."
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#choosing-a-model",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#choosing-a-model",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Choosing a model",
    "text": "Choosing a model\nThe chores package allows you to use any model you can connect to with ellmer. So, how do you choose which one to use?\nThe model powering chores needs the following characteristics:\n\nStrict instruction-following: Looking back at that roxygen prompt, those instructions make two asks of LLMs that are pretty difficult for models that have been trained so strictly into the ‚Äúhelpful assistant‚Äù role: no exposition or explanatory text before or after the roxygen comments, and no (triple) backticks around the response. The chores package writes the LLMs‚Äô output directly to the source file, so it‚Äôs really frustrating when models provide any text other than what‚Äôs requested.\nMinimally- or non-thinking: Thinking adds latency and shouldn‚Äôt be necessary to complete these tasks. There are many interfaces where thinking is nice and/or necessary, but this isn‚Äôt one of them.\n\nNotably, the model does not need the ability to call tools, carry out long-horizon tasks, or be a pleasant conversationalist. It‚Äôs fine if the model used with chores is bad at pretty much everything besides writing syntactically valid code in compliance with the instructions in the provided prompt.\nIn the package documentation, I recommend Claude 3.7 Sonnet and GPT 4.1 (optionally, -mini).1 Up to this point, though, I‚Äôd thought you really had needed to use a frontier-ish model to get any value out of chores. It‚Äôs seemed to me that many of the models that I can currently run on my laptop (up to late 2025) had been trained into the ‚ÄúI‚Äôm a helpful assistant‚Äù persona so strictly‚Äìeven those that are advertised as instruction-tuned‚Äìthat they‚Äôll ramble on and on before and after providing the requested code, even if the provided code is reasonable.\nIn some ways, some time just needed to pass, but I also had overlooked a critical issue (and seemingly everyone else that‚Äôs tried to use chores with local models). In working on another problem, I learned that:\n\nollama‚Äôs and LM Studio‚Äôs default context length is 4,096 tokens, even for models that support much longer context windows, and\nIf you provide a prompt that‚Äôs greater than the size of the context length, it will be truncated to fit inside the length rather than erroring.\n\nI think this probably contributed to my misconception that models small enough to run on my laptop weren‚Äôt capable of powering chores. Once I trimmed the cli helper prompt to fit inside the context window (and/or increased the size of the context length in LM Studio) with some wiggle room, I saw much more promising results on these tasks than I had seen from local models before."
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#a-new-kid-on-the-block",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#a-new-kid-on-the-block",
    "title": "chores 0.3.0 and local LLMs",
    "section": "A new kid on the block",
    "text": "A new kid on the block\nSo, even with the models that I had ollama pulled a few months ago, I realized we were already closer to local models powering chores than I had thought. At that point, I wondered what the newest releases were that might show stronger performance. In particular, I‚Äôve been pretty amazed by several of the Qwen3 models, so I started there.\nAfter some pokings-around, I think that Qwen3 4B Instruct 2507 is a great model for local use with chores. Here‚Äôs a real-time (as in, not sped up) demo of that model in action on my M4 Macbook:\n\nThe cli refactor is a little bit wonky; the model chose to use backticks around the env var markup rather than curly braces, which won‚Äôt render correctly. The templating for the roxygen2 documentation is totally reasonable. Performance isn‚Äôt quite Claude 3.7 Sonnet, but I‚Äôm pretty blown away by how good it is.\nIf you‚Äôre interested in trying this model out, you can either use LM Studio or ollama.\nOn Apple Silicon (Mac M-series), I recommend LM Studio; LM Studio supports MLX, an array framework for Apple Silicon, which helps the model run much more quickly than with ollama. Click ‚ÄúDiscover‚Äù, search ‚ÄúQwen3 4B Instruct 2507‚Äù, and click ‚ÄúDownload.‚Äù Once downloaded, click the ‚ÄúDeveloper‚Äù tab and change the Status from Stopped to Running. Then, in R, configure chores with:\nqwen3_4b &lt;- ellmer::chat_openai_compatible(\n  base_url = \"http://127.0.0.1:1234/v1\",\n  model = \"qwen/qwen3-4b-2507\"\n)\n\noptions(chores.chat = qwen3_4b)\nNote the /v1 in the base URL; this will hit LM Studio‚Äôs OpenAI API completions v1 endpoint.\nOn other systems, you could also use Ollama. Run ollama pull qwen3:4b-instruct at the terminal, then set options(chores.chat = ellmer::chat_ollama(model = \"qwen3:4b-instruct\")).\nAt least with MLX on LM Studio, the model takes up 2.5GB of disk space and requires 2.5GB of RAM to run."
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#oh-and-chores",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#oh-and-chores",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Oh, and chores",
    "text": "Oh, and chores\nFor a good experience ‚Äúby default‚Äù (i.e.¬†without the need to change the context length in LM Studio to use the default helpers), install the new release of chores with install.packages(\"chores\")!"
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#footnotes",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#footnotes",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Footnotes",
    "text": "Footnotes\n\nNotably, I do not currently recommend Claude 4 Sonnet, Claude 4.5 Sonnet, or Claude 4.5. Haiku. The newer Claude models tend to include triple backticks in their responses even when prompted not to.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/2025-12-10-chores-0-3-0/index.html#oh-and-chores-0.3.0",
    "href": "blog/2025-12-10-chores-0-3-0/index.html#oh-and-chores-0.3.0",
    "title": "chores 0.3.0 and local LLMs",
    "section": "Oh, and chores 0.3.0",
    "text": "Oh, and chores 0.3.0\nFor a good experience ‚Äúby default‚Äù (i.e.¬†without the need to change the context length in LM Studio to use the default helpers), install the new release of chores with install.packages(\"chores\")!"
  }
]