[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "I’ve written a few R packages over the years and have contributed to many others.\nThe CRAN sections below enumerate packages on CRAN. Download statistics are from the Posit mirror of CRAN and are current as of 2025-03-20, star statistics were generated at the same time."
  },
  {
    "objectID": "software.html#cran-packages-maintained-by-me",
    "href": "software.html#cran-packages-maintained-by-me",
    "title": "Software",
    "section": "CRAN Packages Maintained By Me",
    "text": "CRAN Packages Maintained By Me"
  },
  {
    "objectID": "software.html#cran-packages-maintained-by-others",
    "href": "software.html#cran-packages-maintained-by-others",
    "title": "Software",
    "section": "CRAN Packages Maintained By Others",
    "text": "CRAN Packages Maintained By Others\nThese packages are those that I have either originally maintained, significantly contributed to directly, or my code was included enough to be listed as a co-author:\n\n\n\n\n\n\nI’ve also worked on many tidymodels packages that I’m not listed as an author on."
  },
  {
    "objectID": "software.html#github-only-packages",
    "href": "software.html#github-only-packages",
    "title": "Software",
    "section": "GitHub-Only Packages",
    "text": "GitHub-Only Packages\nI’ve also prototyped some other packages that haven’t made it to CRAN.\n\n\n\n\n\n\n\nThe source code from this page was adapted from Max Kuhn’s website."
  },
  {
    "objectID": "blog/2024-05-13-parallel/index.html",
    "href": "blog/2024-05-13-parallel/index.html",
    "title": "How to best parallelize boosted tree model fits with tidymodels",
    "section": "",
    "text": "The XGBoost and LightGBM modeling engines both enable distributing the computations needed to train a single boosted tree model across several CPU cores. Similarly, the tidymodels framework enables distributing model fits across cores. The natural question, then, is whether tidymodels users ought to make use of the engine’s parallelism implementation, tidymodels’ implementation, or both at the same time. This blog post is a scrappy attempt at finding which of those approaches will lead to the smallest elapsed time when fitting many models."
  },
  {
    "objectID": "blog/2024-05-13-parallel/index.html#the-problem",
    "href": "blog/2024-05-13-parallel/index.html#the-problem",
    "title": "How to best parallelize boosted tree model fits with tidymodels",
    "section": "The problem",
    "text": "The problem\nFor example, imagine the case where we evaluate a single model against 10 resamples. Doing so sequentially might look something like this, where each dotted orange segment indicates a model fit:\n\n\n\n\n\n\n\n\nThe x-axis here depicts time. The short green segments on either side of the orange segments indicate the portions of the elapsed time allotted to tidymodels “overhead,” like checking arguments and combining results. This graphic depicts a sequential series of model fits, where each fit takes place one after the other.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re feeling lost already, a previous blog post of mine on how we think about optimizing our code in tidymodels might be helpful.\n\n\n1) Use the engine’s parallelism implementation only.\nThe XGBoost and LightGBM engines implement their own parallelism frameworks such that a single model fit can be distributed across many cores. If we distribute a single model fit’s computations across 5 cores, we could see, best-case, a 5-fold speedup in the time to fit each model. The model fits still happen in order, but each individual (hopefully) happens much quicker, resulting in a shorter overall time:\n\n\n\n\n\n\n\n\nThe increased height of each segment representing a model fit represents how the computations for each model fit are distributed across multiple CPU cores. (I don’t know. There’s probably a better way to depict that.)\n2) Use tidymodels’ parallelism implementation only.\nThe tidymodels framework supports distributing model fits across CPU cores in the sense that, when fitting n models across m CPU cores, tidymodels can allot each core to fit n/m of the models. In the case of 10 models across 5 cores, then, each core takes care of fitting two:\n\n\n\n\n\n\n\n\nNote that a given model fit happens on a single core, so the time to fit a single model stays the same.\n3) Use both the engine’s and tidymodels’ parallelism implementation.\nWhy can’t we do both 1) and 2)? If both parallelism approaches play nicely with each other, and neither of them was able to perfectly distribute its computations across all of the available resources, then we’d see that we could get some of the benefits from both approach and get the maximal computational performance out of our available resources:\n\n\n\n\n\n\n\n\nIn reality, parallelism frameworks come with their fair share of overhead, and often don’t play nicely with each other. It’d be nice to know if, in practice, any of these three approaches stand out among the others as the most performant way to resample XGBoost and LightGBM models with tidymodels. We’ll simulate some data and run some quick benchmarks to get some intuition about how to best parallelize parameter tuning with tidymodels.\nThis post is based on a similar idea to an Applied Predictive Modeling blog post from Max Kuhn in 2018, but is generally:\n\nless refined (Max tries out many different dataset sizes on three different operating systems, while I fix both of those variables here),\nuses modern implementations, incl. tidymodels instead of caret, future instead of foreach, and updated XGBoost (and LightGBM) package versions, and\nhappens to be situated in a modeling context more similar to one that I’m currently benchmarking for another project.\n\nI’m running this experiment on an M1 Pro Macbook Pro with 32GB of RAM and 10 cores, running MacOS Sonoma 14.4.1. We’ll create a 10,000-row dataset and partition it into 10 folds for cross-validation, tuning among a set of 10 possible candidate values, resulting in 100 9,000-row model fits per call to tune_grid()."
  },
  {
    "objectID": "blog/2024-05-13-parallel/index.html#setup",
    "href": "blog/2024-05-13-parallel/index.html#setup",
    "title": "How to best parallelize boosted tree model fits with tidymodels",
    "section": "Setup",
    "text": "Setup\nStarting off by loading needed packages and simulating some data using the sim_classification() function from modeldata:\n\nlibrary(tidymodels)\nlibrary(bonsai)\nlibrary(future)\n\nset.seed(1)\ndat &lt;- sim_classification(1e4)\n\nWe’d like to predict class using the rest of the variables in the dataset:\n\ndat\n\n# A tibble: 10,000 × 16\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   &lt;fct&gt;          &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 class_2       -0.329       -1.28         0.186       0.578         0.732\n 2 class_2        0.861       -0.389        0.106       0.701         0.647\n 3 class_2       -0.461       -1.69         0.193       0.337         0.814\n 4 class_2        2.75         1.35        -0.215       0.119         0.104\n 5 class_2        0.719        0.127       -0.479       0.878         0.334\n 6 class_2       -0.743       -1.36         0.480       0.0517        0.400\n 7 class_2        0.805        0.447       -0.947       0.342         0.382\n 8 class_2        0.669        1.23         0.682       0.461         0.924\n 9 class_2        0.887        0.593        0.701       0.772         0.297\n10 class_1       -1.14         0.353       -0.729       0.819         0.331\n# ℹ 9,990 more rows\n# ℹ 10 more variables: linear_01 &lt;dbl&gt;, linear_02 &lt;dbl&gt;, linear_03 &lt;dbl&gt;,\n#   linear_04 &lt;dbl&gt;, linear_05 &lt;dbl&gt;, linear_06 &lt;dbl&gt;, linear_07 &lt;dbl&gt;,\n#   linear_08 &lt;dbl&gt;, linear_09 &lt;dbl&gt;, linear_10 &lt;dbl&gt;\n\nform &lt;- class ~ .\n\nSplitting the data into training and testing sets before making a 10-fold cross-validation object:\n\nset.seed(1)\ndat_split &lt;- initial_split(dat)\ndat_train &lt;- training(dat_split)\ndat_test &lt;- testing(dat_split)\ndat_folds &lt;- vfold_cv(dat_train)\n\ndat_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [6750/750]&gt; Fold01\n 2 &lt;split [6750/750]&gt; Fold02\n 3 &lt;split [6750/750]&gt; Fold03\n 4 &lt;split [6750/750]&gt; Fold04\n 5 &lt;split [6750/750]&gt; Fold05\n 6 &lt;split [6750/750]&gt; Fold06\n 7 &lt;split [6750/750]&gt; Fold07\n 8 &lt;split [6750/750]&gt; Fold08\n 9 &lt;split [6750/750]&gt; Fold09\n10 &lt;split [6750/750]&gt; Fold10\n\n\nFor both XGBoost and LightGBM, we’ll only tune the learning rate and number of trees.\n\nspec_bt &lt;-\n  boost_tree(learn_rate = tune(), trees = tune()) %&gt;%\n  set_mode(\"classification\")\n\nThe trees parameter greatly affects the time to fit a boosted tree model. Just to be super sure that analogous fits are happening in each of the following tune_grid() calls, we’ll create the grid of possible parameter values beforehand and pass it to each tune_grid() call.\n\nset.seed(1)\n\ngrid_bt &lt;-\n  spec_bt %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_latin_hypercube(size = 10)\n\ngrid_bt\n\n# A tibble: 10 × 2\n   trees learn_rate\n   &lt;int&gt;      &lt;dbl&gt;\n 1  1175    0.173  \n 2   684    0.0817 \n 3   558    0.00456\n 4  1555    0.0392 \n 5   861    0.00172\n 6  1767    0.00842\n 7  1354    0.0237 \n 8    42    0.0146 \n 9   241    0.00268\n10  1998    0.222  \n\n\nFor both LightGBM and XGBoost, we’ll test each of those three approaches. I’ll write out the explicit code I use to time each of these computations; note that the only thing that changes in each of those chunks is the parallelism setup code and the arguments to set_engine().\n\n\n\n\n\n\nHomework📄\n\n\n\nWrite a function that takes in a parallelism setup and engine and returns a timing like those below!😉 Make sure to “tear down” the parallelism setup after.\n\n\nFor a summary of those timings, see Section 5."
  },
  {
    "objectID": "blog/2024-05-13-parallel/index.html#xgboost",
    "href": "blog/2024-05-13-parallel/index.html#xgboost",
    "title": "How to best parallelize boosted tree model fits with tidymodels",
    "section": "XGBoost",
    "text": "XGBoost\nFirst, testing 1) engine implementation only, we use plan(sequential) to tell tidymodels’ parallelism framework not to kick in, and set nthread = 10 in set_engine() to tell XGBoost to distribute its computations across 10 cores:\n\nplan(sequential)\n\ntiming_xgb_1 &lt;- system.time({\n  res &lt;-\n    tune_grid(\n      spec_bt %&gt;% set_engine(\"xgboost\", nthread = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n\nNow, for 2) tidymodels implementation only, we use plan(multisession, workers = 10) to tell tidymodels to distribute its computations across cores and set nthread = 1 to disable XGBoost’s parallelization:\n\nplan(multisession, workers = 10)\n\ntiming_xgb_2 &lt;- system.time({\n  res &lt;-\n    tune_grid(\n      spec_bt %&gt;% set_engine(\"xgboost\", nthread = 1),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n\nFinally, for 3) both parallelism implementations, we enable parallelism for both framework:\n\nplan(multisession, workers = 10)\n\ntiming_xgb_3 &lt;- system.time({\n  res &lt;-\n    tune_grid(\n      spec_bt %&gt;% set_engine(\"xgboost\", nthread = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n\nWe’ll now do the same thing for LightGBM."
  },
  {
    "objectID": "blog/2024-05-13-parallel/index.html#lightgbm",
    "href": "blog/2024-05-13-parallel/index.html#lightgbm",
    "title": "How to best parallelize boosted tree model fits with tidymodels",
    "section": "LightGBM",
    "text": "LightGBM\nFirst, testing 1) engine implementation only:\n\nplan(sequential)\n\ntiming_lgb_1 &lt;- system.time({\n  res &lt;-\n    tune_grid(\n      spec_bt %&gt;% set_engine(\"lightgbm\", num_threads = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n\nNow, 2) tidymodels implementation only:\n\nplan(multisession, workers = 10)\n\ntiming_lgb_2 &lt;- system.time({\n  res &lt;-\n    tune_grid(\n      spec_bt %&gt;% set_engine(\"lightgbm\", num_threads = 1),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n\nFinally, 3) both parallelism implementations:\n\nplan(multisession, workers = 10)\n\ntiming_lgb_3 &lt;- system.time({\n  res &lt;-\n    tune_grid(\n      spec_bt %&gt;% set_engine(\"lightgbm\", num_threads = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]"
  },
  {
    "objectID": "blog/2024-05-13-parallel/index.html#sec-putting-it-all-together",
    "href": "blog/2024-05-13-parallel/index.html#sec-putting-it-all-together",
    "title": "How to best parallelize boosted tree model fits with tidymodels",
    "section": "Putting it all together",
    "text": "Putting it all together\nAt a glance, those timings are (in seconds):\n\ntibble(\n  approach = c(\"engine only\", \"tidymodels only\", \"both\"),\n  xgboost = c(timing_xgb_1, timing_xgb_2, timing_xgb_3),\n  lightgbm = c(timing_lgb_1, timing_lgb_2, timing_lgb_3)\n)\n\n\n\n# A tibble: 3 × 3\n  approach        xgboost lightgbm\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n1 engine only      988.46  103.13 \n2 tidymodels only  133.23   23.328\n3 both             132.59   23.279\n\n\nAt least in this context, we see:\n\nUsing only the engine’s parallelization results in a substantial slowdown for both engines.\nFor both XGBoost and LightGBM, just using the tidymodels parallelization vs. combining the tidymodels and engine parallelization seem comparable in terms of timing. (This is nice to see in the sense that users don’t need to adjust their tidymodels parallelism configuration just to fit this particular kind of model; if they have a parallelism configuration set up already, it won’t hurt to keep it around.)\nLightGBM models train quite a bit faster than XGBoost models, though we can’t meaningfully compare those fit times without knowing whether performance metrics are comparable.\n\nThese are similar conclusions to what Max observes in the linked APM blog post. A few considerations that, in this context, may have made tidymodels’ parallelization seem extra advantageous:\n\nWe’re resampling across 10 folds here and, conveniently, distributing those computations across 10 cores. That is, each core is (likely) responsible for the fits on just one fold, and there are no cores “sitting idle” unless one model fit finishes well before than another.\nWe’re resampling models rather than just fitting one model. If we had just fitted one model, tidymodels wouldn’t offer any support for distributing computations across cores, but this is exactly what XGBoost and LightGBM support.\nWhen using tidymodels’ parallelism implementation, it’s not just the model fits that are distributed across cores. Preprocessing, prediction, and metric calculation is also distributed across cores when using tidymodels’ parallelism implementation. (Framed in the context of the diagrams above, there are little green segments dispersed throughout the orange ones that can be parallelized.)"
  },
  {
    "objectID": "blog/2024-05-13-parallel/index.html#session-info",
    "href": "blog/2024-05-13-parallel/index.html#session-info",
    "title": "How to best parallelize boosted tree model fits with tidymodels",
    "section": "Session Info",
    "text": "Session Info\n\nsessioninfo::session_info(\n  c(tidymodels_packages(), \"xgboost\", \"lightgbm\"),\n  dependencies = FALSE\n)\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29)\n os       macOS Sonoma 14.4.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Chicago\n date     2024-05-13\n pandoc   3.1.12.3 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version     date (UTC) lib source\n broom        * 1.0.5.9000  2024-05-09 [1] Github (tidymodels/broom@b984deb)\n cli            3.6.2       2023-12-11 [1] CRAN (R 4.3.1)\n conflicted     1.2.0       2023-02-01 [1] CRAN (R 4.3.0)\n dials        * 1.2.1       2024-02-22 [1] CRAN (R 4.3.1)\n dplyr        * 1.1.4       2023-11-17 [1] CRAN (R 4.3.1)\n ggplot2      * 3.5.1       2024-04-23 [1] CRAN (R 4.3.1)\n hardhat        1.3.1.9000  2024-04-26 [1] Github (tidymodels/hardhat@ae8fba7)\n infer        * 1.0.6.9000  2024-03-25 [1] local\n lightgbm       4.3.0       2024-01-18 [1] CRAN (R 4.3.1)\n modeldata    * 1.3.0       2024-01-21 [1] CRAN (R 4.3.1)\n parsnip      * 1.2.1.9001  2024-05-09 [1] Github (tidymodels/parsnip@320affd)\n purrr        * 1.0.2       2023-08-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.10.9000 2024-04-08 [1] Github (tidymodels/recipes@63ced27)\n rlang          1.1.3       2024-01-10 [1] CRAN (R 4.3.1)\n rsample      * 1.2.1       2024-03-25 [1] CRAN (R 4.3.1)\n rstudioapi     0.16.0      2024-03-24 [1] CRAN (R 4.3.1)\n tibble       * 3.2.1       2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.2.0       2024-03-25 [1] CRAN (R 4.3.1)\n tidyr        * 1.3.1       2024-01-24 [1] CRAN (R 4.3.1)\n tune         * 1.2.1       2024-04-18 [1] CRAN (R 4.3.1)\n workflows    * 1.1.4.9000  2024-05-01 [1] local\n workflowsets * 1.1.0       2024-03-21 [1] CRAN (R 4.3.1)\n xgboost        1.7.7.1     2024-01-25 [1] CRAN (R 4.3.1)\n yardstick    * 1.3.1       2024-03-21 [1] CRAN (R 4.3.1)\n\n [1] /Users/simoncouch/Library/R/arm64/4.3/library\n [2] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "blog/2023-09-01-jmt-2023/index.html",
    "href": "blog/2023-09-01-jmt-2023/index.html",
    "title": "Hiking the 2023 John Muir Trail / Nüümü Poyo",
    "section": "",
    "text": "The John Muir Trail (JMT) is a backpacking trail through California’s Sierra Nevada. For folks heading northbound, the trail begins at the top of Mt. Whitney—the tallest mountain in the contiguous US—ascending 8 mountain passes over 214 miles to Yosemite Valley. The JMT’s route traces portions of Nüümü Poyo, or the People’s Trail: historically significant trade routes maintained by people indigenous to what is now California.\nA group of buddies and I took on the JMT this August. 2023 saw record snow pack in the Sierra Nevada; measures vary, but snowfall was well over twice that of an average year. Usually melted out by the end of July, many of the mountain passes we’d come across still held plenty of snow, and were rapidly melting to make for intense and plentiful river crossings. The record snowpack had also caused the partial collapse of the South Fork San Joaquin River bridge, introducing logistical uncertainty for the JMT class of 2023. It’d be a wet one.\nI typically remember very little of my backpacking trips after the fact. This time around, I tried to take a couple pictures and write a couple sentences a day.1 I decided to stitch them together, unedited, as a blog post below."
  },
  {
    "objectID": "blog/2023-09-01-jmt-2023/index.html#footnotes",
    "href": "blog/2023-09-01-jmt-2023/index.html#footnotes",
    "title": "Hiking the 2023 John Muir Trail / Nüümü Poyo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThank you to Double Dip for additional pictures! Hers are interspersed among my own throughout this post. The good ones are hers, but to know for sure, .jpgs are taken by her and .pngs are my own.↩︎"
  },
  {
    "objectID": "blog/2025-04-18-o3-o4-mini/index.html",
    "href": "blog/2025-04-18-o3-o4-mini/index.html",
    "title": "Evaluating o3 and o4-mini on R coding performance",
    "section": "",
    "text": "48 hours after the drop of the GPT 4.1 series of models, a trio of non-reasoning models focused on “real-world developer needs,” OpenAI dropped another set of models, o3 and o4-mini. These two models are the latest generation of thinking models from OpenAI, and they form the backbone of Codex, a new Claude Code competitor from OpenAI. In short, OpenAI wants market share among developers.\nAfter the 4.1 drop, I put together a blog post evaluating the new models against GPT-4o and my daily driver Claude 3.7 Sonnet on a dataset of R coding tasks. The tl;dr there: they’re better than 4o, but still trailing behind Claude Sonnet 3.7 (with thinking disabled). That said, the GPT 4.1 nano model seems to pack quite the punch for its price point. Following the release of o3 and o4-mini, I was curious how reasoning impacts these models’ performance on the dataset of R coding tasks I’ve been experimenting with.\nThis post will use a new tool I’ve been working on called vitals to evaluate o3 and o4-mini on a series of R coding problems. We’ll compare o3 and o4-mini’s performance against the previous generation of thinking models from OpenAI, o1 and o3-mini, as well as GPT 4.1 and Claude Sonnet 3.7."
  },
  {
    "objectID": "blog/2025-04-18-o3-o4-mini/index.html#getting-situated",
    "href": "blog/2025-04-18-o3-o4-mini/index.html#getting-situated",
    "title": "Evaluating o3 and o4-mini on R coding performance",
    "section": "Getting situated",
    "text": "Getting situated\nEarlier this week, I wrote up a more thorough description of the vitals package as well as are, a dataset of R coding problems. I’ll abbreviate that here, but you can read that post if you’re feeling lost! In short:\n\n\nvitals is an R port of the widely adopted Python framework Inspect for large language model evaluation in R. The package is aimed at supporting ellmer users in evaluating how well their ellmer-based tools work.\n\nare, or “An R Eval” is a dataset of R coding problems. Each input is a question about R code which could be solved on first-read only by human experts and, with a chance to read documentation and run some code, by fluent data scientists. Solutions are in the target column and enable a fluent data scientist to evaluate whether the solution deserves full, partial, or no credit.\n\n\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\n\n\n\n\n\n\n\nNote\n\n\n\nThroughout this post, I refer to “thinking out loud” as “reasoning” in general and when I reference OpenAI’s model specfically, but use “Thinking” when referencing Claude, which seems to be the term they use.\n\n\nDefining models\nIn ellmer, here’s how we define those model connections:\n\nsonnet_3_7 &lt;- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\nsonnet_3_7_thinking &lt;- chat_anthropic(\n  model = \"claude-3-7-sonnet-latest\",\n  api_args = list(\n    thinking = list(type = \"enabled\", budget_tokens = 2000)\n  )\n)\n\ngpt_4_1 &lt;- chat_openai(model = \"gpt-4.1\")\n\ngpt_o1 &lt;- chat_openai(model = \"o1-2024-12-17\")\ngpt_o3_mini &lt;- chat_openai(model = \"o3-mini-2025-01-31\")\ngpt_o3 &lt;- chat_openai(model = \"o3-2025-04-16\")\ngpt_o4_mini &lt;- chat_openai(model = \"o4-mini-2025-04-16\")\n\nI use the default reasoning_effort values for each of these models, which is “medium”. The budget_tokens I set here is approximately half of the default max_tokens used by chat_anthropic().\n\n\n\n\n\n\nNote\n\n\n\nIf you’re interested in how Gemini’s newest 2.5 Pro release stacks up on this eval, check out this post from two weeks ago.\n\n\nNote that I needed to configure a ANTHROPIC_API_KEY and OPENAI_API_KEY to connect to these models, respectively. Their pricing per million tokens is as follows:\n\n\n# A tibble: 6 × 3\n  Name              Input  Output\n  &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt; \n1 Claude 3.7 Sonnet $3.00  $15.00\n2 GPT-4.1           $2.00  $8.00 \n3 o1                $15.00 $60.00\n4 o3-mini           $1.10  $4.40 \n5 o3                $10.00 $40.00\n6 o4-mini           $1.10  $4.40 \n\n\nAltogether, the data underlying this blog post took around $7 USD to generate; the full-sized reasoning models (o1 and o3) are quite expensive, and you pay for both the tokens you receive and the reasoning tokens, which can add up quickly. I reused the results from Claude 3.7 Sonnet (No Thinking) and GPT 4.1 from my post a couple days ago."
  },
  {
    "objectID": "blog/2025-04-18-o3-o4-mini/index.html#a-baseline-model",
    "href": "blog/2025-04-18-o3-o4-mini/index.html#a-baseline-model",
    "title": "Evaluating o3 and o4-mini on R coding performance",
    "section": "A baseline model",
    "text": "A baseline model\nLLM evaluation with vitals happens in two main steps. The first is defining a Task, an R6 method defining important methods for LLM evaluation. Tasks are composed of 3 main components: a dataset, solver, and scorer. Solvers define some system that attempts to solve the problems defined in the dataset, and scorers evaluate how well the solvers did using a grading rubric.\n\nare_task &lt;- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n\nAn evaluation task An-R-Eval.\n\n\nThe above uses model grading (or “LLM-as-a-judge”) to evaluate how well the solver addressed the problem. Since different models exhibit different behaviors as judges, we’ll use the same model Claude 3.7 Sonnet as the judge regardless of the model powering the solver.\nSecond, use Task$eval() to evaluate the solver, evaluate the scorer, and then explore a persistent log of the results in an interactive viewer. This code supplies solver_chat = sonnet_3_7 to evaluate Claude 3.7 Sonnet with thinking disabled as our solver model.\n\nare_sonnet_3_7 &lt;- are_task$clone()\nare_sonnet_3_7$eval(solver_chat = sonnet_3_7)\n\nI use Claude 3.7 Sonnet with thinking disabled as my baseline model here because it’s my daily driver for coding assistance. I find it’s a good balance between price, speed (as in, how long do I need to wait until I start seeing a response), and performance."
  },
  {
    "objectID": "blog/2025-04-18-o3-o4-mini/index.html#evaluating-the-rest",
    "href": "blog/2025-04-18-o3-o4-mini/index.html#evaluating-the-rest",
    "title": "Evaluating o3 and o4-mini on R coding performance",
    "section": "Evaluating the rest",
    "text": "Evaluating the rest\nFrom here, it’s pretty rote. We can evaluate the remaining models by cloning the original task and running $eval() with a new solver chat. First, another round with Claude 3.7 Sonnet, this time with thinking enabled. This is probably the closest apples-to-apples model to compare o3 and o4-mini to:\n\nare_sonnet_3_7_thinking &lt;- are_task$clone()\nare_sonnet_3_7_thinking$eval(solver_chat = sonnet_3_7_thinking)\n\nThen with GPT 4.1:\n\nare_gpt_4_1 &lt;- are_task$clone()\nare_gpt_4_1$eval(solver_chat = gpt_4_1)\n\nNow, with the reasoning models:\n\nare_gpt_o1 &lt;- are_task$clone()\nare_gpt_o1$eval(solver_chat = gpt_o1)\n\n\nare_gpt_o3_mini &lt;- are_task$clone()\nare_gpt_o3_mini$eval(solver_chat = gpt_o3_mini)\n\n\nare_gpt_o3 &lt;- are_task$clone()\nare_gpt_o3$eval(solver_chat = gpt_o3)\n\n\nare_gpt_o4_mini &lt;- are_task$clone()\nare_gpt_o4_mini$eval(solver_chat = gpt_o4_mini)\n\nI’ve also situated the logs for this post in the Inspect Log Viewer, a small .js app that allows you to interactively explore evaluation logs. Especially the first few times you run an eval, the tool is super helpful for uncovering unexpected behavior in solving and scoring. The viewer belows allows you to check out the problems in An R Eval and how effectively each of the models evaluated here handled them:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s kind of hard to tell which Claude 3.7 Sonnet results are from thinking enabled vs. not in this viewer. The results with thinking enabled are shown initially (and at the top of the sidebar). I’ll fix this in vitals in time for my next post. :)"
  },
  {
    "objectID": "blog/2025-04-18-o3-o4-mini/index.html#analysis",
    "href": "blog/2025-04-18-o3-o4-mini/index.html#analysis",
    "title": "Evaluating o3 and o4-mini on R coding performance",
    "section": "Analysis",
    "text": "Analysis\nAt evaluation time, vitals does a naive accuracy calculation that you can see displayed in the app, but in general is quite restrained in its analysis functionality. Instead, the package aims to get analysts to Happy Data Frame Land as quickly as possible using vitals_bind():\n\nare_eval &lt;- \n  vitals_bind(\n    `Claude Sonnet 3.7\\n(No Thinking)` = are_sonnet_3_7,\n    `Claude Sonnet 3.7\\n(Thinking)` = are_sonnet_3_7_thinking,\n    `GPT 4.1` = are_gpt_4_1,\n    `o1` = are_gpt_o1,\n    `o3-mini` = are_gpt_o3_mini,\n    `o3` = are_gpt_o3,\n    `o4-mini` = are_gpt_o4_mini\n  ) %&gt;%\n  rename(model = task) %&gt;%\n  mutate(\n    model = factor(model, levels = c(\n      \"Claude Sonnet 3.7\\n(No Thinking)\",\n      \"Claude Sonnet 3.7\\n(Thinking)\",\n      \"GPT 4.1\",\n      \"o1\",\n      \"o3-mini\",\n      \"o3\",\n      \"o4-mini\"\n    )),\n    reasoning = case_when(\n      model %in% c(\"Claude Sonnet 3.7\\n(No Thinking)\", \"GPT 4.1\") ~ \"Non-reasoning\",\n      .default = \"Reasoning\"\n    )\n  )\n\nare_eval\n\n# A tibble: 546 × 6\n   model                          id    epoch score metadata reasoning\n   &lt;fct&gt;                          &lt;chr&gt; &lt;int&gt; &lt;ord&gt; &lt;list&gt;   &lt;chr&gt;    \n 1 \"Claude Sonnet 3.7\\n(No Think… afte…     1 I     &lt;tibble&gt; Non-reas…\n 2 \"Claude Sonnet 3.7\\n(No Think… afte…     2 I     &lt;tibble&gt; Non-reas…\n 3 \"Claude Sonnet 3.7\\n(No Think… afte…     3 I     &lt;tibble&gt; Non-reas…\n 4 \"Claude Sonnet 3.7\\n(No Think… cond…     1 P     &lt;tibble&gt; Non-reas…\n 5 \"Claude Sonnet 3.7\\n(No Think… cond…     2 P     &lt;tibble&gt; Non-reas…\n 6 \"Claude Sonnet 3.7\\n(No Think… cond…     3 C     &lt;tibble&gt; Non-reas…\n 7 \"Claude Sonnet 3.7\\n(No Think… corr…     1 P     &lt;tibble&gt; Non-reas…\n 8 \"Claude Sonnet 3.7\\n(No Think… corr…     2 C     &lt;tibble&gt; Non-reas…\n 9 \"Claude Sonnet 3.7\\n(No Think… corr…     3 P     &lt;tibble&gt; Non-reas…\n10 \"Claude Sonnet 3.7\\n(No Think… curl…     1 I     &lt;tibble&gt; Non-reas…\n# ℹ 536 more rows\n\n\nIn this dataset, each row represents a single time a solver is invoked to answer a question:\n\nmodel gives the model used to solve a given question\nid gives the question id\nepoch identifies the run/resample of the given question\nscores shows whether the scoring model (Claude Sonnet 3.7) identified the solver’s answer as Incorrect, Partially Correct, or Correct. It’s an ordinal factor with I &lt; P &lt; C.\nmetadata is a list column containing just about all of the information that vitals collects during the evaluation process.\n\nWe’re interested in which of these models are right more often. We have 26 unique questions, each resampled across 3 epochs for each of a number of models. To get a glimpse of each of these models’ performance, we could start off with a bar chart:\n\nare_eval %&gt;%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n  ) %&gt;%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"OpenAI's newest reasoning models are a substantial improvement on their\\nprevious generation for R coding. Claude 3.7 Sonnet trails these new models\\nslightly, regardless of whether thinking is enabled.\"\n  ) +\n  theme(\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\",\n    axis.text.y = element_text(angle = 25, hjust = 1)\n  ) +\n  facet_grid(vars(reasoning), scales = \"free\", space = \"free\")\n\n\n\n\n\n\n\nCould the differences we’re seeing be attributed to random noise, though? We can use a hierarchical modeling technique called a mixed model to model the probability of each score (i.e., correct, etc.) as a function of the LLM. In this case, observations are not independent; some questions may be harder than others, and we’re repeating each question multiple times since we’ve set epochs = 3. A random intercept on the question id can help account for this variation. Since score is ordinal, we use a cumulative link mixed model rather than the usual suspect lme4::glmer():\n\nlibrary(ordinal)\n\nare_mod &lt;- clmm(score ~ model + (1|id), data = are_eval)\n\n\nsummary(are_mod)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  546  -400.57 819.13 464(2748) 7.81e-05 6.7e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 8.047    2.837   \nNumber of groups:  id 26 \n\nCoefficients:\n                                   Estimate Std. Error z value\nmodelClaude Sonnet 3.7\\n(Thinking) -0.04864    0.37509  -0.130\nmodelGPT 4.1                       -1.03628    0.37864  -2.737\nmodelo1                            -0.74840    0.37672  -1.987\nmodelo3-mini                       -0.43926    0.37543  -1.170\nmodelo3                             0.63618    0.38425   1.656\nmodelo4-mini                        0.39391    0.39141   1.006\n                                   Pr(&gt;|z|)   \nmodelClaude Sonnet 3.7\\n(Thinking)   0.8968   \nmodelGPT 4.1                         0.0062 **\nmodelo1                              0.0470 * \nmodelo3-mini                         0.2420   \nmodelo3                              0.0978 . \nmodelo4-mini                         0.3142   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P -2.10134    0.57381  -3.662\nP|C  0.03191    0.57125   0.056\n\n\nFor the purposes of this post, we’ll just take a look at the Coefficients table. The reference model here is Claude 3.7 Sonnet with thinking disabled. Positive coefficent estimates for a given model indicate that that model is more likely to receive higher ratings than Claude 3.7 Sonnet with thinking disabled. Altogether, we see:\n\nEnabling thinking doesn’t seem to impact Claude 3.7 Sonnet’s performance on this eval.\nWhile, directionally, o3 and o4-mini seem to achieve higher ratings than Claude 3.7 Sonnet without thinking, those differences aren’t shown to be statistically significant.\nWhile we don’t quantify the variation in this analysis, it seems like o3 and o4-mini do demonstrate an improvement over the previous generation of reasoning models from OpenAI.\n\nI’m certainly interested to try out o4-mini in the next couple weeks and see what it’s capable of!\n\nThank you to Max Kuhn for advising on the model-based analysis here."
  },
  {
    "objectID": "blog/2020-11-12-pdfs-from-data/index.html",
    "href": "blog/2020-11-12-pdfs-from-data/index.html",
    "title": "Programatically Generating PDF Reports with the Tidyverse",
    "section": "",
    "text": "Over the past few years, I’ve come across the problem of generating PDF reports programmatically from data in a few different settings. This is, of course, a really vaguely defined problem, and they’ve all varied slightly in what they entailed and how automated they needed to be, but my solutions for each shared a few common ideas that I think are worth putting down in writing.\nMost recently, in my role at the Reed College mLab, the problem went something like this:\n\nIt’s presentation week in a virtual class, and while each student is giving a presentation, the other students in the class are filling out a Google form with feedback for the presenter. The professor would like to put together a report for each student in the class containing all of the feedback for them.\n\nI worked through this problem in my most recent shift, and my wonderful supervisor gave me the go-ahead on writing up some tips/tricks for working with similar problems in the future. (Thank you, Kristin!)\nAt a high level, the process usually looks something like this:\n\nImport and tidy your data\nCome up with some syntax to convert your data to a vector of lines that will form an .md file\nWrite those vectors to .md files, render them, and delete the .md files\n\nFor this problem specifically, in addition to the {tidyverse}, I used {googlesheets4} and {rmarkdown}. {googlesheets4} will allow us to read in the needed data, and {rmarkdown} will allow us to render the .md files we end up putting together to PDFs.\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(rmarkdown)\nLet’s check out the data:\n# read in all of the responses\nresponses &lt;-\n  read_sheet(\n    \"https://docs.google.com/spreadsheets/d/1saowcRmK3S4mUO5qOuDT_WQycNtGG1_eSYpETodzePw/edit?usp=sharing\"\n  )\n\nresponses\n## # A tibble: 8 x 4\n##   timestamp           presenter main_argument           additional_feedback     \n##   &lt;dttm&gt;              &lt;chr&gt;     &lt;chr&gt;                   &lt;chr&gt;                   \n## 1 2020-11-09 13:27:29 Simon     Simon thinks it's abou… I agree!                \n## 2 2020-11-09 13:32:04 Simon     He argued that medium-… This is a valid stateme…\n## 3 2020-11-09 13:56:17 Ingrid    Visualizing legos with… You can bet your bottom…\n## 4 2020-11-09 15:13:27 Ingrid    Her data viz chops are… Foreal!                 \n## 5 2020-11-09 15:21:37 Josh      Pamplemousse is the be… Controversial statement.\n## 6 2020-11-09 15:58:36 Josh      Josh's Tidy Tuesday su… Not-controversial state…\n## 7 2020-11-10 08:48:51 Leila     ggplot2 pedagogy shoul… Leila's puppy is really…\n## 8 2020-11-10 08:51:30 Leila     Leila demonstrated a r… She is an incredible te…\nThese responses might be fake, but my coworkers are the real deal.😄 I’ve created a couple rows for each mLabbie (i.e. student worker in the mLab) with some sample responses that look similar to the format that Google Forms exported.\nThat URL is public, so feel free to browse around the data! We’d like to use the {tidyverse} to create a PDF for each presenter, with all of their feedback from each respondent neatly collated.\nFirst, we’ll split up the data by presenter using {dplyr}.\nresponses_list &lt;- \n  responses %&gt;%\n  group_split(presenter)\n\nresponses_list[[1]]\n## # A tibble: 2 x 4\n##   timestamp           presenter main_argument            additional_feedback    \n##   &lt;dttm&gt;              &lt;chr&gt;     &lt;chr&gt;                    &lt;chr&gt;                  \n## 1 2020-11-09 13:56:17 Ingrid    Visualizing legos with … You can bet your botto…\n## 2 2020-11-09 15:13:27 Ingrid    Her data viz chops are … Foreal!\nEach element of responses_list is a data frame giving the responses for a given presenter.\nNow, we’d like to come up with some sort of standard way to convert each of these data frames to lines in an .md file. This is the step that will look most different from application to application, but here’s what I used for this problem:\n# collates responses from the dataframe for a given presenter into\n# lines of a .md file\nwrite_feedback_lines &lt;- function(presenter_df) {\n  out &lt;- \n    c(\n      paste0(\"# \", presenter_df$presenter[1]),\n      \"\",\n      \"### Summaries of Main Argument\",\n      paste0(\"* \", presenter_df$main_argument),\n      \"\",\n      \"### Additional Comments\",\n      paste0(\"* \", presenter_df$additional_feedback)\n    )\n  \n  out\n}\nThere are all sorts of ways to go about this, many of which are likely more slick than the above code, but the main goal here is come up with a vector where each line will ultimately become a line in an .md file. With some calls to paste0 here, we can add bullet points before each response and section headers throughout.\nThe output of this function for Ingrid looks like this:\nwrite_feedback_lines(responses_list[[1]])\n## [1] \"# Ingrid\"                                      \n## [2] \"\"                                              \n## [3] \"### Summaries of Main Argument\"                \n## [4] \"* Visualizing legos with brickr is super neat!\"\n## [5] \"* Her data viz chops are the real deal!\"       \n## [6] \"\"                                              \n## [7] \"### Additional Comments\"                       \n## [8] \"* You can bet your bottom dollar it is.\"       \n## [9] \"* Foreal!\"\nWe want to run this function on each presenter and write the results to an .md file. Using the map function from purrr to run this function on each presenter:\n# make a vector of lines out of each data subset\npresenter_lines &lt;-\n  map(\n    responses_list,\n    write_feedback_lines\n  ) %&gt;%\n  # set the names of the object to the presenter's name\n  set_names(\n    map(\n      responses_list, \n      pluck, \n      \"presenter\", \n      1\n    )\n  )\nThis next step is a bit clunky, and I’d absolutely welcome feedback here. We’ll write these vectors to .md files, render the .md to .pdf, and then delete the .md files!\n# path to the directory you'd like to write to\nfolder &lt;- \"feedback/\"\n\nmap2(\n  # the lines for each presenter\n  presenter_lines,\n  # the path to write the lines to for the presenter\n  paste0(folder, names(presenter_lines), \".md\"),\n  # the function to use to write the lines\n  write_lines\n)\nChecking that our code did what we want it to:\nlist.files(folder)\n## [1] \"Ingrid.md\" \"Josh.md\"   \"Leila.md\"  \"Simon.md\"\nLooks like the function wrote the files where we intended! Now, we can make use of the render function from rmarkdown to render the .mds to .pdfs.\n# create the .pdf files\nmap(\n  paste0(folder, names(presenter_lines), \".md\"),\n  rmarkdown::render,\n  \"pdf_document\"\n)\n\n# delete the source .md files\nfiles &lt;- list.files(folder, full.names = TRUE)\nmds &lt;- files[str_detect(files, \".md\")]\nfile.remove(mds)\nNow, looking at all of the files in the directory again:\nlist.files(folder)\n## [1] \"Ingrid.pdf\" \"Josh.pdf\"   \"Leila.pdf\"  \"Simon.pdf\"\nEach .pdf looks something like this:\n\nNothing too fancy, but definitely snazzier and more pleasant to read than a spreadsheet.🦋\nThat’s it! If you’d like to spend some time with this code yourself, an abbreviated .R file is available here.\nIt seems like these kinds of problems come with all sorts of twists and turns in practice. Some other directions you could go with this:\n\nYou could run this script on some regular interval, scheduling this task with CRON or GitHub Actions.\nInterweaving actual code results with text in these PDFs is a bit tricky. rmarkdown’s render() will parse code chunks as normal when called on .Rmd files, and you’ll be dropping in code chunks with \"```{r}\". Parameterized reports could come in handy here as well.\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2022-07-20-netlify-subdomain-redirect/index.html",
    "href": "blog/2022-07-20-netlify-subdomain-redirect/index.html",
    "title": "Redirecting from sub-domains with Netlify",
    "section": "",
    "text": "This post outlines how to redirect from a subdomain with Netflify for people who—like me—who don’t really understand what that means.🌝🌚"
  },
  {
    "objectID": "blog/2022-07-20-netlify-subdomain-redirect/index.html#is-this-relatable",
    "href": "blog/2022-07-20-netlify-subdomain-redirect/index.html#is-this-relatable",
    "title": "Redirecting from sub-domains with Netlify",
    "section": "Is this relatable?",
    "text": "Is this relatable?\nI used to have two websites.\nOne was the “main” one, and it was a fork of a college friend’s slick JavaScript setup that I didn’t really understand. But—it looked awesome on a desktop, which made me think it looked awesome everywhere, until I completely finished putting it all together and then tried to show it to a friend on my phone. Completely incomprehensible on mobile.\nAnd then there was the blog. That one lived on a subdomain, blog.simonpcouch.com, and it was a fork of Julia Silge’s (you’re the best, Julia!) old blog. I also didn’t really understand that one, but it was a Hugo Academic type deal that I found pretty easy to work with. Except when it didn’t typeset math properly, or… build on any computer except the laptop I originally wrote it on. The production build of that site was set up as a branch deploy—I pushed commits to a different GitHub branch, and the site used different build settings for that site.\nSo.\nI merged them! My main website is now a Hugo Apéro site, and the blog lives on the same site. How modern."
  },
  {
    "objectID": "blog/2022-07-20-netlify-subdomain-redirect/index.html#the-problem",
    "href": "blog/2022-07-20-netlify-subdomain-redirect/index.html#the-problem",
    "title": "Redirecting from sub-domains with Netlify",
    "section": "The problem",
    "text": "The problem\nThe content that used to live at:\nblog.simonpcouch.com/blog/blog-post-slug\n…now also lives at:\nsimonpcouch.com/blog/blog-post-slug\nI’d rather that those old blog posts only live in one place, and preferably on my new website. So, it’d be great if, when someone heads to that first URL, the website will automatically send them to the same post on my new one.\nNetlify’s docs about redirects seem pretty clear for the problem of redirecting within a domain, but I had trouble wrapping my head around how to solve this problem between domains. So, here’s the I-don’t-know-anything-about-web-dev version."
  },
  {
    "objectID": "blog/2022-07-20-netlify-subdomain-redirect/index.html#wheee",
    "href": "blog/2022-07-20-netlify-subdomain-redirect/index.html#wheee",
    "title": "Redirecting from sub-domains with Netlify",
    "section": "Wheee",
    "text": "Wheee\nLet’s say your Branch Deploy deploys from the blog branch to blog.yoursite.com.\nYou’ll first need to find your publish directory of your branch-deployed site. That’s the folder where your deploy-ready website lives.\nIf you use the netlify.toml config file, that’s the publish entry in your context.branch-deploy config. Mine looked like this:\n[context.branch-deploy]\ncommand = \"hugo --gc --minify -b $DEPLOY_PRIME_URL\"\npublish = \"public\"\nHUGO_VERSION = \"0.68.3\"\nIf you’ve set up your site build settings on the Netlify site, that entry lives in Site &gt; Site Settings &gt; Build & Deploy &gt; Build settings &gt; Publish directory.\nFor Hugo sites, that publish directory is public.\nNavigate to that publish directory on the branch that you’d like to redirect from. That is, I wanted to redirect from blog.simonpcouch.com to simonpcouch.com, so I went to public folder in the blog branch.\nNow, you’ll create a new text file here called _redirects. No extension needed. :)\nTo be clear, you don’t need to do anything on the main, production branch to make this work!"
  },
  {
    "objectID": "blog/2022-07-20-netlify-subdomain-redirect/index.html#configuring-_redirects",
    "href": "blog/2022-07-20-netlify-subdomain-redirect/index.html#configuring-_redirects",
    "title": "Redirecting from sub-domains with Netlify",
    "section": "Configuring _redirects",
    "text": "Configuring _redirects\nDepending on your setup, this file might only be a couple lines. Generally, the format is like so:\nhttps://blog.yoursite.com/* https://www.yoursite.com/blog/:splat 301!\nThe LHS side entry matches URLs on the branch site, and RHS side entry matches URLs on the new site. Some things worth pointing out:\n\nThe * at the end of the LHS URL means “anything can go here.” Anything that starts with blog.yoursite.com/ will be matched.\nThe :splat at the end of the RHS URL will be replaced with whatever * was. If you want anything from blog.yoursite.com to just go to your new site’s homepage, you can just leave out :splat.\nThe ! at the end of the RHS URL is weird. I’m just pattern matching. See these docs from Netlify for more on what’s happening there.\n\nOnce you have this entry set up as you like, it seems like it’s also worth making sure you do the same thing to URLs that are prefixed with http:// rather than https://. You’ll still want to send them to https:// on the RHS, though. For example, if you were using the rule above, pair it with:\nhttp://blog.yoursite.com/* https://www.yoursite.com/blog/:splat 301!\n…on a new line.\nFor an example, you can check out my redirect rules here:\nhttp://blog.simonpcouch.com/* https://www.simonpcouch.com/:splat 301!\nhttps://blog.simonpcouch.com/* https://www.simonpcouch.com/:splat 301!\nAll of the URLs for blog posts on my old website just have that first blog. bit taken away. Slick. 🦆\n\nI’m not sure how niche this problem is and how obvious this solution may be to others. I guess I could set up analytics on this website and see how many people view this post, but that would require more messing around with web stuff. 🤪\nHehe. I appreciate you dropping by, and hope this will be helpful for some!"
  },
  {
    "objectID": "blog/2023-11-30-listening-2023/index.html",
    "href": "blog/2023-11-30-listening-2023/index.html",
    "title": "Analyzing my own music listening data with R and the tidyverse (2023)",
    "section": "",
    "text": "Aside from exchanging playlists with my partner every once in a while, I’m not much of a Spotify user. Around this time every year, though, all of my friends start posting their Spotify Wrapped, and I get jealous, as the platform that I listen to music on doesn’t have anything like it. Of course, though, it collects data about me (it’s 2023!); last year, I got to wondering whether I could make a lo-fi knockoff of wrapped using R, the tidyverse, and the data that I have access to. You already know:\nlibrary(tidyverse)\nIf you’re an R user and a listener of local files on the Mac Music app, this post is for you.🎁"
  },
  {
    "objectID": "blog/2023-11-30-listening-2023/index.html#importing-the-data",
    "href": "blog/2023-11-30-listening-2023/index.html#importing-the-data",
    "title": "Analyzing my own music listening data with R and the tidyverse (2023)",
    "section": "Importing the data",
    "text": "Importing the data\nIn the Mac music app, navigate to:\nMusic app &gt; File &gt; Library &gt; Export Library\n…to export a .xml file. Last year, I griped about how much of a pain in the ass it was to tidy the resulting output. This year, we can all just install the package I wrote last year and forget about our woes:\n\npak::pak(\"simonpcouch/wrapped\")\n\nThe wrapped package contains a function, wrap_library(), to tidy that .xml file into a tabular data frame.\n\nlibrary(wrapped)\n\nwrapped &lt;- wrap_library(\"data/Library.xml\", 2022:2023)\n\nwrapped\n\n# A tibble: 12,545 × 8\n      id track_title         artist album genre date_added skip_count play_count\n   &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 11729 Atom                Mediu… Heal… Indi… 2023-02-11         15        234\n 2 11862 Reelin'             Matt … Ever… Indi… 2023-03-24         19        208\n 3 11732 Gimme Back My Soul  Mediu… Heal… Indi… 2023-02-11         11        195\n 4 12179 Swim                Noah … If T… Sing… 2023-07-21         11        191\n 5 11733 Never Learned To D… Mediu… Heal… Indi… 2023-02-11         12        161\n 6 12088 Get The Girl        Seafo… Get … Coun… 2023-06-16         26        159\n 7 11855 Everything's Fine   Matt … Ever… Indi… 2023-03-24         12        153\n 8 11656 Never Learned To D… Mediu… Neve… Indi… 2022-12-26          6        143\n 9 12388 Desert Land         Matt … Dese… Indi… 2023-10-22         11        141\n10 12097 Given               Justi… Dayd… R&B/… 2023-06-16         11        132\n# ℹ 12,535 more rows\n\n\nAfter that, Spotify Wrapped is just group_by() %&gt;% summarize() %&gt;% arrange() in a trench coat.🧥\nFor easier printing in this blog post, I’ll rearrange this data to show the most commonly noted output:\n\nwrapped &lt;- \n  wrapped %&gt;%\n  select(-id) %&gt;%\n  relocate(date_added, skip_count, .after = everything()) %&gt;%\n  relocate(play_count, .before = everything())\n\nwrapped\n\n# A tibble: 12,545 × 7\n   play_count track_title            artist    album genre date_added skip_count\n        &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;date&gt;          &lt;dbl&gt;\n 1        234 Atom                   Medium B… Heal… Indi… 2023-02-11         15\n 2        208 Reelin'                Matt Cor… Ever… Indi… 2023-03-24         19\n 3        195 Gimme Back My Soul     Medium B… Heal… Indi… 2023-02-11         11\n 4        191 Swim                   Noah Gun… If T… Sing… 2023-07-21         11\n 5        161 Never Learned To Dance Medium B… Heal… Indi… 2023-02-11         12\n 6        159 Get The Girl           Seaforth  Get … Coun… 2023-06-16         26\n 7        153 Everything's Fine      Matt Cor… Ever… Indi… 2023-03-24         12\n 8        143 Never Learned To Dance Medium B… Neve… Indi… 2022-12-26          6\n 9        141 Desert Land            Matt Cor… Dese… Indi… 2023-10-22         11\n10        132 Given                  Justin N… Dayd… R&B/… 2023-06-16         11\n# ℹ 12,535 more rows"
  },
  {
    "objectID": "blog/2023-11-30-listening-2023/index.html#analyzing-it",
    "href": "blog/2023-11-30-listening-2023/index.html#analyzing-it",
    "title": "Analyzing my own music listening data with R and the tidyverse (2023)",
    "section": "Analyzing it",
    "text": "Analyzing it\nTop songs\nThe output is already arranged in descending order by play count, so we can just print the first few rows:\n\nwrapped %&gt;%\n  select(track_title, artist, play_count) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  track_title            artist         play_count\n  &lt;chr&gt;                  &lt;chr&gt;               &lt;dbl&gt;\n1 Atom                   Medium Build          234\n2 Reelin'                Matt Corby            208\n3 Gimme Back My Soul     Medium Build          195\n4 Swim                   Noah Gundersen        191\n5 Never Learned To Dance Medium Build          161\n6 Get The Girl           Seaforth              159\n\n\nMedium! Build!\nTop artists\n\nwrapped %&gt;%\n  group_by(artist) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  artist         play_count\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Medium Build         1921\n2 Matt Corby           1622\n3 Justin Nozuka        1058\n4 Noah Gundersen        907\n5 Patrick Droney        569\n6 Mac Ayres             546\n\n\ngroup_by() %&gt;% summarize()! I told you!\nI will fly to Australia to see Matt Corby play live if I have to.\nTop genres\nOne of my first steps after buying a new record is to edit it’s metadata to fit into one of a few pre-defined genres. Many of these categorizations are sort of silly as a result, but it does make for a nice summary:\n\nwrapped %&gt;%\n  group_by(genre) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  head(5)\n\n# A tibble: 5 × 2\n  genre                  play_count\n  &lt;chr&gt;                       &lt;dbl&gt;\n1 Indie/Alternative            5337\n2 Singer-Songwriter/Folk       3937\n3 R&B/Soul                     2855\n4 Country                      2258\n5 Indie Pop                     971\n\n\nSort of confused by the existence of the “Indie Pop” category.remo::ji(“confused”)` Definitely need to clean up some of those entries.\n\n\n\n\n\n\nTip\n\n\n\nYou can selectively use the n argument to head() to hide things that you’re embarrassed about.\n\n\nTop albums\n\nwrapped %&gt;%\n  group_by(album, artist) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  album                        artist         play_count\n  &lt;chr&gt;                        &lt;chr&gt;               &lt;dbl&gt;\n1 Everything's Fine            Matt Corby           1217\n2 Health - EP                  Medium Build          971\n3 Never Learned To Dance       Medium Build          819\n4 Daydreams and Endless Nights Justin Nozuka         736\n5 If This Is The End           Noah Gundersen        598\n6 Comfortable Enough           Mac Ayres             428"
  },
  {
    "objectID": "blog/2023-11-30-listening-2023/index.html#bonus-points",
    "href": "blog/2023-11-30-listening-2023/index.html#bonus-points",
    "title": "Analyzing my own music listening data with R and the tidyverse (2023)",
    "section": "Bonus points",
    "text": "Bonus points\nThere are a couple summarizations that Wrapped doesn’t do that I’m curious about.\nTop song by month\nI don’t have the right level of observation to see which songs I listened to the most every month, but I do have a variable giving the data I added a given song. We can use that information to find the top songs by month added:\n\nwrapped %&gt;%\n  mutate(month = month(date_added)) %&gt;%\n  group_by(month) %&gt;%\n  summarize(\n    track_title = track_title[which.max(play_count)], \n    artist = artist[which.max(play_count)]\n  ) %&gt;%\n  head(11)\n\n# A tibble: 11 × 3\n   month track_title            artist           \n   &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;            \n 1     1 Sad Song               Brandon Ratcliff \n 2     2 Atom                   Medium Build     \n 3     3 Reelin'                Matt Corby       \n 4     4 Be Yourself            Wilder Woods     \n 5     5 tennessee is mine      Alana Springsteen\n 6     6 Get The Girl           Seaforth         \n 7     7 Swim                   Noah Gundersen   \n 8     8 You Take The High Road Bruno Major      \n 9     9 Better Days            Noah Gundersen   \n10    10 Desert Land            Matt Corby       \n11    11 PANIC ATTACK           Clinton Kane     \n\n\nTop artist by genre\n\nwrapped %&gt;%\n  group_by(genre, artist) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(genre) %&gt;%\n  summarize(\n    artist = artist[which.max(play_count)],\n    play_count = play_count[which.max(play_count)]\n  ) %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  genre                  artist            play_count\n  &lt;chr&gt;                  &lt;chr&gt;                  &lt;dbl&gt;\n1 Indie/Alternative      Matt Corby              1459\n2 R&B/Soul               Justin Nozuka           1005\n3 Indie Pop              Medium Build             971\n4 Singer-Songwriter/Folk Noah Gundersen           598\n5 Country                Alana Springsteen        538\n6 Bluegrass              Mighty Poplar            369\n\n\nMoved on\nHow many albums in my library did I not listen to at all this year? (I reset the play count for all of my library to zero each time I do this analysis.)\n\nwrapped %&gt;%\n  group_by(album, artist) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  filter(play_count == 0) %&gt;%\n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1195\n\n\nThat number is a lot bigger than I thought.😬"
  },
  {
    "objectID": "blog/2021-05-10-dev-docs-p1/index.html",
    "href": "blog/2021-05-10-dev-docs-p1/index.html",
    "title": "How {stacks} Came To Be (Developer Documentation pt. 1)",
    "section": "",
    "text": "This is the first of four blog posts on the development process of the {stacks} package, excerpted from my Reed senior thesis project Tidy Model Stacking with R.\nPart 2: Splitting Things Up\nPart 3: Naming Things\nPart 4: Big Things\n\nIn summer 2020, the prominent R programmer and organizational analytics expert Emily Riederer tweeted the following thread:\n\nWhen I first read it, I was struck by Riederer’s thread. (I didn’t favorite the last one, though? Shame.)\nAt the time, I was in the first stages of writing {stacks}. From the user’s end, I was thinking about how working with the package ought to feel, the lessons it ought to teach, and the moments for pause it ought to encourage. From my end, I was thinking about what objects would be pushed around from place to place and how the package would manipulate—and keep track of—them. The first hundred or so commits to {stacks} document a whirlwind of implementations and API shufflings (as well as very few passing continuous integration checks):\n\nRiederer’s thread planted the seed for the thought that the work of making sense of the volatility of {stacks}’ initial interfaces may be productive. I found the thought exciting in that, for one, I may convince myself in the process that the tumultuousness of {stacks}’ initial APIs may have had some rhyme or reason to it and, additionally, that there were actually some lessons learned in the process that may be passed on to others. Developer documentation driven development, if you will!\nThis series of posts—excerpted from a chapter of my Reed senior thesis project—makes an attempt at Riederer’s concept of developer documentation, focusing on how some elements of {stacks} came to be. In the name of coherence (and forgetfulness), this is a somewhat revisionist history—this chapter makes linear and sensical what was often a short-sighted and goofy sequence of decisions. I don’t entirely remember what was going on in my head in those first few months of the package’s development, though journal notes, commit messages, GitHub issues, and Slack threads have helped piece together bits and pieces of those thought processes.\nIn the three posts that follow, to be released one per day over the next three days, I consider the following questions at length:\n\nSplitting Things Up: Why does making a stacked ensemble model with {stacks} require four core functions? Couldn’t there just be one?\nNaming Things: Why are those four core functions named the way they are?\nBig Things: Why did you write that so weirdly? “That,” here, will be many things.\n\nThoughtful answers to these questions may only scratch the surface of what one may ask of the package’s design, though I assume no one is really that interested anyway.\nSome quick notes on conventions throughout these posts:\nGenerally, references to code in these posts follow the tidyverse style and design guides, with a few notable exceptions.\n\nWhen referring to functions as objects (or ideas), this thesis will not use parentheses. For example, I will write fn rather than fn(). This style follows that which the tidyverse style guide uses for other types of objects—for instance, we write obj rather than obj$ or obj[] when referring to data structures that are subsettable with that syntax. This also allows for delineating between the function itself and calls to functions which do not require arguments. That is, in this thesis, fn() refers to the result of calling fn() rather than fn itself. This convention was inspired by Jerry Shurman’s course Vector Calculus at Reed College as well as a forthcoming book on software engineering with Javascript by Greg Wilson.\nI wrap package names in {brackets}, linking to the package website on the first usage in the series of posts. Later references will only use brackets. When referring to package ecosystems or communities surrounding a package that go by the same name as the package (such as “the tidymodels”), I will not use brackets.\n\nA lot of the language here comes off more formally than my typical posts. These posts are mostly copy + pasted from my Reed senior thesis project, except for some formatting and style edits. I’m a little sleepy, so… so it goes.\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2023-10-24-quarto-site/index.html",
    "href": "blog/2023-10-24-quarto-site/index.html",
    "title": "Quarto!",
    "section": "",
    "text": "When I was in college, a coworker at one of my jobs did web development gigs on the side. His personal website was a super slick single-page site, built with Flask. I had no idea how it worked, but I really liked it, so I asked if it was okay to fork his repository and took that site on as my own. Editing the site was painful, and I borked it at least hourly whenever I worked on it. It mostly functioned like a high-gloss CV, and didn’t have any sort of blogging component. It looked something like this:\n\n\n\n\n\n\n\n\nA couple years in, I decided I wanted to give blogging a go. blogdown + Hugo Academic seemed to be the go-to tooling at the time, so I put together a site (with much repo-diving on Silvia Canelón and Julia Silge’s websites). I mostly liked it, though it was also quite bork-prone due to Hugo version management headaches. Because of how much I liked my “main page,” I maintained two separate websites, with totally different styling and deployment setups. The “main page” stayed as-is, and the blogdown site lived on a subdomain and contained no pages other than blog posts. They linked to each other in their navbars, but each was a totally different user experience, and I didn’t know how to maintain either properly.\nIn the years that followed, many folks gradually migrated to Hugo Apéro. That shit was beautiful. It was so, so easy to make stunning pages with very little knowledge of CSS or web dev. Again, with much repo-diving on Silvia and Julia’s repositories, I made the switch. Transitioning to Hugo Apéro eased the development experience quite a bit, for the most part due to the introduction (or my discovery?) of the blogdown package’s Hugo version management tools and docs. At that point, as well, I also collapsed the “main site” and its blog subdomain into one. Ultimately, though, I found myself still plagued by Hugo authoring woes, spending a lot of time poring over GitHub Issues and StackOverflow posts trying to figure out citation/footnote/linking/math formatting/workflow workarounds.\nOver the last few days, again following Silvia’s lead, I’ve remade my personal website with Quarto. Already, I feel that Quarto has both eased the developer and authoring experiences while also keeping the barrier to making beautiful pages low. Much of the unborking I’ve ran into while putting this site together has been a matter of undoing all of the strange workarounds I put together while working on my Hugo sites. As for the appearance, between strong documentation from the Quarto folks, great examples to follow in Silvia and Emil Hvitfeldt’s sites, and heavy use of my browser’s inspector, I feel happy with what I’ve been able to put together. Between the built-in search functionality, quick RSS setup, straightforward introduction of Utterances commenting, and ease of maintenance and deployment, I think I’m sold.🙂\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2021-08-02-apply-to-nsf-grfp/index.html",
    "href": "blog/2021-08-02-apply-to-nsf-grfp/index.html",
    "title": "Applying to the NSF Graduate Research Fellowship (GRFP)",
    "section": "",
    "text": "Last fall, I applied to (and was awarded) the National Science Foundation’s Graduate Research Fellowship (NSF GRFP). Much of the advice that made me feel most informed and confident about the application process required some degree of access that it should not have—personal connections, institutional resources, and/or hindsight—and I feel this information should be publicly available. I’m far from an expert about how this all works, and can only speak to my personal experience, but I hope I can help others feel more confident in putting together their applications (or deciding whether or not to) by sharing my experience. The format (and, sometimes, content) of this post will follow from my post on applying to graduate school in statistics/biostatistics.\nA few things to note that influenced my personal experience: I’m a cis white man with U.S. citizenship who is an alumni of a private U.S. liberal arts college majoring in math with a concentration in statistics. I first heard about the fellowship the summer before my senior year of undergrad, and applied during the fall semester of my senior year. My GPA probably didn’t do much to put me ahead, but I’d been lucky enough to gain quite a bit of experience in research and software development and had strong recommendation letters. I’m now almost a year out from completing my application, so I might be a bit rusty on the details, here. :-)\nAs I applied to the fellowship as an undergrad, I don’t know much about how the application process changes for folks who apply as PhD students. I’ll try to note when I understand the process to look different, in that case!\nThe order of this post roughly follows the order in which I asked myself questions about the fellowship."
  },
  {
    "objectID": "blog/2021-08-02-apply-to-nsf-grfp/index.html#intellectual-merit",
    "href": "blog/2021-08-02-apply-to-nsf-grfp/index.html#intellectual-merit",
    "title": "Applying to the NSF Graduate Research Fellowship (GRFP)",
    "section": "Intellectual Merit",
    "text": "Intellectual Merit\nThis year’s (2021) solicitation defines the Intellectual Merit criteria as one’s “potential to advance knowledge.”\nSome things that could fall under the umbrella of Intellectual Merit:\n\nAuthorship on publications/posters\nAdvanced coursework\nGrants/awards\nConference attendance/talks\nProfessional certifications"
  },
  {
    "objectID": "blog/2021-08-02-apply-to-nsf-grfp/index.html#broader-impacts",
    "href": "blog/2021-08-02-apply-to-nsf-grfp/index.html#broader-impacts",
    "title": "Applying to the NSF Graduate Research Fellowship (GRFP)",
    "section": "Broader Impacts",
    "text": "Broader Impacts\nThis year’s (2021) solicitation defines the Broader Impacts criteria as one’s “potential to benefit society and contribute to the achievement of specific, desired societal outcomes.”\nSome things that could fall under the umbrella of Broader Impacts:\n\nExtracurricular leadership positions\nVolunteering/outreach experience\nTeaching (assistant) experience\nConference attendance/talks\nOpen source software/analyses/teaching materials\nBlog posts/professional social media presence"
  },
  {
    "objectID": "blog/2021-08-02-apply-to-nsf-grfp/index.html#recommendation-letters",
    "href": "blog/2021-08-02-apply-to-nsf-grfp/index.html#recommendation-letters",
    "title": "Applying to the NSF Graduate Research Fellowship (GRFP)",
    "section": "Recommendation Letters",
    "text": "Recommendation Letters\nFor this fellowship, you’ll need to ask at least three recommenders for letters (similar to many graduate school applications). A solid letter of recommendation is from a professor or research mentor who knows you well and can speak to your specific strengths, ideally at length. Preferably, they have a terminal degree in their field (e.g., a PhD in (bio)stat, math, etc.). If possible, at least one of your letters should be from a professor or practitioner working in the field of your proposed research. For particularly quantitative fields, it can be helpful for one of your letters to speak to your mathematical abilities specifically. You should keep your recommenders in the loop about how your application is coming together and gently remind them about the deadline for recommenders (it’s different than for the rest of your application) as it approaches.\nAs with the essay component of your application, recommendation letters should use explicit headings for the two merit criteria."
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html",
    "href": "blog/2023-11-28-flights/index.html",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "",
    "text": "Last week, I virtually dropped by University of Wisconsin-Madison for a webinar on tidymodels. Heading into the holidays, I thought a fun example problem might be to try and predict flight delays using flights data from Madison’s airport. This is a very, very difficult modeling problem, and the results aren’t very impressive, but it’s a fun one nonetheless.\nI’ve collected some data on all of the outbound flights from Madison, Wisconsin in 2022. In this blog post, we’ll use predictors based on the weather, plane, airline, and flight duration to try to predict whether a flight will be delayed or not. To do so, we will split up the source data and then train models in two stages:\nRound 1) Try out a variety of models, from a logistic regression to a boosted tree to a neural network, using a grid search for each.\nRound 2) Try out more advanced search techniques for the model that seems the most performant in Round 1).\nOnce we’ve trained the final model fit, we can assess the predictive performance on the test set and prepare the model to be deployed."
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#setup",
    "href": "blog/2023-11-28-flights/index.html#setup",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "Setup",
    "text": "Setup\nFirst, loading the tidyverse and tidymodels, along with a few additional tidymodels extension packages:\n\n# for data analysis:\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# for modeling:\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(bonsai)\nlibrary(baguette)\n\nThe finetune package will give us additional tuning functionality, while the bonsai and baguette packages provide support for additional model types.\ntidymodels supports a number of R frameworks for parallel computing:\n\n# loading needed packages:\nlibrary(doMC)\nlibrary(parallelly)\n\n# check out how many cores we have:\navailableCores()\n\nsystem \n    10 \n\n# register those cores so that tidymodels can see them:\nregisterDoMC(cores = max(1, availableCores() - 1))\n\nWith a multi-core setup registered, tidymodels will now make use of all of the cores on my computer for expensive computations."
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#data-import",
    "href": "blog/2023-11-28-flights/index.html#data-import",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "Data Import",
    "text": "Data Import\nWe’ll make use of a dataset, msnflights22, containing data on all outbound flights from Madison, Wisconsin in 2022.\n\nload(\"data/msnflights22.rda\")\n\nmsnflights22\n\n# A tibble: 10,754 × 14\n   delayed airline     flight origin destination date        hour plane distance\n   &lt;fct&gt;   &lt;chr&gt;       &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;       &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;\n 1 No      Endeavor A… 51     MSN    ATL         2022-01-01     5 N901…      707\n 2 No      PSA Airlin… 59     MSN    CLT         2022-01-01     6 N570…      708\n 3 No      Envoy Air   27     MSN    MIA         2022-01-01     6 N280…     1300\n 4 No      American A… 3      MSN    PHX         2022-01-01     6 N662…     1396\n 5 Yes     American A… 16     MSN    DFW         2022-01-01     7 N826…      821\n 6 No      SkyWest Ai… 26     MSN    MSP         2022-01-01     7 N282…      228\n 7 No      United Air… 1      MSN    EWR         2022-01-01     7 &lt;NA&gt;       799\n 8 No      United Air… 6      MSN    DEN         2022-01-01     8 N830…      826\n 9 No      Republic A… 29     MSN    ORD         2022-01-01     8 N654…      109\n10 No      Endeavor A… 62     MSN    DTW         2022-01-01     9 N582…      311\n# ℹ 10,744 more rows\n# ℹ 5 more variables: duration &lt;dbl&gt;, wind_speed &lt;dbl&gt;, precip &lt;dbl&gt;,\n#   visibility &lt;dbl&gt;, plane_year &lt;int&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can make your own flights data using the anyflights package! The query_data.R file contains the code used to generate this dataset.\n\n\nWe’d like to model delayed, a binary outcome variable giving whether a given flight was delayed by 10 or more minutes.\n\n# summarize counts of the outcome variable\nggplot(msnflights22) +\n  aes(x = delayed) +\n  geom_bar(fill = \"#4A7862\")\n\n\n\n\n\n\n\nPredicting flight delays seems quite difficult given the data we have access to. For example, plotting whether a flight is delayed based on precipitation and wind speed:\n\n# plot 2 predictors, colored by the outcome\nmsnflights22 %&gt;%\n  filter(precip != 0) %&gt;%\n  ggplot() +\n  aes(x = wind_speed, y = precip, color = delayed) +\n  geom_jitter()\n\n\n\n\n\n\n\nLooks like there’s a bit of signal in the time of the day of the flight, but those higher-proportion-delayed hours also have quite a bit fewer flights (and thus more variation):\n\n(\n  ggplot(msnflights22, aes(x = hour, fill = delayed)) +\n  geom_bar()\n) /\n(\n  ggplot(msnflights22, aes(x = hour, fill = delayed)) +\n  geom_bar(position = \"fill\") + labs(y = \"proportion\")\n)\n\n\n\n\n\n\n\nA machine learning model may be able to get some traction here, though."
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#splitting-up-data",
    "href": "blog/2023-11-28-flights/index.html#splitting-up-data",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "Splitting up data",
    "text": "Splitting up data\nWe split data into training and testing sets so that, once we’ve trained our final model, we can get an honest assessment of the model’s performance. Since this data is a time series, we’ll allot the first ~10 months to training and the remainder to testing:\n\n# set the seed for random number generation\nset.seed(1)\n\n# split the flights data into...\nflights_split &lt;- initial_time_split(msnflights22, prop = 5/6)\n# training [jan - oct]\nflights_train &lt;- training(flights_split)\n# ...and testing  [nov - dec]\nflights_test &lt;- testing(flights_split)\n\nThen, we’ll resample the training data using a sliding period.\n\n\n\n\n\n\nNote\n\n\n\nA sliding period is a cross-fold validation technique that takes times into account. The tidymodels packages support more basic resampling schemes like bootstrapping and v-fold cross-validation as well—see the rsample package’s website.\n\n\nWe create 8 folds, where in each fold the analysis set is a 2-month sample of data and the assessment set is the single month following.\n\nset.seed(1)\nflights_folds &lt;- \n  sliding_period(\n    flights_train, \n    index = date, \n    \"month\", \n    lookback = 1, \n    assess_stop = 1\n  )\n\nflights_folds\n\n# Sliding period resampling \n# A tibble: 8 × 2\n  splits             id    \n  &lt;list&gt;             &lt;chr&gt; \n1 &lt;split [1783/988]&gt; Slice1\n2 &lt;split [1833/819]&gt; Slice2\n3 &lt;split [1807/930]&gt; Slice3\n4 &lt;split [1749/901]&gt; Slice4\n5 &lt;split [1831/906]&gt; Slice5\n6 &lt;split [1807/920]&gt; Slice6\n7 &lt;split [1826/888]&gt; Slice7\n8 &lt;split [1808/826]&gt; Slice8\n\n\nFor example, in the second split,\n\n# training: february, march, april\ntraining(flights_folds$splits[[2]]) %&gt;% pull(date) %&gt;% month() %&gt;% unique()\n\n[1] 2 3\n\n# testing: may\ntesting(flights_folds$splits[[2]])  %&gt;% pull(date) %&gt;% month() %&gt;% unique()\n\n[1] 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat months will be in the training and testing sets in the third fold?"
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#defining-our-modeling-strategies",
    "href": "blog/2023-11-28-flights/index.html#defining-our-modeling-strategies",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "Defining our modeling strategies",
    "text": "Defining our modeling strategies\nOur basic strategy is to first try out a bunch of different modeling approaches, and once we have an initial sense for how they perform, delve further into the one that looks the most promising.\nWe first define a few recipes, which specify how to process the inputted data in such a way that machine learning models will know how to work with predictors:\n\nrecipe_basic &lt;-\n  recipe(delayed ~ ., flights_train) %&gt;%\n  step_nzv(all_predictors())\n\nrecipe_normalize &lt;-\n  recipe_basic %&gt;%\n  step_YeoJohnson(all_double_predictors()) %&gt;%\n  step_normalize(all_double_predictors())\n\nrecipe_pca &lt;- \n  recipe_normalize %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = tune())\n\nThese recipes vary in complexity, from basic checks on the input data to advanced feature engineering techniques like principal component analysis.\n\n\n\n\n\n\nNote\n\n\n\nThese preprocessors make use of predictors based on weather. Given that prediction models are only well-defined when trained using variables that are available at prediction time, in what use cases would this model be useful?\n\n\nWe also define several model specifications. tidymodels comes with support for all sorts of machine learning algorithms, from neural networks to LightGBM boosted trees to plain old logistic regression:\n\nspec_lr &lt;-\n  logistic_reg() %&gt;%\n  set_mode(\"classification\")\n\nspec_bm &lt;- \n  bag_mars(num_terms = tune(), prod_degree = tune()) %&gt;%\n  set_engine(\"earth\") %&gt;% \n  set_mode(\"classification\")\n\nspec_bt &lt;- \n  bag_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nspec_nn &lt;- \n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine(\"nnet\", MaxNWts = 15000) %&gt;%\n  set_mode(\"classification\")\n\nspec_svm &lt;- \n  svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %&gt;%\n  set_mode(\"classification\")\n\nspec_lgb &lt;-\n  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(),\n             learn_rate = tune(), stop_iter = 10) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nNote how similar the code for each of these model specifications looks! tidymodels takes care of the “translation” from our unified syntax to the code that these algorithms expect.\nIf typing all of these out seems cumbersome to you, or you’re not sure how to define a model specification that makes sense for your data, the usemodels RStudio addin may help!"
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#round1",
    "href": "blog/2023-11-28-flights/index.html#round1",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "Evaluating models: round 1",
    "text": "Evaluating models: round 1\nWe’ll pair machine learning models with the recipes that make the most sense for them using workflow sets:\n\nwf_set &lt;-\n  # pair the basic recipe with a boosted tree and logistic regression\n  workflow_set(\n    preproc = list(basic = recipe_basic),\n    models = list(boost_tree = spec_lgb, logistic_reg = spec_lr)\n  ) %&gt;%\n  # pair the recipe that centers and scales input variables\n  # with the bagged models, support vector machine, and neural network\n  bind_rows(\n    workflow_set(\n      preproc = list(normalize = recipe_normalize),\n      models = list(\n        bag_tree = spec_bt,\n        bag_mars = spec_bm,\n        svm_rbf = spec_svm,\n        mlp = spec_nn\n      )\n    )\n  ) %&gt;%\n  # pair those same models with a more involved, principal component\n  # analysis preprocessor\n  bind_rows(\n    workflow_set(\n      preproc = list(pca = recipe_pca),\n      models = list(\n        bag_tree = spec_bt,\n        bag_mars = spec_bm,\n        svm_rbf = spec_svm,\n        mlp = spec_nn\n      )\n    )\n  )\n\nwf_set\n\n# A workflow set/tibble: 10 × 4\n   wflow_id           info             option    result    \n   &lt;chr&gt;              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n 1 basic_boost_tree   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 2 basic_logistic_reg &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 3 normalize_bag_tree &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 4 normalize_bag_mars &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 5 normalize_svm_rbf  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 6 normalize_mlp      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 7 pca_bag_tree       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 8 pca_bag_mars       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 9 pca_svm_rbf        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n10 pca_mlp            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nNow that we’ve defined each of our modeling configurations, we’ll fit them to the resamples we defined earlier. Here, tune_grid() is applied to each workflow in the workflow set, testing out a set of tuning parameter values for each workflow and assessing the resulting fit.\n\nwf_set_fit &lt;-\n  workflow_map(\n    wf_set, \n    fn = \"tune_grid\", \n    verbose = TRUE, \n    seed = 1,\n    resamples = flights_folds,\n    control = control_grid(parallel_over = \"everything\")\n  )\n\n\n\n\n\n\n\nNote\n\n\n\nworkflow_map() is calling tune_grid() on each modeling workflow we’ve created. You can read more about tune_grid() here.\n\n\n\nwf_set_fit\n\n# A workflow set/tibble: 8 × 4\n  wflow_id           info             option    result   \n  &lt;chr&gt;              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 basic_boost_tree   &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n2 normalize_bag_tree &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n3 normalize_bag_mars &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n4 normalize_mlp      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n5 pca_bag_tree       &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n6 pca_bag_mars       &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n7 pca_svm_rbf        &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n8 pca_mlp            &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n\n\nNote that the result column that was previously empty in wf_set now contains various tuning results, denoted by &lt;tune[+]&gt;, in wf_set_fit.\n\n\n\n\n\n\nNote\n\n\n\nThere’s a few rows missing here; I filtered out results that failed to tune.\n\n\nCollecting the information on performance from the resulting object:\n\n# first look at metrics:\nmetrics_wf_set &lt;- collect_metrics(wf_set_fit)\n\n\n# extract the top roc_auc values\nmetrics_wf_set %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  arrange(desc(mean)) %&gt;%\n  select(wflow_id, mean, n)\n\n# A tibble: 71 × 3\n   wflow_id            mean     n\n   &lt;chr&gt;              &lt;dbl&gt; &lt;int&gt;\n 1 normalize_bag_mars 0.615     8\n 2 normalize_bag_mars 0.613     8\n 3 pca_svm_rbf        0.608     8\n 4 normalize_bag_mars 0.606     8\n 5 pca_bag_mars       0.604     8\n 6 normalize_bag_mars 0.603     8\n 7 pca_svm_rbf        0.603     8\n 8 pca_bag_mars       0.601     8\n 9 pca_bag_mars       0.598     8\n10 normalize_bag_mars 0.598     8\n# ℹ 61 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe use one of the default metrics, roc_auc(), to evaluate our models here. Any metric defined using the yardstick package is fair game here (including custom metrics)!\n\n\nAlternatively, we can use the autoplot() method for workflow sets to visualize the same results:\n\nautoplot(wf_set_fit)\n\n\n\n\n\n\n\nIn terms of accuracy(), several of the models we evaluated performed quite well (with values near the event rate). With respect to roc_auc(), though, we can see that the bagged MARS models were clear winners."
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#round2",
    "href": "blog/2023-11-28-flights/index.html#round2",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "Evaluating models: round 2",
    "text": "Evaluating models: round 2\nIt looks like a bagged MARS model with centered and scaled predictors was considerably more performant than the other proposed models. Let’s work with those MARS results and see if we can make any further improvements to performance:\n\nmars_res &lt;- extract_workflow_set_result(wf_set_fit, \"normalize_bag_mars\")\n\nmars_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(recipe_normalize) %&gt;%\n  add_model(spec_bm)\n\nmars_sim_anneal_fit &lt;-\n  tune_sim_anneal(\n    object = mars_wflow,\n    resamples = flights_folds,\n    iter = 10,\n    initial = mars_res,\n    control = control_sim_anneal(verbose = TRUE, parallel_over = \"everything\")\n  )\n\nLooks like we did make a small improvement, though the model still doesn’t do much better than randomly guessing:\n\ncollect_metrics(mars_sim_anneal_fit) %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  arrange(desc(mean))\n\n\n\n# A tibble: 10 × 8\n   num_terms prod_degree .metric .estimator .config  mean     n std_err\n       &lt;int&gt;       &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1         4           2 roc_auc binary     Iter1   0.619     8  0.0191\n 2         3           2 roc_auc binary     Iter6   0.612     8  0.0249\n 3         5           2 roc_auc binary     Iter8   0.610     8  0.0185\n 4         4           1 roc_auc binary     Iter9   0.606     8  0.0154\n 5         3           2 roc_auc binary     Iter4   0.605     8  0.0211\n 6         4           1 roc_auc binary     Iter7   0.595     8  0.0205\n 7         3           2 roc_auc binary     Iter10  0.594     8  0.0192\n 8         3           1 roc_auc binary     Iter2   0.591     8  0.0248\n 9         2           1 roc_auc binary     Iter5   0.559     8  0.0183\n10         2           2 roc_auc binary     Iter3   0.555     8  0.0175\n\n\nJust like the workflow set result from before, the simulated annealing result also has an autoplot() method:\n\nautoplot(mars_sim_anneal_fit)\n\n\n\n\n\n\n\nWe can now train the model with the most optimal performance in cross-validation on the entire training set."
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#final-fit",
    "href": "blog/2023-11-28-flights/index.html#final-fit",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "The final model fit",
    "text": "The final model fit\nThe last_fit() function will take care of fitting the most performant model specification to the whole training dataset and evaluating it’s performance with the test set:\n\nmars_final_fit &lt;-\n  mars_sim_anneal_fit %&gt;%\n  # extract the best hyperparameter configuration\n  select_best(\"roc_auc\") %&gt;%\n  # attach it to the general workflow\n  finalize_workflow(mars_wflow, .) %&gt;%\n  # evaluate the final workflow on the train/test split\n  last_fit(flights_split)\n\nmars_final_fit\n\nThe test set roc_auc() for this model was 0.625. The final fitted workflow can be extracted from mars_final_fit and is ready to predict on new data:\n\nfinal_fit &lt;- extract_workflow(mars_final_fit)"
  },
  {
    "objectID": "blog/2023-11-28-flights/index.html#deploy",
    "href": "blog/2023-11-28-flights/index.html#deploy",
    "title": "Predicting flight delays with tidymodels🛩",
    "section": "Deploying to Connect",
    "text": "Deploying to Connect\nFrom here, all we’d need to do to deploy our fitted model is pass it off to vetiver for deployment to Posit Connect:\n\nfinal_fit_vetiver &lt;- vetiver_model(final_fit, \"simon.couch/flights\")\n\nboard &lt;- board_connect()\n\nvetiver_pin_write(board, final_fit_vetiver)\n\nvetiver_deploy_rsconnect(board, \"simon.couch/flights\")"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Some Pitters and Patters with R",
    "section": "",
    "text": "Kimi K2 and R Coding\n\n\nAn open-weights model released over the weekend by a little-known company has drawn quite a bit of attention. Is it any good?\n\n\n\n\n\nJul 14, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nClaude 4 and R Coding\n\n\nEvaluating the most recent releases of Claude Sonnet and Opus on challenging R coding problems.\n\n\n\n\n\nMay 27, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Gemini 2.5 Flash on R coding tasks\n\n\nGoogle’s Gemini 2.5 Pro release really made a splash last month. They’ve just announced an update to 2.5 Flash, a faster and cheaper model.\n\n\n\n\n\nMay 21, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the new Gemini 2.5 Pro update on R coding\n\n\nThe initial Gemini 2.5 Pro release a month ago was surprisingly strong, so I was excited to benchmark the update announced yesterday on R coding problems.\n\n\n\n\n\nMay 7, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating o3 and o4-mini on R coding performance\n\n\nWith a pair of coding-focused model drops, it’s clear OpenAI is aiming for the developer market. How well do the newest o3 and o4-mini perform on R coding tasks?\n\n\n\n\n\nApr 18, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nHow good are the GPT 4.1 models at writing R code?\n\n\nOpenAI dropped a trio of models yesterday that they claim result from “focusing closely on real-world developer needs.” I was curious whether this might mean stronger R coding performance.\n\n\n\n\n\nApr 15, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing chores\n\n\nA package to help with repetitive but hard-to-automate data science tasks.\n\n\n\n\n\nApr 11, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nHow Good Is Gemini 2.5 Pro at Writing R Code?\n\n\nSince Gemini 2.5 Pro’s release last week, I’ve been seeing a lot of hype claiming that the model is the new state of the art. How well does it know R?\n\n\n\n\n\nApr 2, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nHow I’m Using Claude Code to Develop R Packages\n\n\nI’ve come to really appreciate having Claude Code as part of my LLM toolkit, but it’s taken a bit of getting used to.\n\n\n\n\n\nMar 26, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nExploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements\n\n\nWhat happens when you ask language models to evaluate their own output?\n\n\n\n\n\nJan 30, 2025\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nWhich names that are also names of countries are most common?\n\n\nSitting around the living room over the weekend, someone asked this question. Let’s poke at an answer.\n\n\n\n\n\nDec 23, 2024\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing a new book\n\n\nToday, during R/Pharma 2024, I open-sourced the draft of a book I’ve been working on.\n\n\n\n\n\nOct 29, 2024\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nPostprocessing is coming to tidymodels\n\n\nThe tidymodels team has been hard at work on postprocessing, a set of features to adjust model predictions. The functionality includes a new package as well as changes across the framework.\n\n\n\n\n\nOct 16, 2024\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nA new package for profiling parallel R code\n\n\nPeriodic snapshots of CPU and memory usage consumed by R sessions enable better analyses of parallel computation.\n\n\n\n\n\nJul 15, 2024\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nHow to best parallelize boosted tree model fits with tidymodels\n\n\nShould tidymodels users use the parallelism implementations from XGBoost and LightGBM?\n\n\n\n\n\nMay 13, 2024\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring elapsed time to fit with tidymodels\n\n\nThe development versions of tidymodels packages now include tools to benchmark training time.\n\n\n\n\n\nApr 8, 2024\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nRun an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS\n\n\nStarting a containerized Oracle database on an Apple Silicon Mac.\n\n\n\n\n\nMar 14, 2024\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse (2023)\n\n\nAll of my friends are posting their Spotify Wrapped, and I’m jealous.\n\n\n\n\n\nNov 30, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nPredicting flight delays with tidymodels🛩\n\n\nTrying out a variety of machine learning models to predict flight delays out of Madison, WI.\n\n\n\n\n\nNov 28, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nQuarto!\n\n\nI recently rebuilt this site with Quarto.\n\n\n\n\n\nOct 24, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nDown the submodels rabbit hole with tidymodels\n\n\nDebugging something that turned out not to be a bug at all.\n\n\n\n\n\nOct 11, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nHiking the 2023 John Muir Trail / Nüümü Poyo\n\n\nA group of buddies and I walked 242 miles from Mt. Whitney to Yosemite Valley.\n\n\n\n\n\nSep 1, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing model parameters faster with tidymodels\n\n\nA couple small changes can greatly speed up the hyperparameter tuning process with tidymodels.\n\n\n\n\n\nAug 4, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nMoving On From Baltimore\n\n\nAfter 2 years in Baltimore, I recently made the move to Chicago, IL.\n\n\n\n\n\nJul 28, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nThe tidymodels is getting a whole lot faster\n\n\nRecent optimizations have made fits on small datasets much, much snappier.\n\n\n\n\n\nMar 24, 2023\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing my own music listening data with R and the tidyverse\n\n\nAll of my friends are posting their Spotify Wrapped, and I’m jealous.\n\n\n\n\n\nDec 1, 2022\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nRedirecting from sub-domains with Netlify\n\n\nAfter merging two websites that deployed from different branches, I had to figure out how to redirect my old blog posts to their new home.\n\n\n\n\n\nJul 20, 2022\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n{infer} v1.0.0 is on CRAN\n\n\nSome short reflections on working on the {infer} R package.\n\n\n\n\n\nAug 21, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nApplying to the NSF Graduate Research Fellowship (GRFP)\n\n\nA compilation of the advice that made me feel most informed and confident while applying for the NSF Graduate Research Fellowship.\n\n\n\n\n\nAug 2, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nPipe-esque Programming with {ggplot2}’s Plus Operator\n\n\nWriting iterative code with ‘+’ rather than ‘%&gt;%’ was a tough transition my first time around.\n\n\n\n\n\nJun 10, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nBig Things (Developer Documentation pt. 4)\n\n\nOn the tension between documenting R packages exhaustively and maintainably.\n\n\n\n\n\nMay 13, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nNaming the Things (Developer Documentation pt. 3)\n\n\nWeighing the pros and cons of several possible schemas for naming the core functions in {stacks}.\n\n\n\n\n\nMay 12, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nSplitting Things Up (Developer Documentation pt. 2)\n\n\nWhy {stacks} requires (at least) four separate functions to build an ensemble model rather than wrapping them all up into one.\n\n\n\n\n\nMay 11, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nHow {stacks} Came To Be (Developer Documentation pt. 1)\n\n\nIntroducing a set of blog posts reflecting on the development process of the {stacks} package.\n\n\n\n\n\nMay 10, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s Next For Me\n\n\n\n\n\n\n\n\nMar 23, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nApplying to Graduate School in Statistics and Biostatistics\n\n\nThere were all sorts of tips and tricks about applying to grad school that I was only able to learn about via office hours, personal meetings, and Twitter DMs, and I thought it would be worth publicly compiling some lessons learned.\n\n\n\n\n\nMar 15, 2021\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nRunning R Scripts on a Schedule with GitHub Actions\n\n\nSome pointers on running R scripts and committing their results to a GitHub repository on a regular interval using Actions.\n\n\n\n\n\nDec 27, 2020\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n{stacks} v0.1.0 is on CRAN!\n\n\nIntroducing ensemble learning to the tidymodels.\n\n\n\n\n\nNov 30, 2020\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Generating PDF Reports with the Tidyverse\n\n\nIn a few different roles over the past few years, I’ve come across the problem of programatically generating some kind of PDF reports from data. Here are some tips/tricks I’ve come across while making that happen.\n\n\n\n\n\nNov 12, 2020\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to Tidy Model Stacking\n\n\nModel stacking is an ensembling technique that involves training a model to combine the outputs of many diverse statistical models. The stacks package implements a grammar for tidymodels-aligned model stacking.\n\n\n\n\n\nOct 27, 2020\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nThe Blog Thing! We’re Doing It!\n\n\nFor a while now, I’ve wished I had a place to share some short posts on what I’m up to related to R, data science, and statistics. I decided I’d take a Sunday to get this subdomain up and running!\n\n\n\n\n\nOct 11, 2020\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2024-10-16-postprocessing/index.html",
    "href": "blog/2024-10-16-postprocessing/index.html",
    "title": "Postprocessing is coming to tidymodels",
    "section": "",
    "text": "Note\n\n\n\nThis is a cross-post of a post of mine on the tidyverse blog.\nWe’re bristling with elation to share about a set of upcoming features for postprocessing with tidymodels. Postprocessors refine predictions outputted from machine learning models to improve predictive performance or better satisfy distributional limitations. The developmental versions of many tidymodels core packages include changes to support postprocessors, and we’re ready to share about our work and hear the community’s thoughts on our progress so far.\nPostprocessing support with tidymodels hasn’t yet made it to CRAN, but you can install the needed versions of tidymodels packages with the following code.\npak::pak(\n  paste0(\n    \"tidymodels/\",\n    c(\"tune\", \"workflows\", \"rsample\", \"tailor\")\n  )\n)\nNow, we load packages with those developmental versions installed.\nlibrary(tidymodels)\nlibrary(probably)\nlibrary(tailor)\nExisting tidymodels users might have spotted something funky already; who is this tailor character?"
  },
  {
    "objectID": "blog/2024-10-16-postprocessing/index.html#meet-tailor",
    "href": "blog/2024-10-16-postprocessing/index.html#meet-tailor",
    "title": "Postprocessing is coming to tidymodels",
    "section": "Meet tailor👋",
    "text": "Meet tailor👋\nThe tailor package introduces tailor objects, which compose iterative adjustments to model predictions. tailor is to postprocessing as recipes is to preprocessing; applying your mental model of recipes to tailor should get you a good bit of the way there.\n\n\n\n\n\n\n\n\n\n\nTool\nApplied to...\nInitialize with...\nComposes...\nTrain with...\nPredict with...\n\n\n\nrecipes\nTraining data\nrecipe()\n\nstep_*()s\nprep()\nbake()\n\n\ntailor\nModel predictions\ntailor()\n\nadjust_*()ments\nfit()\npredict()\n\n\n\nFirst, users can initialize a tailor object with tailor().\n\ntailor()\n## \n## ── tailor ──────────────────────────────────────────────────────────────────────\n## A postprocessor with 0 adjustments.\n\nTailors compose “adjustments,” analogous to steps from the recipes package.\n\ntailor() %&gt;%\n  adjust_probability_threshold(threshold = .7)\n## \n## ── tailor ──────────────────────────────────────────────────────────────────────\n## A binary postprocessor with 1 adjustment:\n## \n## • Adjust probability threshold to 0.7.\n\nAs an example, we’ll apply this tailor to the two_class_example data made available after loading tidymodels.\n\nhead(two_class_example)\n##    truth      Class1       Class2 predicted\n## 1 Class2 0.003589243 0.9964107574    Class2\n## 2 Class1 0.678621054 0.3213789460    Class1\n## 3 Class2 0.110893522 0.8891064779    Class2\n## 4 Class1 0.735161703 0.2648382969    Class1\n## 5 Class2 0.016239960 0.9837600397    Class2\n## 6 Class1 0.999275071 0.0007249286    Class1\n\nThis data gives the true value of an outcome variable truth as well as predicted probabilities (Class1 and Class2). The hard class predictions, in predicted, are \"Class1\" if the probability assigned to \"Class1\" is above .5, and \"Class2\" otherwise.\nThe model predicts \"Class1\" more often than it does \"Class2\".\n\ntwo_class_example %&gt;% count(predicted)\n##   predicted   n\n## 1    Class1 277\n## 2    Class2 223\n\nIf we wanted the model to predict \"Class2\" more often, we could increase the probability threshold assigned to \"Class1\" above which the hard class prediction will be \"Class1\". In the tailor package, this adjustment is implemented in adjust_probability_threshold(), which can be situated in a tailor object.\n\ntlr &lt;-\n  tailor() %&gt;%\n  adjust_probability_threshold(threshold = .7)\n\ntlr\n## \n## ── tailor ──────────────────────────────────────────────────────────────────────\n## A binary postprocessor with 1 adjustment:\n## \n## • Adjust probability threshold to 0.7.\n\ntailors must be fitted before they can predict on new data. For adjustments like adjust_probability_threshold(), there’s no training that actually happens at the fit() step besides recording the name and type of relevant variables. For other adjustments, like numeric calibration with adjust_numeric_calibration(), parameters are actually estimated at the fit() stage and separate data should be used to train the postprocessor and evaluate its performance. More on this in Tailors in context.\nIn this case, though, we can fit() on the whole dataset. The resulting object is still a tailor, but is now flagged as trained.\n\ntlr_trained &lt;- fit(\n  tlr,\n  two_class_example,\n  outcome = truth,\n  estimate = predicted,\n  probabilities = c(Class1, Class2)\n)\n\ntlr_trained\n## \n## ── tailor ──────────────────────────────────────────────────────────────────────\n## A binary postprocessor with 1 adjustment:\n## \n## • Adjust probability threshold to 0.7. [trained]\n\nWhen used with a model workflow via add_tailor(), the arguments to fit() a tailor will be set automatically. Generally, as in recipes, we recommend that users add tailors to model workflows for training and prediction rather than using them standalone for greater ease of use and to prevent data leakage, but tailors are totally functional by themselves, too.\nNow, when passed new data, the trained tailor will determine the outputted class based on whether the probability assigned to the level \"Class1\" is above .7, resulting in more predictions of \"Class2\" than before.\n\npredict(tlr_trained, two_class_example) %&gt;% count(predicted)\n## # A tibble: 2 × 2\n##   predicted     n\n##   &lt;fct&gt;     &lt;int&gt;\n## 1 Class1      236\n## 2 Class2      264\n\nChanging the probability threshold is one of many possible adjustments available in tailor.\n\nFor probabilities: calibration\n\nFor transformation of probabilities to hard class predictions: thresholds, equivocal zones\n\nFor numeric outcomes: calibration, range\n\n\nSupport for tailors is now plumbed through workflows (via add_tailor()) and tune, and rsample includes a set of infrastructural changes to prevent data leakage behind the scenes. That said, we haven’t yet implemented support for tuning parameters in tailors, but we plan to implement that before this functionality heads to CRAN."
  },
  {
    "objectID": "blog/2024-10-16-postprocessing/index.html#tailors-in-context",
    "href": "blog/2024-10-16-postprocessing/index.html#tailors-in-context",
    "title": "Postprocessing is coming to tidymodels",
    "section": "Tailors in context",
    "text": "Tailors in context\nAs an example, let’s model a study of food delivery times in minutes (i.e., the time from the initial order to receiving the food) for a single restaurant. The deliveries data is available upon loading the tidymodels meta-package.\n\ndata(deliveries)\n\n# split into training and testing sets\nset.seed(1)\ndelivery_split &lt;- initial_split(deliveries)\ndelivery_train &lt;- training(delivery_split)\ndelivery_test  &lt;- testing(delivery_split)\n\n# resample the training set using 10-fold cross-validation\nset.seed(1)\ndelivery_folds &lt;- vfold_cv(delivery_train)\n\n# print out the training set\ndelivery_train\n## # A tibble: 7,509 × 31\n##    time_to_delivery  hour day   distance item_01 item_02 item_03 item_04 item_05\n##               &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n##  1             21.2  16.1 Tue       3.02       0       0       0       0       0\n##  2             17.9  12.4 Sun       3.37       0       0       0       0       0\n##  3             22.4  14.2 Fri       2.59       0       0       0       0       0\n##  4             30.9  19.1 Sat       2.77       0       0       0       0       0\n##  5             30.1  16.5 Fri       2.05       0       0       0       1       0\n##  6             35.3  14.7 Sat       4.57       0       0       2       1       1\n##  7             13.1  11.5 Sat       2.09       0       0       0       0       0\n##  8             18.3  13.4 Tue       2.35       0       2       1       0       0\n##  9             25.2  20.5 Sat       2.43       0       0       0       1       0\n## 10             30.7  16.7 Fri       2.24       0       0       0       1       0\n## # ℹ 7,499 more rows\n## # ℹ 22 more variables: item_06 &lt;int&gt;, item_07 &lt;int&gt;, item_08 &lt;int&gt;,\n## #   item_09 &lt;int&gt;, item_10 &lt;int&gt;, item_11 &lt;int&gt;, item_12 &lt;int&gt;, item_13 &lt;int&gt;,\n## #   item_14 &lt;int&gt;, item_15 &lt;int&gt;, item_16 &lt;int&gt;, item_17 &lt;int&gt;, item_18 &lt;int&gt;,\n## #   item_19 &lt;int&gt;, item_20 &lt;int&gt;, item_21 &lt;int&gt;, item_22 &lt;int&gt;, item_23 &lt;int&gt;,\n## #   item_24 &lt;int&gt;, item_25 &lt;int&gt;, item_26 &lt;int&gt;, item_27 &lt;int&gt;\n\nLet’s deliberately define a regression model that has poor predicted values: a boosted tree with only three ensemble members.\n\ndelivery_wflow &lt;-\n  workflow() %&gt;%\n  add_formula(time_to_delivery ~ .) %&gt;%\n  add_model(boost_tree(mode = \"regression\", trees = 3))\n\nEvaluating against resamples:\n\nset.seed(1)\ndelivery_res &lt;- \n  fit_resamples(\n    delivery_wflow, \n    delivery_folds, \n    control = control_resamples(save_pred = TRUE)\n  )\n\nThe \\(R^2\\) looks quite strong!\n\ncollect_metrics(delivery_res)\n## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n## 1 rmse    standard   9.52     10 0.0533  Preprocessor1_Model1\n## 2 rsq     standard   0.853    10 0.00357 Preprocessor1_Model1\n\nLet’s take a closer look at the predictions, though. How well are they calibrated? We can use the cal_plot_regression() helper from the probably package to put together a quick diagnostic plot.\n\ncollect_predictions(delivery_res) %&gt;%\n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)\n\n\n\n\n\n\n\nOoof.\nIn comes tailor! Numeric calibration can help address the correlated errors here. We can add a tailor to our existing workflow to “bump up” predictions towards their true value.\n\ndelivery_wflow_improved &lt;-\n  delivery_wflow %&gt;%\n  add_tailor(tailor() %&gt;% adjust_numeric_calibration())\n\nThe resampling code looks the same from here.\n\nset.seed(1)\ndelivery_res_improved &lt;- \n  fit_resamples(\n    delivery_wflow_improved, \n    delivery_folds, \n    control = control_resamples(save_pred = TRUE)\n  )\n\nChecking out the same plot reveals a much better fit!\n\ncollect_predictions(delivery_res_improved) %&gt;%\n  cal_plot_regression(truth = time_to_delivery, estimate = .pred)\n\n\n\n\n\n\n\nThere’s actually some tricky data leakage prevention happening under the hood here. When you add tailors to workflow and fit them with tune, this is all taken care of for you. If you’re interested in using tailors outside of that context, check out this documentation section in add_tailor()."
  },
  {
    "objectID": "blog/2024-10-16-postprocessing/index.html#whats-to-come",
    "href": "blog/2024-10-16-postprocessing/index.html#whats-to-come",
    "title": "Postprocessing is coming to tidymodels",
    "section": "What’s to come",
    "text": "What’s to come\nWe’re excited about how this work is shaping up and would love to hear yall’s thoughts on what we’ve brought together so far. Please do comment on our social media posts about this blog entry or leave issues on the tailor GitHub repository and let us know what you think!\nBefore these changes head out to CRAN, we’ll also be implementing tuning functionality for postprocessors. You’ll be able to tag arguments like adjust_probability_threshold(threshold) or adjust_probability_calibration(method) with tune() to optimize across several values. Besides that, post-processing with tidymodels should “just work” on the developmental versions of our packages—let us know if you come across anything wonky."
  },
  {
    "objectID": "blog/2024-10-16-postprocessing/index.html#acknowledgements",
    "href": "blog/2024-10-16-postprocessing/index.html#acknowledgements",
    "title": "Postprocessing is coming to tidymodels",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPostprocessing support has been a longstanding feature request across many of our repositories; we’re grateful for the community discussions there for shaping this work. Additionally, we thank Ryan Tibshirani and Daniel McDonald for fruitful discussions on how we might scope these features."
  },
  {
    "objectID": "blog/2024-04-08-fit-time/index.html",
    "href": "blog/2024-04-08-fit-time/index.html",
    "title": "Measuring elapsed time to fit with tidymodels",
    "section": "",
    "text": "tl;dr: The development versions of tidymodels packages include methods for a new extract function, extract_fit_time(), that returns the time required to train a workflow. Pass extract_fit_time() as a control option while tuning and run collect_extracts() to see training times for resampled workflows. In this example, we can identify a modeling workflow that trains more than 10x faster than the most performant model with very little decrease in predictive performance.\nA year ago, Emil put together a proof of concept for a function that would return measurements of how long it took for different tidymodels objects to fit. This had been a longstanding feature request across many of our repositories. Then, we got pretty busy implementing survival analysis and fairness metrics and only recently picked these changes back up to polish off the rough edges. We just merged them into the main developmental versions of the packages and are interested in hearing what folks think before they head off to CRAN!\nTo install the packages with these changes, use the following code:\npak::pak(\n  paste0(\n    \"tidymodels/\", \n    c(\"workflows\", \"recipes\", \"parsnip\", \"hardhat\")\n  )\n)\nNow, loading the tidymodels packages:\nlibrary(tidymodels)"
  },
  {
    "objectID": "blog/2024-04-08-fit-time/index.html#getting-started",
    "href": "blog/2024-04-08-fit-time/index.html#getting-started",
    "title": "Measuring elapsed time to fit with tidymodels",
    "section": "Getting started",
    "text": "Getting started\nFor a simpler example, let’s start off with just fitting a parsnip model. We’ll use the taxi data from modeldata as an example, predicting whether a trip will result in a tip or not:\n\ntaxi\n\n# A tibble: 10,000 × 7\n   tip   distance company                      local dow   month  hour\n   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n 1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n 2 yes       0.88 City Service                 yes   Thu   Mar       8\n 3 yes      18.1  other                        no    Mon   Feb      18\n 4 yes      20.7  Chicago Independents         no    Mon   Apr       8\n 5 yes      12.2  Chicago Independents         no    Sun   Mar      21\n 6 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n 7 yes      17.5  Flash Cab                    no    Fri   Mar      12\n 8 yes      17.7  other                        no    Sun   Jan       6\n 9 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n10 yes       1.47 City Service                 no    Tue   Mar      14\n# ℹ 9,990 more rows\n\n\nThe following code fits an XGBoost boosted tree with parsnip:\n\ntaxi_fit &lt;- fit(boost_tree(mode = \"classification\"), tip ~ ., taxi)\n\nWith these new package versions, we now have access to a function called extract_fit_time() that will return the elapsed time to fit the model:\n\nextract_fit_time(taxi_fit)\n\n# A tibble: 1 × 2\n  stage_id   elapsed\n  &lt;chr&gt;        &lt;dbl&gt;\n1 boost_tree   0.120\n\n\nCouldn’t we just wrap that whole fit() expression in system.time() and get the same thing, though?\nActually, no! extract_fit_time() returns the elapsed time to evaluate the engine fit, without tidymodels’ overhead on top. The differences between system.time() and the value of extract_fit_time() are exactly where this new function will come in handy.\ntidymodels doesn’t actually implement it’s own training algorithms for boosted trees. Instead, the framework takes in code with a common input interface, translates that code to pass off to modeling engines which take care of the actual training, and then translates the output back to a common output interface. That process can be visualized something like this:\n\n\n\n\n\n\n\n\nWhen viewed through the lens of elapsed time, the portion of the diagram in orange, labeled Step 2), is what is measured by extract_fit_time(). The portions of the elapsed time shown in green, labeled Steps 1) and 3), are tidymodels’ “overhead.” That could be measured as the difference between system.time() and extract_fit_time(). Let’s calculate that difference exactly:\n\ntaxi_elapsed &lt;- \n  system.time({\n    taxi_fit &lt;- fit(boost_tree(mode = \"classification\"), tip ~ ., taxi)\n  })[[\"elapsed\"]]\n\nJuxtaposing those two values:\n\ntaxi_elapsed\n\n[1] 0.119\n\nextract_fit_time(taxi_fit)$elapsed\n\n[1] 0.113\n\n\nThe total elapsed time of the fit is 0.119 seconds, and the portion of that elapsed time spent inside of XGBoost is 0.113 seconds. Said another way, the XGBoost fit itself accounts for 95% of the total time of this model fit.\n\n\n\n\n\n\nNote\n\n\n\nThe story is a bit more complicated for recipes, which also now have extract_fit_time() methods. We do implement the training routines for many of those ourselves, so the concept of “overhead” isn’t so straightforward there. For those methods, fit times refer to the elapsed times of prep() and bake() while each step is trained.\n\n\nMeasuring the relative overhead of tidymodels is of interest to us as the developers, of course. More importantly, though, these tools can also be used to get a sense for how long different parts of a larger ML pipeline take compared to each other and choose models that fit and predict more quickly (as long as predictions are similarly performant)."
  },
  {
    "objectID": "blog/2024-04-08-fit-time/index.html#resampling-a-model",
    "href": "blog/2024-04-08-fit-time/index.html#resampling-a-model",
    "title": "Measuring elapsed time to fit with tidymodels",
    "section": "Resampling a model",
    "text": "Resampling a model\nAs an example, let’s tune this XGBoost model using cross-validation. First, we’ll split the taxi data up into training and testing sets, resampling the training set:\n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi, prop = 0.8, strata = tip)\ntaxi_train &lt;- training(taxi_split)\ntaxi_test &lt;- testing(taxi_split)\ntaxi_folds &lt;- vfold_cv(taxi_train)\n\nThen, defining a boosted tree model that tunes learn_rate and trees; boosted tree ensembles with higher learn_rates might not perform as effectively, but they require fewer trees in the ensemble and thus train faster. We’d like to find the “sweet spot” of combinations of these parameters.\n\nbt &lt;- \n  boost_tree(\n    mode = \"classification\", \n    learn_rate = tune(), \n    trees = tune()\n  )\n\nNow, we’ll conduct a grid search for the best values of learn_rate and trees, passing the new extract_fit_time() function to the control function control_grid(). Every time we fit a model when resampling (nrow(taxi_folds) * grid = 10 * 10 = 100 times!), we’ll extract how long it took the engine to fit.\n\nbt_res &lt;- \n  tune_grid(\n    bt, \n    tip ~ ., \n    taxi_folds, \n    control = control_grid(extract = extract_fit_time)\n  )\n\nThis is what the bt_res object looks like:\n\nbt_res\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics          .notes           .extracts\n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;   \n 1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n 9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt; \n10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [30 × 6]&gt; &lt;tibble [0 × 4]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "blog/2024-04-08-fit-time/index.html#understanding-fit-time-and-performance",
    "href": "blog/2024-04-08-fit-time/index.html#understanding-fit-time-and-performance",
    "title": "Measuring elapsed time to fit with tidymodels",
    "section": "Understanding fit time and performance",
    "text": "Understanding fit time and performance\nEvery column in a tuning result that’s prefixed with a . has a collect_*() function associated with it that helps to summarize that column. We’ll use collect_extracts() to collect information on the extracts which are, in this case, elapsed fit times:\n\nbt_extracts &lt;- collect_extracts(bt_res)\n\nbt_extracts\n\n# A tibble: 100 × 5\n   id     trees learn_rate .extracts        .config              \n   &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;list&gt;           &lt;chr&gt;                \n 1 Fold01   922    0.00161 &lt;tibble [1 × 3]&gt; Preprocessor1_Model01\n 2 Fold01  1804    0.00293 &lt;tibble [1 × 3]&gt; Preprocessor1_Model02\n 3 Fold01    81    0.00394 &lt;tibble [1 × 3]&gt; Preprocessor1_Model03\n 4 Fold01   445    0.00886 &lt;tibble [1 × 3]&gt; Preprocessor1_Model04\n 5 Fold01  1074    0.0171  &lt;tibble [1 × 3]&gt; Preprocessor1_Model05\n 6 Fold01   347    0.0218  &lt;tibble [1 × 3]&gt; Preprocessor1_Model06\n 7 Fold01  1486    0.0393  &lt;tibble [1 × 3]&gt; Preprocessor1_Model07\n 8 Fold01  1391    0.0604  &lt;tibble [1 × 3]&gt; Preprocessor1_Model08\n 9 Fold01   788    0.111   &lt;tibble [1 × 3]&gt; Preprocessor1_Model09\n10 Fold01  1609    0.203   &lt;tibble [1 × 3]&gt; Preprocessor1_Model10\n# ℹ 90 more rows\n\n\nUnnesting on .extracts so we can see some of those timings:\n\nbt_extracts_unnested &lt;- bt_extracts %&gt;% unnest(cols = \".extracts\")\n\nbt_extracts_unnested\n\n# A tibble: 100 × 7\n   id     trees learn_rate stage    process_id elapsed .config              \n   &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                \n 1 Fold01   922    0.00161 workflow workflow     4.94  Preprocessor1_Model01\n 2 Fold01  1804    0.00293 workflow workflow     9.54  Preprocessor1_Model02\n 3 Fold01    81    0.00394 workflow workflow     0.444 Preprocessor1_Model03\n 4 Fold01   445    0.00886 workflow workflow     2.36  Preprocessor1_Model04\n 5 Fold01  1074    0.0171  workflow workflow     5.40  Preprocessor1_Model05\n 6 Fold01   347    0.0218  workflow workflow     1.8   Preprocessor1_Model06\n 7 Fold01  1486    0.0393  workflow workflow     7.34  Preprocessor1_Model07\n 8 Fold01  1391    0.0604  workflow workflow     6.99  Preprocessor1_Model08\n 9 Fold01   788    0.111   workflow workflow     3.87  Preprocessor1_Model09\n10 Fold01  1609    0.203   workflow workflow     7.99  Preprocessor1_Model10\n# ℹ 90 more rows\n\n\nThe times to fit vary quite a bit between workflows, to say the least:\n\nggplot(bt_extracts_unnested) +\n  aes(x = elapsed) +\n  geom_histogram(binwidth = 1, boundary = 0) +\n  scale_x_continuous(\n    breaks = seq(0, max(bt_extracts_unnested$elapsed) + 1, 1L)\n  ) +\n  labs(x = \"Elapsed fit time\")\n\n\n\n\n\n\n\nAlmost all of this variation in elapsed time is explained by the value of trees:\n\nggplot(bt_extracts_unnested) +\n  aes(x = elapsed, y = trees) +\n  geom_jitter() +\n  labs(x = \"Elapsed fit time\", y = \"# of trees\")\n\n\n\n\n\n\n\nThe “clumps” of points are the 10 different unique combinations of trees and learn_rates that are fitted on 10 different resamples; identical hyperparameters lead to very similar fit times, in this case.\nIn use cases where we will go on to resample/train this model many times (such as in iterative search or continuous training) it may be advantageous for us to pick a model that fits as quickly as possible as long as it has comparable performance with the overall most performant model. We can integrate the information from collect_metrics() with collect_extracts() to see if there any quick-fitting models with comparable performance. First, collecting metrics:\n\nbt_metrics &lt;- collect_metrics(bt_res)\n\nbt_metrics\n\n# A tibble: 30 × 8\n   trees learn_rate .metric     .estimator   mean     n  std_err .config        \n   &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n 1   922    0.00161 accuracy    binary     0.924     10 0.00214  Preprocessor1_…\n 2   922    0.00161 brier_class binary     0.0784    10 0.00128  Preprocessor1_…\n 3   922    0.00161 roc_auc     binary     0.639     10 0.0112   Preprocessor1_…\n 4  1804    0.00293 accuracy    binary     0.924     10 0.00218  Preprocessor1_…\n 5  1804    0.00293 brier_class binary     0.0693    10 0.00160  Preprocessor1_…\n 6  1804    0.00293 roc_auc     binary     0.628     10 0.0126   Preprocessor1_…\n 7    81    0.00394 accuracy    binary     0.924     10 0.00212  Preprocessor1_…\n 8    81    0.00394 brier_class binary     0.165     10 0.000467 Preprocessor1_…\n 9    81    0.00394 roc_auc     binary     0.636     10 0.0110   Preprocessor1_…\n10   445    0.00886 accuracy    binary     0.923     10 0.00211  Preprocessor1_…\n# ℹ 20 more rows\n\n\nJoining this output:\n\nbt_metrics_extracts &lt;-\n  left_join(\n    # metrics, summarized across resamples\n    bt_metrics,\n    # summarize fit times across resamples\n    summarize(\n      bt_extracts_unnested, \n      elapsed = mean(elapsed),\n      .by = c(trees, learn_rate, .config)\n    ),\n    by = c(\"trees\", \"learn_rate\", \".config\")\n  ) %&gt;%\n  relocate(elapsed, .before = everything())\n\nbt_metrics_extracts\n\n# A tibble: 30 × 9\n   elapsed trees learn_rate .metric     .estimator   mean     n  std_err .config\n     &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1   4.95    922    0.00161 accuracy    binary     0.924     10 0.00214  Prepro…\n 2   4.95    922    0.00161 brier_class binary     0.0784    10 0.00128  Prepro…\n 3   4.95    922    0.00161 roc_auc     binary     0.639     10 0.0112   Prepro…\n 4   9.55   1804    0.00293 accuracy    binary     0.924     10 0.00218  Prepro…\n 5   9.55   1804    0.00293 brier_class binary     0.0693    10 0.00160  Prepro…\n 6   9.55   1804    0.00293 roc_auc     binary     0.628     10 0.0126   Prepro…\n 7   0.441    81    0.00394 accuracy    binary     0.924     10 0.00212  Prepro…\n 8   0.441    81    0.00394 brier_class binary     0.165     10 0.000467 Prepro…\n 9   0.441    81    0.00394 roc_auc     binary     0.636     10 0.0110   Prepro…\n10   2.37    445    0.00886 accuracy    binary     0.923     10 0.00211  Prepro…\n# ℹ 20 more rows\n\n\nNow, plotting:\n\nggplot(bt_metrics_extracts) +\n  aes(x = elapsed, y = mean) +\n  geom_point() +\n  facet_wrap(~.metric, scales = \"free\") +\n  labs(x = \"Elapsed fit time\", y = \"Metric value\")\n\n\n\n\n\n\n\nIn this context, we see that w.r.t accuracy() and brier_class(), there’s not a clear relationship between the elapsed fit time and model performance. w.r.t. roc_auc(), longer-fitting models seem less performant.\n\n\n\n\n\n\nNote\n\n\n\nBy most measures, none of these models are particularly performant. Alas.🙂"
  },
  {
    "objectID": "blog/2024-04-08-fit-time/index.html#model-selection",
    "href": "blog/2024-04-08-fit-time/index.html#model-selection",
    "title": "Measuring elapsed time to fit with tidymodels",
    "section": "Model selection",
    "text": "Model selection\nWe can use the select_best() function from tune to pick the most performant model, without regard for elapsed fit time:\n\nbt_most_performant &lt;- select_best(bt_res, metric = \"roc_auc\")\n\nbt_most_performant\n\n# A tibble: 1 × 3\n  trees learn_rate .config              \n  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n1   922    0.00161 Preprocessor1_Model01\n\n\nUsing roc_auc(), ignoring fit time, we’d choose the model configuration Preprocessor1_Model01, corresponding to an roc_auc() of 0.639.\nInstead, we could choose the quickest-fitting model with a performance value within one standard error of the most performant model.\n\nbest_fit &lt;-\n  bt_metrics_extracts %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  arrange(desc(mean)) %&gt;%\n  select(mean, std_err, elapsed, .config)\n\nbest_speedy_fit &lt;- \n  bt_metrics_extracts %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  filter(mean &gt;= best_fit$mean - best_fit$std_err) %&gt;%\n  arrange(elapsed) %&gt;%\n  slice(1)\n\nbest_speedy_fit\n\n# A tibble: 1 × 9\n  elapsed trees learn_rate .metric .estimator  mean     n std_err .config       \n    &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         \n1   0.441    81    0.00394 roc_auc binary     0.636    10  0.0110 Preprocessor1…\n\n\n\n\n\n\n\n\nNote\n\n\n\nExisting tidymodels users may recognize this selection workflow as an ad-hoc version of select_by_one_std_err().\n\n\nNow, integrating our knowledge of fit time, we’d choose the model configuration Preprocessor1_Model03, corresponding to an roc_auc() of 0.636. The mean elapsed fit time for this model is 0.441 seconds, compared a mean elapsed fit time for the most performant model of 4.951 seconds, a speedup of 11.2x! Of course, this isn’t as much of an issue when model fits are as quick as they are here, but for more complex fits on larger datasets, an order of magnitude is a gamechanger."
  },
  {
    "objectID": "blog/2024-04-08-fit-time/index.html#what-do-you-think",
    "href": "blog/2024-04-08-fit-time/index.html#what-do-you-think",
    "title": "Measuring elapsed time to fit with tidymodels",
    "section": "What do you think?",
    "text": "What do you think?\nThese changes haven’t yet made it to the CRAN versions of tidymodels packages (and likely won’t for a good bit). Before they hit CRAN, we’d love to hear your thoughts on the interface and take any suggestions on how we might improve the interface."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "",
    "text": "Since last fall, I’ve been going through the process of applying to graduate school in (bio)statistics.1 I found that I was only able to learn about some parts of the process through office hours, personal meetings, and Twitter DMs, and I thought it would be worth publicly compiling some lessons learned.2🐥 I’m far from an expert about how this all works, and can only speak to my personal experience.\nA few things to note that influenced my personal experience: I’m a cis white man with U.S. citizenship who is an alumni (soon-to-be) of a private U.S. liberal arts college majoring in math with a concentration in statistics. I ultimately decided to apply to Ph.D. programs in Biostatistics in the U.S. during fall 2020. When I started my undergrad, I didn’t know what a Ph.D. was and had little—if any—sense for what graduate school looked like. However, by the time I was starting to think about starting my applications, I had learned a good bit more about what graduate school was. My test scores and GPA were quite unimpressive, but I’d been lucky enough to gain quite a bit of experience in statistical research and software development and had strong recommendation letters.\nThe order of this post roughly follows the order in which I asked myself questions about graduate school in (bio)statistics.\nMany of these answers speak to Ph.D. programs more so than M.A./M.S.’, and some may apply more accurately to biostatistics than statistics. I don’t have a good understanding of how many of these answers apply to schools outside of the U.S., and many of these answers depend on my lived experience in some other way.3 I’ll try to specify when I understand that to be the case.\nI’ve tried to be as forthcoming as possible while writing this up, as I’m not sure it helps anyone to keep so much of this information behind closed doors. I apologize if I’m unnecessarily frank."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-is-graduate-school-in-biostatistics",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-is-graduate-school-in-biostatistics",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "What is graduate school in (bio)statistics?",
    "text": "What is graduate school in (bio)statistics?\nThis was the most difficult question for me to answer and the question that resulted in the most people looking at me like I had just grown a second head when I asked it.\nA reality of graduate school: many people who attend(ed) graduate school grew up around a lot of people who attended graduate school. That does not mean you need to have grown up around a lot of people who attended graduate school to go (or so I’m told—we’ll see). In many important ways, though, it doesn’t look like your time as an undergraduate.\n“Graduate school,” at least in (bio)statistics, generally refers to Masters (M.A./M.S.) and Doctorate (Ph.D.) programs. Typically, you enter graduate school after receiving a Bachelors (B.A., undergraduate) degree, whether that’s directly after or after a few years of work experience. You can also apply to Ph.D. programs after having received an M.A./M.S. I’ll speak more to this in a bit, but M.A./M.S. programs typically take 1-2 years and present somewhat more like undergraduate programs. Ph.D. programs take longer—4-7 years—and look (and pay) a bit more like a job."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#should-i-go",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#should-i-go",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "Should I go?",
    "text": "Should I go?\nI recommend spending a good amount of time with this question, especially if you’re coming from an institutional setting where going to grad school feels like the “logical next step.” In some ways, it’s not.\nThe best first step to answering this question is learning a lot about what it means to attend grad school—for your finances, lifestyle, job prospects, and life timeline. I can speak somewhat to how these things could look if you do attend grad school, but how they might look if you don’t is more specific to you.\nThings you will do a lot of during your time in grad school:\n\ntake classes in statistics (and possibly fields of specialization)\nteach courses in statistics, but maybe also math\ntake part in research, including\n\nmeeting with lots of folks + talking science\nwriting math, code, papers\nattending conferences\nattending and giving talks\n\nall of the above things, at once. You’ll work quite a bit.\nnot get rich\n\nThe relative importance of those first three bullets can depend quite a bit on whether you’re doing an M.A./M.S. or Ph.D. More on that in a sec.\nIf most of the things above get you pretty stoked, maybe it’s the right thing.🦆"
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#how-much-does-it-cost",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#how-much-does-it-cost",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "How much does it cost?",
    "text": "How much does it cost?\nThis actually wasn’t one of my first questions, but it ought to have been. I assumed since grad school is, you know, school, you probably pay for it like undergrad. Sometimes (maybe often?) not—read on. :-)\n(Update, 8/2/21: I wrote a bit about the NSF GRFP, a fellowship program that can pay you even more for pursuing a Ph.D. You can read that post here.)"
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#should-i-do-an-m.a.m.s.-or-ph.d.",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#should-i-do-an-m.a.m.s.-or-ph.d.",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "Should I do an M.A./M.S. or Ph.D.?",
    "text": "Should I do an M.A./M.S. or Ph.D.?\nSome assorted thoughts on how the two are different and alike:\n\nM.A./M.S. programs are shorter (usually 1-2 years).\nM.A./M.S. programs tend to look a bit more like undergraduate programs in that 1) you usually pay to attend them and 2) the majority of the experience revolves around taking classes. You might also do some research or teach.\nThe first year or two of a Ph.D. is mostly focused on coursework. The latter part is generally based around you carrying out research and teaching undergraduate courses, and lasts something like 2-5 years. This research culminates in a dissertation, which is… a big paper, often composed of research papers you published during your time in the program, and some change.\nYou get paid to do a Ph.D. Bonkers. Usually, salaries (“stipends”) range from $20-35k annually (depending, among other things, on the cost of living in the city), also covering the cost of tuition, and require that you do some sort of research or teaching along the way.💰 Don’t do a Ph.D. if you will not be financially supported by your department. Some departments also offer signing bonuses, cover moving expenses, and provide pools of money for conference travel. Note, though, that in many cases, academic departments will often require you to pay for travel up front and then reimburse you after the fact. It’s also my sense that academic departments do not have the sense of urgency to pay their students in the way that other employers do—you may be doing lots of “checking in” in order to receive the payment you were promised.\n(Update, 8/2/21: I wrote a bit about the NSF GRFP, a fellowship program that can pay you even more for pursuing a Ph.D. You can read that post here.)\nIt seems like M.S. programs tend to offer a wider range of degree titles (like Data Science or Business Analytics) tailored to specific career goals. M.A. programs tend to look more like the first two years of a Ph.D. and are funded more often than M.S. programs.\nIt seems like Ph.D. programs allow you dive deeper into more specific concentrations in your latter years of the program.\nYou can apply to Ph.D. programs after graduating with an M.A./M.S.! Some M.A. programs will offer graduating students admission into their Ph.D. programs.\nGenerally, Ph.D. students seem to be treated better than M.A./M.S. students. Better access to office space, health insurance, faculty attention, campus amenities, etc.\n\nI’m not sure how well this applies to programs outside of the U.S."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#whats-the-difference-between-statistics-and-biostatistics",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#whats-the-difference-between-statistics-and-biostatistics",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "What’s the difference between statistics and biostatistics?",
    "text": "What’s the difference between statistics and biostatistics?\nOf all of the questions that I try to speak to in this blog post, I feel like this answer might be the most unsatisfactory for people who really know “what’s up.” I think many in the statistical community could benefit from speaking and listening earnestly to how we delineate these fields.\nI’ll list out a few of the main tendencies I’ve picked up on over the last year or two. In reality, these characteristics exist more so at the departmental level rather than at the “field” level, and you’ll see a lot of variation in how departments in either field position themselves relative to these traits.\n\nBiostatistics departments are usually situated in public health schools, while statistics departments tend be situated in schools of arts and sciences with some relation to the mathematics department.\nMany biostatistics departments seem to really value interdisciplinary research with collaborators from elsewhere in that school of public health. Statistics departments seem to be more self-sustaining in generating their research questions.\nBiostatistics seems to focus more on application, while statistics seems to focus more on theory. You will surely take part in both in either kind of program, though.\nStatistics departments seem to look to your math chops (however displayed) in admissions more so than biostatistics departments. Biostatistics departments seem to appreciate some non-math backgrounds more so than stat departments might, like software development or fields in public health. You’ll need math chops for either, though.\n\nThere are a few “applied statistics” programs out there as well. They tend to look somewhat more like biostatistics programs (omitting the first bullet point) yet draw from a very wide pool of disciplines in their collaborative work.\nIf any of these distinctions make you feel as if you’re particularly excited about one of biostatistics or statistics, I’d encourage you to rather look for programs that exhibit that trait rather than fall into the biostats/stats bin I mention above. These traits exist on spectra, and the biostats/stats feature here is only moderately predictive.4"
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#whats-the-deal-with-the-gre",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#whats-the-deal-with-the-gre",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "What’s the deal with the GRE?",
    "text": "What’s the deal with the GRE?\nThe GRE is something like the ACT/SAT of graduate admissions. In comparison to those tests, though, it’s more expensive and even less correlated with success in the program it’s supposed to test your preparation for. Nevertheless, it’s still a part of admissions for many programs, so it’s worth speaking to.5\nThe test will put you out 200 bucks or so, but fee waivers are available. They have some income/citizenship requirements, and decrease the cost of the test by 50 percent. Some undergraduate institutions have internal scholarships to help pay for taking the test, so that’s worth a look.🌞\nThere’s all sorts of advice out there about how to do well on the test, and I didn’t do well, so I won’t speak to that. A few stray notes about how the test is situated/regarded, though:\n\nThere’s a “general” test and a “subject” test. The general test feels more like the SAT/ACT, and is required by many more programs than the subject test. Generally, biostatistics programs don’t require the subject test. Some statistics programs do.\nThe general test is broken up into math, reading, and writing sections. The math and reading sections are graded on a scale from 130-170, and you receive a separate score for each. Apparently, these programs don’t care too much about your reading score. More emphasis is placed on the math score, though. Programs will typically mention some sort of distributional measures about their admitted students’ test scores on their admissions websites. The most competitive programs, if they require the GRE, tend to admit only students with near-perfect math sub-scores—think 166-70. 150s through mid-low 160s seem to be typical for less competitive programs.\nI didn’t take the subject test, but my understanding is that it’s very hard to achieve a score that will put you ahead in admissions unless you have significant coursework in computation-based upper-level mathematics courses and/or are willing to put significant time in to studying for the test.\nSome graduate schools use the GRE as a “filter.” At these schools, a score below some threshold means the committee may never put eyes on your application. I don’t have a good sense for how common this practice is.\n\nWho’s to say whether grad programs will stick with this decision to omit the GRE as an application requirement once in-person standardized testing is available again. Props to those who do."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#how-many-schools-should-i-apply-to",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#how-many-schools-should-i-apply-to",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "How many schools should I apply to?",
    "text": "How many schools should I apply to?\nThere are a few things to think about here.\nThe biggest limiting factor for me was price—it’s about 100 bucks per application. Most schools provide fee waivers, which are a varying degree of 1) financially helpful and 2) a pain in the ass to apply for. Generally, you might need to be on a Pell grant to apply for a fee waiver, and the waiver will cover most, but not all, of the application fee. Also, ask professors/mentors/staff at your institution about possible pools of money which you may able to draw from in order to help cover these fees.\nAnother thing to keep in mind is how you think about your chances of getting into the schools you apply to.6 If you feel you have a strong application and the schools you’re applying to aren’t particularly competitive, you might decide to apply to fewer schools than you otherwise would, though I’d caution from leaning on this sort of thinking too heavily. From what I’ve seen, folks’ rate of admission to grad programs has been much less correlated with that schools’ ranking than I expected while applying.😕\nSome sage advice I received that ultimately influenced me to decide to cut back on my number of applications: don’t apply anywhere that you don’t genuinely want to go to. If you’re not feeling excited about living in a city and working in a department for 4-7 years (or 1-2 for a Masters), be earnest with yourself—save your money and don’t apply.\nBottom line: I applied to seven schools and was rejected from most of them. I’ve heard of some folks applying to four or five, and some well into the teens. It seems like a typical number is seven to ten.🌿"
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#how-do-i-decide-which-schools-to-apply-to",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#how-do-i-decide-which-schools-to-apply-to",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "How do I decide which schools to apply to?",
    "text": "How do I decide which schools to apply to?\nI recommend starting with thinking about the things that are most important to you before thinking about specific programs. How important is location to you? The size of the school? Presence of faculty with a specific research interest? Prestige?7 Anything else?\nAnother big factor here is getting a sense for how competitive of an applicant you are. The answer to this might depend on the program, both in that some schools are harder to get into than others, and also that different programs look for different qualifications in their applicant pool. For instance, a statistics department with an emphasis on theory might really care about your performance in a real analysis course and GRE math scores, or a biostatistics department with an applied slant might be especially excited to see significant experience with statistical computing. In reality, most all school websites will say they look for—and offer—a good mix of both theory and application, and it’s hard to really know until you have a chance to talk with some folks in the department during an interview.\nSome criterion, like ranking and location, are much easier to find information about than others, like finding faculty who do research you’re interested in or determining a program’s orientation toward international students.8 I thus found it easier to start with the information that’s easier to access to initially filter down to a list of schools that I could reasonably research about the harder-to-find information. That harder-to-find information, though, will likely be what really helps you know whether you want to apply to a given program.\nIn reality, you might not know exactly what it was you’re looking for in a program. That was the case for me. This is what my process looked like:\n\nScan through the U.S. News Rankings to get a sense for what all is out there.9\nChoose 7-12 schools that are 1) in places you’re down to live in and 2) distributed somewhat uniformly across the rankings you think you have a chance at. For example, I really didn’t have a good sense for how competitive my application was, so my initial list was something like 1 school per “tens place” (e.g. one from the single digits, one from the tens, twenties, and so on through the eighties).\nPull up the websites for each of those schools. Read about the faculty and the town/city. Most departments have a faculty page that lists names and a few research interests—check out the personal websites of any professors you think might be cool to work with. Write down the names of professors whose work you found interesting and general research areas that seem to pop up a lot in their profiles.\nFor each school, ask yourself “Am I genuinely excited about this place?” If so, keep it on your list. If not, scratch it off.\nWhile you were checking these places out, did you learn anything about what you’re looking for in a program? Write down common characteristics of the schools you were interested in. If so, keep that in mind while you…\nBulk your list back up to 10 or so schools again with U.S. News—and your new criteria, if any—trying again to keep somewhat of a balance in terms of really highly-ranked schools and not-so-highly-ranked schools.\nIterate on steps 3-6, giving yourself a day or two between iterations to stew a bit, until you land on a list of schools that you feel excited about possibly spending a few years attending.\n\nA stray note—I wasn’t interested in joining departments that continue to value the GRE as an informative criterion for persistence/success in the program, despite the evidence to the contrary as well as its deleterious effects for diversity of class and race. It felt to me like a window into departmental culture. So that was another way I whittled down my options.🗜"
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-does-a-grad-school-application-look-like",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-does-a-grad-school-application-look-like",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "What does a grad school application look like?",
    "text": "What does a grad school application look like?\nA few of the common elements of these applications, binned by how important they seem to be:\n\nVery important:\n\nsolid letters of recommendation\none or more of: research,10 internship/work, or software development experience, or some other “selling point”\n\nImportant:\n\nthoughtful personal/research statement11\nthoughtful diversity statement, if applicable\nsolid grades in key courses\na lack of a negative internet presence\n\nGood to have:\n\nsolid grades in courses early on in undergrad\npositive internet presence\n\n\nAgain, the relative importance of each of these pieces will vary quite a bit depending on the program of interest, and I may be flat out wrong in some of my generalizations here.\nA solid letter of recommendation is from a professor or research mentor who knows you well and can speak to your specific strengths, ideally at length. Ideally, they have a terminal degree in their field (e.g. a Ph.D. in (bio)stat, math, etc.). Usually, programs will ask for three letters—ideally, at least one of those will be from a (bio)statistics professor or practitioner. If you’re applying to stat programs, one of these probably ought to be from a math professor, ideally your professor for Real Analysis, if you’ve taken it. If you’re applying to biostat programs, one of these probably ought to be from a research or internship mentor. You should keep your recommenders in the loop on how your application is coming together and where you’re applying (including the application deadlines for those programs).\nThere a few things I’m thinking about when I say “research, internship, work, or software development experience, or some other ‘selling point’.” For one, having done one of these things means you know what you’re getting into beyond coursework. If you’ve gotten a feel for any of these things, you’ll have a bit better sense for what grad school could be like. Also, having done one of these things likely means that you had a supervisor or collaborator that you worked closely with that can write you a strong and specific letter of recommendation. Lastly, having done one or more of these things will help you articulate your “story.”\nWhen I say “story,” I’m mostly thinking of the personal statement. Your personal statement gives you a chance to explain how it is that you became interested in grad school and how your previous experiences show that you will succeed there. There’s lot of advice out there about how to write a personal statement, and most of the prompts you’ll come across are very similar, so I won’t speak to this too much. Rohan Alexander12 recently wrote a really thoughtful blog post that spends more time with what a personal statement ought to look like (as well as some good notes on how to think about the role of recommenders).\nOne thing, though—I think some are hesitant to be publicly forthcoming in their workflow on personal statements. The private advice I’ve been given on how much time + effort to put into a personal statement has often tended to recommend much less care than that from advice I’ve read online. So, to be frank—personally, most of my personal statement was generic and sent to every program I applied to. For each program, I wrote a few sentences on why I was specifically interested in it, and pushed surrounding sentences from the generic document around as needed so that the program-specific statements flowed naturally. I also did Find + Replace for the program name and type (biostat vs. stat) in a couple places. Before submitting each to the official portal, I read the document in its entirety. I spent a weekend total on writing my personal statements, and had my two roommates give a round of edits. This does not include the time spent learning the information about programs I ultimately drew from in writing my statements.\nI mention “positive” and “negative” internet presence above. I’m generally thinking about what might come up if I look up your name with a search engine. Positive kinds of presence could be a personal website, LinkedIn, blog, professional Twitter, et cetera. If you’re an R user, no one writes better learning materials related to getting a personal website or blog started than Alison Hill. Check out her resources on blogdown if you’re interested! Negative kinds of presence are the typical social media goofballery you’ve probably been warned to be wary of participating in—just give a thoughtful eye to your privacy settings.🙂\nRegarding internet presence, there are many reasons why one may not want to be discoverable on the internet. Many of these relate to violence and harassment committed by cis men via the internet. Cis men, cut that shit out. My DMs are open if you want to talk with another cis man about being a more welcoming presence online—I’ll always have more to learn here as well."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-should-i-spend-the-most-time-on-while-applying",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-should-i-spend-the-most-time-on-while-applying",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "What should I spend the most time on while applying?",
    "text": "What should I spend the most time on while applying?\nIn my opinion, you ought to spend most of your time thinking about why it is that you want to go to grad school and finding programs that you believe will be able to offer that thing that you want. The process will be much, much easier easier if you’re genuinely stoked about the thought of ending up where you’re applying.\nOnce you’ve decided where you want to apply, here’s some sage advice from Rohan’s blog post linked above: “There are three bits to the application—transcript, letters, statement/CV. You’re spending four years getting transcripts, so you should also spend a lot of time on the other two.” You’ll likely need to spend a good amount of time putting together a thoughtful application. That said, grad school applications will take up as much time as you let them. This is one of the few spots in this post where I think it might be uncool to suggest specific “amounts of” something (time or effort, here) to spend applying, especially because I have no idea how long it took me and this will vary a lot from person to person.\nI will say that some of my most productive moments in figuring out what I wanted from grad school—and how to reflect that in my application—happened away from my computer. If it’s an option for you timeline- and mental health-wise, take a couple days away from your applications frequently."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#should-i-reach-out-to-potential-advisers-or-mention-their-names-in-my-application",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#should-i-reach-out-to-potential-advisers-or-mention-their-names-in-my-application",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "Should I reach out to potential advisers? Or mention their name(s) in my application?",
    "text": "Should I reach out to potential advisers? Or mention their name(s) in my application?\nAfter some discussions, an update to this section of this post, and further discussions on this section, I’m not sure I can speak confidently to this question.\nMy best advice here is to:\n\nRead professors’ Contact pages thoroughly to see if they appreciate/expect being reached out to.\nAsk your recommenders if they have thoughts about this practice, especially as related to the specific departments you’re applying to.\n\n:-)\n\n\nI’ve collapsed the previous iteration of this answer below!\n\nIt seems like the norm in some academic fields is to have specific advisers that you’d like to work with in mind before applying. In some, even, you might be expected to reach out to those advisers and have informational interviews before applying.\nMy sense is that this is generally not an expectation in (bio)statistics. Smaller schools may appreciate if you show that you know what some number of specific faculty do, but in general, do not feel like you need to reach out to specific faculty or even mention them in your application.\nYou should mention specific faculty in your application if there are people you’re really interested in working with (and especially if you really only want to go to a school if you’re able to work with a specific adviser). If you do so, expect that a school will take that seriously to some extent. For example, if you are given an interview with a school, that person might be your interviewer. If not, and you are accepted, that person might reach out to you personally following your acceptance. (My n here is really small.) If you mention a really prominent faculty member in your application, show that you know how their research interests relate to others’ in the department, and maybe mention another more junior professor or two.\nOnly send faculty an email while you’re applying if all of the following are true:\n\nYou really want to work with them\nYou’ve checked their website to see if they mention their boundaries re: being reached out to\nYou can’t find any signs that they are surely not taking students—if this is unclear from what you can find online, asking whether they’re taking students could be part of your introductory email!\n\nJohn Muschelli at Johns Hopkins Biostatistics gave some great advice related to what that introductory email could look like in a recent blog post.\nRegardless of whether you mention the names of specific faculty members, your application should reflect that you do have a sense for what faculty in the department research and that you have demonstrated interest in those research areas. Demonstrated interest could be prior research, but also speaking insightfully to the research area and its implications."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-do-i-do-after-i-apply",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#what-do-i-do-after-i-apply",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "What do I do after I apply?",
    "text": "What do I do after I apply?\nYou’ll likely wait a good while. Some programs list dates you’ll hear back by on their website, and some subset of those dates are correct/up-to-date. The latest date for most offers is April 15th, though offers start coming out as early as January. Biostatistics programs seem to have earlier application deadlines and get back to people earlier.🕰\nThere are online forums that exist where people post their admissions results and share their anxieties about the process. I peeked at them a bit, and ultimately found that, more than anything else, they just made me more anxious. For that reason, I won’t link them specifically here, but they exist, and you can find them if you so please.\nDepending on the program, you may be accepted outright via an email, or you may go through some rounds of interviews beforehand, or maybe even flown out to their campus to visit. Since I applied during the COVID-19 pandemic, my experience looked very, very different than yours probably will. John Muschelli wrote up some really thorough notes, though, on the post-application part of the grad admissions process, and he’s a wicked smart person. So, maybe, check his writing out.🌚\nHis sage advice from that post regarding this part of the process that I want to shout from the rooftops is this:\n\nAsking for things is not only OK, but is expected.\n\nEspecially once you’re in, or even interviewing, ask away. I’ve often felt like a nuisance with my questions about what my life will be like here soon, but once my wonderful advisor13 manages to convince me to send out the email, I’m always pleasantly surprised at how genuinely helpful and forthcoming interviewers, professors, and staff are."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#some-final-thoughts",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#some-final-thoughts",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "Some Final Thoughts",
    "text": "Some Final Thoughts\nMany blog posts in this genre have a common ending, and for good reason. It goes something like this: be good to yourself. There’s a lot of luck and randomness involved in this process—the more you can separate your self-worth from the results of your applications, the better.🐛\nOther people have spoken earnestly and helpfully to navigating this process; this is surely not the first or last post in this genre. Here are some links to other wonderful blog posts on similar topics, some of which I’ve cited already:\n\nRohan Alexander on your CV, personal statement, and rec letters\nJohn Muschelli on applying, interviewing, choosing between offers, early grad school life\nAmelia McNamara in a Q&A style post, including many, many links to helpful information and perspectives\n\nI’m not sure I’m comfortable sharing my application materials publicly. However, please feel free to shoot me an email if you’re going through this process and would appreciate giving a glance at the documents I ended up putting together. Mostly thinking of personal statement and CV here.\nAlso, if you identify as a person holding an underrepresented identity in (bio)statistics, I’d be glad to give some or all of your materials a read over and answer any other questions about the process you may have. Just give me a holler, no need to speak to the identities in question if you don’t want to.\nI’m generally, like, not a popular blogger, or whatever, so I’m not too worried on being overloaded with requests here… so please feel free to ask away! I’ll update this text if this changes.\nRegardless, if this post has been helpful for you, give me a holler. It’ll make me smile.🌸`\n\nThanks to Grayson White (@graysonwwhite) for his feedback on this post. Incredibly helpful.\nEdit, 25 March 2021: Added some additional thoughts thanks to enlightening conversations with Keshav Motwani (@keshav_motwani)."
  },
  {
    "objectID": "blog/2021-03-15-apply-to-stats-grad-school/index.html#footnotes",
    "href": "blog/2021-03-15-apply-to-stats-grad-school/index.html#footnotes",
    "title": "Applying to Graduate School in Statistics and Biostatistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThroughout this blog post, (bio)statistics refers to something like “statistics and/or biostatistics.”🐳↩︎\nIf you were one of those people with whom I met to help me navigate this process, thank you. I’m better for knowing you.↩︎\nNamely, in being a U.S. Citizen. While the mechanisms by which this takes place remain opaque to me, it seems to be commonly understood that ones experience with the U.S. graduate school admissions process is heavily influenced by U.S. citizenship. Which is fucked.↩︎\nI’d appreciate links to any blog posts or talks you’ve found helpful clarifying this distinction that I could link out to. PRs, tweets, whatever! Thanks to Paul Nguyen for passing along this one from Michael Lopez.↩︎\nI applied to graduate school during the COVID-19 epidemic, during which many schools waived their requirement for the GRE. Hopefully, the omission of these tests in admissions decisions is a more lasting phenomenon, but… power. Damn.↩︎\nYou may be thinking of a distribution here. It might be the poisson-binomial. Have at it.😉↩︎\nPrestige likely ought to be a factor if you’re applying to Ph.D. programs and your goal is to ultimately teach at the university level, especially at big, research-focused universities. This may not be as important if your goal is to ultimately enter the workforce following graduation.↩︎\nThe latter of which is fucked.↩︎\nLots to say here, for sure. I take issue with what a lot of these rankings represent and reinforce re: status obsession among the rich and/or well-educated, but I’d rather be up-front here. The U.S. News Rankings were my go-to for getting a sense for what programs were out there and, roughly, how they were regarded, and the same goes for others with whom I talked who were going through the same process.↩︎\nIncluding, but not limited to, published papers.↩︎\nApplications seemed to refer to this essay type by one of “Personal Statement” or “Research Statement.” The associated prompts didn’t seem to differ too much based on which word they used.↩︎\nAwesome person.☃️↩︎\nHi, Kelly. Thank you.↩︎"
  },
  {
    "objectID": "blog/2020-10-27-gentle-intro-stacks/index.html",
    "href": "blog/2020-10-27-gentle-intro-stacks/index.html",
    "title": "A Gentle Introduction to Tidy Model Stacking",
    "section": "",
    "text": "A few months ago, the tidymodels team coordinated a community survey to get a sense for what users most wanted to see next in the tidymodels ecosystem. One resounding theme from responses was that tidymodels users wanted a framework for tidymodels-aligned model stacking.\n\nI spent the latter half of my Summer 2020 internship with RStudio working on a package for model stacking in the tidymodels, and have since continued this work as the subject of my undergraduate thesis at Reed College.\nModel stacking is an ensembling technique that involves training a model to combine the outputs of many diverse statistical models. The stacks package implements a grammar for tidymodels-aligned model stacking.\nTo demonstrate how to build an ensemble with stacks, we’ll make use of this week’s TidyTuesday data on Canadian wind turbines. From a recent article from the National Observer by Carl Meyer:\n\nNatural Resources Canada has published the Canadian Wind Turbine Database, which contains the precise latitude and longitude of every turbine, along with details like its dimensions, its power output, its manufacturer and the date it was commissioned. There is also an interactive map… “For the first time, Canadians have access to centralized geographic and technical information on individual wind turbines that make up individual wind farms, (which) were all collected prior to now very much on an aggregated basis,” said Tom Levy, senior wind engineer at the federal Natural Resources Department.\n\nMaking use of the stacks package, we’ll build a stacked ensemble model to predict turbine capacity in kilowatts based on turbine characteristics.\nWe’ll load up needed packages and then get going!\nlibrary(tidyverse)\nlibrary(tidymodels)\nIn addition to the tidyverse and tidymodels, we’ll load the stacks package. If you haven’t installed the stacks package before, you can use the following code:\nremotes::install_github(\"tidymodels/stacks\", ref = \"main\")\nYou might be prompted to update some packages from the tidymodels; make sure to update to the developmental version of tune. Now, loading the stacks package:\nlibrary(stacks)\n\nData\nwind_raw &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-27/wind-turbine.csv')\nFirst thing, we’ll subset down to variables that we’ll use in the stacked ensemble model. For the most part, I’m just getting rid of ID variables and qualitative variables with a lot of levels.\nwind &lt;-\n  wind_raw %&gt;%\n  select(\n    province_territory, \n    total_project_capacity_mw,\n    turbine_rated_capacity_kw = turbine_rated_capacity_k_w,\n    rotor_diameter_m,\n    hub_height_m,\n    year = commissioning_date\n  ) %&gt;%\n  group_by(province_territory) %&gt;%\n  mutate(\n    year = as.numeric(year),\n    province_territory = case_when(\n      n() &lt; 50 ~ \"Other\",\n      TRUE ~ province_territory\n    )\n  ) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  ungroup()\n\n\nCreating Model Definitions\nAt the highest level, ensembles are formed from model definitions. In this package, model definitions are an instance of a minimal workflow, containing a model specification (as defined in the parsnip package) and, optionally, a preprocessor (as defined in the recipes package). Model definitions specify the form of candidate ensemble members.\n\nNote that the diagrams will refer to a K-nearest neighbors, linear regression, and neural network. In these examples, we’ll use different model types!\nDefining the constituent model definitions is undoubtedly the longest part of building an ensemble with stacks. If you’re familiar with tidymodels “proper,” you’re probably fine to skip this section, with one note: you’ll need to save the assessment set predictions and workflow utilized in your tune_grid(), tune_bayes(), or fit_resamples() objects by setting the control arguments save_pred = TRUE and save_workflow = TRUE. Note the use of the control_stack_*() convenience functions below!\nTo be used in the same ensemble, each of these model definitions must share the same resample. This rsample rset object, when paired with the model definitions, can be used to generate the tuning/fitting results objects for the candidate ensemble members with tune.\n\nWe’ll first start out with splitting up the training data, generating resamples, and setting some options that will be used by each model definition.\nSplitting up the wind data using rsample:\n# split into training and testing sets\nset.seed(1)\nwind_split &lt;- initial_split(wind)\nwind_train &lt;- training(wind_split)\nwind_test  &lt;- testing(wind_split)\n\n# use a 5-fold cross-validation\nset.seed(1)\nfolds &lt;- rsample::vfold_cv(wind_train, v = 5)\nNow, with the recipes and workflows packages, we’ll set up a foundation for all of our model definitions. Each model definition will try to predict turbine_rated_capacity_kw using the remaining variables in the data.\n# set up a basic recipe\nwind_rec &lt;- \n  recipe(turbine_rated_capacity_kw ~ ., data = wind_train) %&gt;%\n  step_dummy(all_nominal()) %&gt;%\n  step_zv(all_predictors())\n\n# define a minimal workflow\nwind_wflow &lt;- \n  workflow() %&gt;% \n  add_recipe(wind_rec)\nWe’ll use the root mean squared error, defined using the yardstick package, as our metric in this tutorial:\nmetric &lt;- metric_set(rmse)\nTuning and fitting results for use in ensembles need to be fitted with the control arguments save_pred = TRUE and save_workflow = TRUE—these settings ensure that the assessment set predictions, as well as the workflow used to fit the resamples, are stored in the resulting object. For convenience, stacks supplies some control_stack_*() functions to generate the appropriate objects for you.\nIn this example, we’ll be working with tune_grid() and fit_resamples() from the tune package, so we will use the following control settings:\nctrl_grid &lt;- control_stack_grid()\nctrl_res &lt;- control_stack_resamples()\nWe’ll define three different model definitions to try to predict turbine capacity—a linear model, a spline model (with hyperparameters to tune), and a support vector machine model (again, with hyperparameters to tune).\nStarting out with linear regression:\n# create a linear model definition\nlin_reg_spec &lt;-\n  linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# add it to a workflow\nlin_reg_wflow &lt;- \n  wind_wflow %&gt;% \n  add_model(lin_reg_spec)\n\n# fit to the 5-fold cv\nset.seed(1)\nlin_reg_res &lt;- \n  fit_resamples(\n    lin_reg_wflow,\n    resamples = folds,\n    metrics = metric,\n    control = ctrl_res\n  )\nSince this model definition only defines one model (i.e. doesn’t have any hyperparameters to tune), we use fit_resamples() rather than tune_grid().\nNow, moving on to the spline model definition:\n# modify the recipe and use the same linear reg spec\nspline_rec &lt;- \n  wind_rec %&gt;%\n  step_ns(rotor_diameter_m, deg_free = tune::tune(\"length\"))\n\n# add it to a workflow\nspline_wflow &lt;- \n  workflow() %&gt;% \n  add_recipe(spline_rec) %&gt;% \n  add_model(lin_reg_spec)\n\n# tune deg_free and fit to the 5-fold cv\nset.seed(1)\nspline_res &lt;- \n  tune_grid(\n    spline_wflow,\n    resamples = folds,\n    metrics = metric,\n    control = ctrl_grid\n  )\nFinally, putting together the model definition for the support vector machine:\n# define a model using parsnip\nsvm_spec &lt;- \n  svm_rbf(\n    cost = tune(), \n    rbf_sigma = tune()\n  ) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"regression\")\n\n# add it to a workflow\nsvm_wflow &lt;- \n  wind_wflow %&gt;% \n  add_model(svm_spec)\n\n# tune cost and rbf_sigma and fit to the 5-fold cv\nset.seed(1)\nsvm_res &lt;- \n  tune_grid(\n    svm_wflow, \n    resamples = folds, \n    grid = 5,\n    control = ctrl_grid\n  )\nWith these three model definitions fully specified, we’re ready to start putting together an ensemble! In most applied settings, you’d probably specify a few more models—Max specified over 300 in a talk a few weeks ago on this package—but this will do for demonstration purposes!\n\n\nAdding Candidates to a Data Stack\nCandidate members first come together in a data_stack object through the add_candidates() function. Principally, these objects are just tibbles, where the first column gives the true outcome in the assessment set, and the remaining columns give the predictions from each candidate ensemble member. (When the outcome is numeric, there’s only one column per candidate ensemble member. Classification requires as many columns per candidate as there are levels in the outcome variable.) They also bring along a few extra attributes to keep track of model definitions.\n\nThe first step to building a data stack is the initialization step. The stacks() function works sort of like the ggplot() constructor from ggplot2—the function creates a basic structure that the object will be built on top of—except you’ll pipe the outputs rather than adding them with +.\nstacks()\n## # A data stack with 0 model definitions and 0 candidate members.\nThe add_candidates() function adds candidate ensemble members to the data stack!\nwind_data_st &lt;- \n  stacks() %&gt;%\n  add_candidates(lin_reg_res) %&gt;%\n  add_candidates(spline_res) %&gt;%\n  add_candidates(svm_res)\n\nwind_data_st\n## # A data stack with 3 model definitions and 15 candidate members:\n## #   lin_reg_res: 1 sub-model\n## #   spline_res: 9 sub-models\n## #   svm_res: 5 sub-models\n## # Outcome: turbine_rated_capacity_kw (numeric)\nAs mentioned before, under the hood, a data_stack object is really just a tibble with some extra attributes. Checking out the actual data:\nas_tibble(wind_data_st)\n## # A tibble: 4,373 x 16\n##    turbine_rated_c… lin_reg_res_1_1 spline_res_5_1 spline_res_7_1 spline_res_8_1\n##               &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n##  1              150           -74.5          -116.          -434.          -437.\n##  2              600           502.            483.           407.           465.\n##  3              600           662.            638.           555.           573.\n##  4              600           649.            631.           537.           556.\n##  5              600           690.            665.           580.           579.\n##  6              660           731.            709.           656.           664.\n##  7             1300           905.            903.           954.          1007.\n##  8             1300           956.            949.           985.          1034.\n##  9             1300           912.            907.           957.           996.\n## 10             1300           940.            936.           988.          1036.\n## # … with 4,363 more rows, and 11 more variables: spline_res_2_1 &lt;dbl&gt;,\n## #   spline_res_3_1 &lt;dbl&gt;, spline_res_4_1 &lt;dbl&gt;, spline_res_1_1 &lt;dbl&gt;,\n## #   spline_res_6_1 &lt;dbl&gt;, spline_res_9_1 &lt;dbl&gt;, svm_res_1_4 &lt;dbl&gt;,\n## #   svm_res_1_2 &lt;dbl&gt;, svm_res_1_3 &lt;dbl&gt;, svm_res_1_5 &lt;dbl&gt;, svm_res_1_1 &lt;dbl&gt;\nThat’s it! We’re now ready to evaluate how it is that we need to combine predictions from each candidate ensemble member.\n\n\nCreating A Model Stack\nThen, the data stack can be evaluated using blend_predictions() to determine to how best to combine the outputs from each of the candidate members.\nThe outputs of each member are likely highly correlated. Thus, depending on the degree of regularization you choose, the coefficients for the inputs of (possibly) many of the members will zero out—their predictions will have no influence on the final output, and those terms will thus be thrown out.\n\nwind_model_st &lt;-\n  wind_data_st %&gt;%\n  blend_predictions()\n\nwind_model_st\n## # A tibble: 3 x 3\n##   member         type         weight\n##   &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;\n## 1 spline_res_9_1 linear_reg 0.737   \n## 2 svm_res_1_3    svm_rbf    0.323   \n## 3 svm_res_1_4    svm_rbf    0.000920\nThe blend_predictions function determines how member model output will ultimately be combined in the final prediction, and is how we’ll calculate our stacking coefficients. Now that we know how to combine our model output, we can fit the models that we now know we need on the full training set. Any candidate ensemble member that has a stacking coefficient of zero doesn’t need to be refitted!\nwind_model_st &lt;-\n  wind_model_st %&gt;%\n  fit_members()\n\nwind_model_st\n## # A tibble: 3 x 3\n##   member         type         weight\n##   &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;\n## 1 spline_res_9_1 linear_reg 0.737   \n## 2 svm_res_1_3    svm_rbf    0.323   \n## 3 svm_res_1_4    svm_rbf    0.000920\nNow that we’ve fitted the needed ensemble members, our model stack is ready to go! For the most part, a model stack is just a list that contains a bunch of ensemble members and instructions on how to combine their predictions.\n\nThis model_stack object is now ready to predict with new data!\n\n\nEvaluating Performance\nLet’s check out how well the model stack performs! Predicting on new data:\nwind_test &lt;- \n  wind_test %&gt;%\n  bind_cols(predict(wind_model_st, .))\nJuxtaposing the predictions with the true data:\nwind_test %&gt;%\n  ggplot() +\n  aes(\n    x = turbine_rated_capacity_kw, \n    y = .pred\n  ) +\n  geom_point() + \n  coord_obs_pred()\n\nLooks like our predictions were pretty strong! How do the stacks predictions perform, though, as compared to the members’ predictions? We can use the type = \"members\" argument to generate predictions from each of the ensemble members.\nmember_preds &lt;- \n  wind_test %&gt;%\n  select(turbine_rated_capacity_kw) %&gt;%\n  bind_cols(\n    predict(\n      wind_model_st, \n      wind_test, \n      members = TRUE\n    )\n  )\nNow, evaluating the root mean squared error from each model:\ncolnames(member_preds) %&gt;%\n  map_dfr(\n    .f = rmse, \n    truth = turbine_rated_capacity_kw, \n    data = member_preds\n  ) %&gt;%\n  mutate(member = colnames(member_preds))\n## # A tibble: 5 x 4\n##   .metric .estimator .estimate member                   \n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                    \n## 1 rmse    standard          0  turbine_rated_capacity_kw\n## 2 rmse    standard        254. .pred                    \n## 3 rmse    standard        260. spline_res_9_1           \n## 4 rmse    standard        642. svm_res_1_4              \n## 5 rmse    standard        337. svm_res_1_3\nAs we can see, the stacked ensemble outperforms each of the member models, though is closely followed by one of the spline members.\nVoila! You’ve now made use of the stacks package to predict wind turbine capacity using a stacked ensemble model!\n\n\nThat’s A Wrap!\nI appreciate yall checking out this tutorial, and hope you’re stoked to give stacks a try!\nIf it’s helpful, this is a completed version of the diagram we were putting together throughout this tutorial!\n\n\n\nSession Info\nsessionInfo()\n## R version 3.6.3 (2020-02-29)\n## Platform: x86_64-apple-darwin15.6.0 (64-bit)\n## Running under: macOS Catalina 10.15.4\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] glmnet_4.0-2         Matrix_1.2-18        kernlab_0.9-29      \n##  [4] vctrs_0.3.4          rlang_0.4.8          stacks_0.0.0.9000   \n##  [7] yardstick_0.0.7      workflows_0.2.1.9000 tune_0.1.1.9001     \n## [10] rsample_0.0.8.9000   recipes_0.1.13.9000  parsnip_0.1.3.9000  \n## [13] modeldata_0.1.0      infer_0.5.3.9000     dials_0.0.9.9000    \n## [16] scales_1.1.1         broom_0.7.2          tidymodels_0.1.1    \n## [19] forcats_0.5.0        stringr_1.4.0        dplyr_1.0.2         \n## [22] purrr_0.3.4          readr_1.4.0          tidyr_1.1.2         \n## [25] tibble_3.0.4.9000    ggplot2_3.3.2        tidyverse_1.3.0     \n## \n## loaded via a namespace (and not attached):\n##  [1] colorspace_1.4-1   ellipsis_0.3.1     class_7.3-15       fs_1.5.0          \n##  [5] rstudioapi_0.11    listenv_0.8.0      furrr_0.2.1        farver_2.0.3      \n##  [9] prodlim_2019.11.13 fansi_0.4.1        lubridate_1.7.9    xml2_1.3.2        \n## [13] codetools_0.2-16   splines_3.6.3      knitr_1.30         jsonlite_1.7.1    \n## [17] pROC_1.16.2        dbplyr_1.4.4       compiler_3.6.3     httr_1.4.2        \n## [21] backports_1.1.10   assertthat_0.2.1   cli_2.1.0          htmltools_0.5.0   \n## [25] tools_3.6.3        gtable_0.3.0       glue_1.4.2         Rcpp_1.0.5        \n## [29] cellranger_1.1.0   DiceDesign_1.8-1   blogdown_0.21      iterators_1.0.13  \n## [33] timeDate_3043.102  gower_0.2.2        xfun_0.18          globals_0.13.1    \n## [37] rvest_0.3.6        lifecycle_0.2.0    future_1.19.1      MASS_7.3-53       \n## [41] ipred_0.9-9        hms_0.5.3          parallel_3.6.3     yaml_2.2.1        \n## [45] curl_4.3           rpart_4.1-15       stringi_1.5.3      foreach_1.5.1     \n## [49] butcher_0.1.2      lhs_1.1.1          hardhat_0.1.4.9000 lava_1.6.8        \n## [53] shape_1.4.5        pkgconfig_2.0.3    evaluate_0.14      lattice_0.20-38   \n## [57] labeling_0.4.2     tidyselect_1.1.0   plyr_1.8.6         magrittr_1.5      \n## [61] bookdown_0.14      R6_2.4.1           generics_0.0.2     DBI_1.1.0         \n## [65] pillar_1.4.6       haven_2.3.1        withr_2.3.0        survival_3.1-8    \n## [69] nnet_7.3-12        modelr_0.1.5       crayon_1.3.4.9000  utf8_1.1.4        \n## [73] rmarkdown_2.5      usethis_1.6.3      grid_3.6.3         readxl_1.3.1      \n## [77] blob_1.2.1         reprex_0.3.0.9001  digest_0.6.27      GPfit_1.0-8       \n## [81] munsell_0.5.0\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2021-03-23-whats-next-s21/index.html",
    "href": "blog/2021-03-23-whats-next-s21/index.html",
    "title": "What’s Next For Me",
    "section": "",
    "text": "I’ve been a bit unsure about how personal this blog ought to be, and what qualifies as “significant” enough to post. A lot of changes are coming up soon for me, though, and this feels like a good place to document.\nFirst thing—I’ll be moving to Baltimore this fall to pursue my PhD in Biostatistics at the Johns Hopkins Bloomberg School of Public Health. My first three years will be supported by an NSF Graduate Research Fellowship. Unreal. So excited, grateful, intimidated, all of it.\nI want to be clear and forthcoming that I was rejected from the (vast) majority of grad schools I applied to. The admissions process is beyond tough, and my journey through it included plenty of failures.\nIn the meantime, I’ll be wrapping up my senior thesis project at Reed focusing on {stacks} and some of what I thought about as I was developing it. For the first couple months of this summer, I’ll working for RStudio making some major changes to the way the {infer} package walks and talks. For the last month or so of summer, I’ll be road tripping and having my last (socially-distanced) hurrahs with my good people in the Pacific Northwest. Road time, backpacking, camping, yada yada, and a trip back through Kansas to spend time with my folks. Really excited to take a moment before heading out to the East Coast, and going to miss living out here more than I can say.\nTagging along with me for it all will be Millie, an 11-week old border collie who grows and learns faster than I can wrap my head around. She has my heart.\n\nSo thankful for the people that have made this all possible for me. Money, trust, encouragement, opportunity, you name it—I’ve often felt that I’m the recipient of so much that I feel that I don’t deserve, and this all is no exception. Trying to do my best with it.\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2020-12-27-r-github-actions-commit/index.html",
    "href": "blog/2020-12-27-r-github-actions-commit/index.html",
    "title": "Running R Scripts on a Schedule with GitHub Actions",
    "section": "",
    "text": "I have an R script that queries data from a server that only supplies live data, and I want to run it on a regular basis, saving the results with each run. This problem gets at the intersection of a few things that are often hard (for me) to get right:\nI made many mistakes on the way to combining the solutions to these problems effectively, and thought it would be worth a short write-up of the approach I landed on! The solution looks something like this:\nFor an example of what the final product could look like, I’ve made a publicly-available example repository with everything you’ll need to get going! If you’re familiar with Git, forking this repository rather than setting yours up from scratch might save you some time. No worries if not.\nDisclaimer: I don’t know what I’m doing! There are likely better ways of the doing the thing I’m about to show you. Please feel free to file PRs or share other approaches. 🙂"
  },
  {
    "objectID": "blog/2020-12-27-r-github-actions-commit/index.html#write-your-r-script-save-the-things",
    "href": "blog/2020-12-27-r-github-actions-commit/index.html#write-your-r-script-save-the-things",
    "title": "Running R Scripts on a Schedule with GitHub Actions",
    "section": "Write your R script, save the things",
    "text": "Write your R script, save the things\nI imagine most uses for this type of setup will include querying some sort of livestreamed data. I’ll be using a relatively simple script in this example. First, I draw ten numbers from the standard normal, assigning them to an object x. Then, I save x with a filename that gives the time that I generated the numbers.\nx &lt;- rnorm(1:10)\nsave(x, file = paste0(\"data-raw/data_\", make.names(Sys.time()), \".Rda\"))\nAs I write this, the output of that file argument looks like:\n\"data-raw/data_X2020.12.27.08.26.06.Rda\"\n\n\n\nIt ain’t much, but it’s honest work.\n\n\nYour file path should be relative to your current working directory. I decided to make a folder called data-raw to save my results into. You’ll only need to save your results to file if you’d eventually like to commit them to the GitHub repository you’re working inside of.\nWhile you’re putting together your script, keep an eye on what R packages you’re making use of! Make sure you’re calling library(packagename) for each package that gets used in your script."
  },
  {
    "objectID": "blog/2020-12-27-r-github-actions-commit/index.html#situate-your-script-in-an-r-package",
    "href": "blog/2020-12-27-r-github-actions-commit/index.html#situate-your-script-in-an-r-package",
    "title": "Running R Scripts on a Schedule with GitHub Actions",
    "section": "Situate your script in an R package",
    "text": "Situate your script in an R package\nIf you haven’t worked with the internals of an R package before, this step might feel intimidating, but the R community has put together a few tools to make your experience as pleasant as possible!\nSituating your script in an R package means that you’ll have access to a lot of helpful tools to tell GitHub Actions, the server you’ll eventually run the script on, how to set up an R session like yours.\nTo get started, we’ll make use of the usethis package to set up a minimal package. If you haven’t installed usethis before, you can use the following code:\ninstall.packages(\"usethis\")\nCreating a package template:\nusethis::create_package(path = \"packagename\")\nusethis should automatically open the R project for the package you just made!\nThis function, among other things, will make you a DESCRIPTION file to fill out. Description files allow you to supply some basic information about a package, such as its name and description, the name of its authors, and the packages it relies on.\nFor the most part, if you want, you can leave this file as is. (I think it’s kind of fun to fill this stuff out, but you do your thing.🐣) If you use any non-base packages in your script, though, you will need to add an Imports: field. This field allows you to specify all of the packages your script uses so that GitHub Actions can install what it needs to before it runs your script. You’ll want to include any package you called library() on in your original script here. I don’t use the Imports: field at all in my example repository since I don’t use any non-base packages, but you can check out the stacks description file to see how Imports: is formatted!\nAfter editing the DESCRIPTION, drop your .R script in a folder called R—this is where most R scripts go in R packages. If you want to read more about best practices for writing R packages, check out the R Packages book by Hadley Wickham and Jenny Bryan."
  },
  {
    "objectID": "blog/2020-12-27-r-github-actions-commit/index.html#host-your-package-in-a-github-repository",
    "href": "blog/2020-12-27-r-github-actions-commit/index.html#host-your-package-in-a-github-repository",
    "title": "Running R Scripts on a Schedule with GitHub Actions",
    "section": "Host your package in a GitHub repository",
    "text": "Host your package in a GitHub repository\nIf you’re not familiar with Git and GitHub, this component of the workflow might be the trickiest for you. Karl Broman wrote a great primer on getting your package up on GitHub. For more details, you could check out the Git chapter of the R Packages book or, for the ultimate Git + R resource, Happy Git with R by Jenny Bryan.\nHosting our package on GitHub gives us access to Actions, which, for me, was the selling point of this approach. If you don’t have a GitHub Pro account, your repository will need to be publicly-available to have access to unlimited Actions runtime."
  },
  {
    "objectID": "blog/2020-12-27-r-github-actions-commit/index.html#set-up-a-github-action",
    "href": "blog/2020-12-27-r-github-actions-commit/index.html#set-up-a-github-action",
    "title": "Running R Scripts on a Schedule with GitHub Actions",
    "section": "Set up a GitHub Action",
    "text": "Set up a GitHub Action\nGitHub Actions is a tool that allows you to automate all sorts of software development tasks. In the R community, it’s widely used to check R packages hosted on GitHub every time they’re updated. I use GitHub Actions to help build this website! Here, we’ll use another feature of GitHub Actions: CRON scheduling. CRON is a job scheduler, allowing you to run scripts at a regular interval (or any specific time or set of times in the future, generally).\nThe building blocks of Actions are workflows. Workflows are YAML files that allow you to specify when and how to carry out some software development task. Here, our when is a regular interval of time—hourly, daily, etc. The how, in addition to your R script itself, involves telling Actions how to set up an R session that looks like yours. Thankfully, the R community has put together tools to set up an R session that looks like that which an R package requires. Since we’ve situated our script in an R package, we can make use of those tools.\nThe process for building our workflow will look something like this:\n\nSpecify your time interval\nSet up R\nRun your script\nSave the results of your script\n\nYou’ll first need to make your workflow file. It should live inside of a .github/workflows folder and have a .yaml extension. Mine looks like this:\n├── .github\n│   ├── workflows\n│       ├── schedule-commit.yaml\nThat schedule-commit file name can be whatever you want!\n\n1) Specify your time interval\nWe will use CRON to specify how often we want to run our script.\nThe most important part of CRON for you to understand is how to specify the interval of time you’re working with using an expression; do you want to run this script every 5 minutes? hourly? daily? monthly? These expressions are highly formatted strings that allow you to specify all sorts of different conditions that help you specify when to run a script. Rather than learning the rules for formatting these expressions, I recommend making use of one of many online tools to assist you in specifying your time interval. My favorite tool for generating CRON expressions is https://crontab.cronhub.io/.\nI want to run my script every hour. The CRON expression for this interval is 0 * * * *. Situated inside of the workflow formatting, it looks like:\non:\n  schedule:\n    - cron: \"0 * * * *\"\n\n\n2) Set up R\nNext, we’ll set up an R session on the Actions server. This approach borrows heavily from a few different template actions supplied by the r-lib team!\nBriefly, this component of the script\n\nspecifies the kind of build system to use (I use an ubuntu server here)\nsets R environmental variables and configures access to the GitHub repo\ninstalls R\ninstalls needed R packages (specified in the DESCRIPTION)\ncaches stuff that will be helpful to keep around for the next time this Action is run\n\njobs:\n  generate-data:\n    runs-on: ${{ matrix.config.os }}\n\n    name: ${{ matrix.config.os }} (${{ matrix.config.r }})\n\n    strategy:\n      fail-fast: false\n      matrix:\n        config:\n          - {os: ubuntu-latest,   r: 'release'}\n\n    env:\n      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true\n      RSPM: ${{ matrix.config.rspm }}\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - uses: r-lib/actions/setup-r@master\n        with:\n          r-version: ${{ matrix.config.r }}\n          http-user-agent: ${{ matrix.config.http-user-agent }}\n\n      - uses: r-lib/actions/setup-pandoc@master\n\n      - name: Query dependencies\n        run: |\n          install.packages('remotes')\n          install.packages('sessioninfo')\n          saveRDS(remotes::dev_package_deps(dependencies = TRUE), \".github/depends.Rds\", version = 2)\n          writeLines(sprintf(\"R-%i.%i\", getRversion()$major, getRversion()$minor), \".github/R-version\")\n        shell: Rscript {0}\n\n      - name: Cache R packages\n        uses: actions/cache@v1\n        with:\n          path: ${{ env.R_LIBS_USER }}\n          key: ${{ runner.os }}-${{ hashFiles('.github/R-version') }}-1-${{ hashFiles('.github/depends.Rds') }}\n          restore-keys: ${{ runner.os }}-${{ hashFiles('.github/R-version') }}-1-\n\n      - name: Install dependencies\n        run: |\n          remotes::install_deps(dependencies = TRUE)\n        shell: Rscript {0}\n\n\n3) Run your script\nWith R now set up, you’ll want to run the script you wrote up. Note the indentation here—this part of the script extends the “job” you have going already.\n      - name: Generate data\n        run: |\n          source(\"R/job.R\")\n        shell: Rscript {0} \nI called my own script job.R. You’ll want to switch that name out for whatever you called your own!\n\n\n4) Save the results of your script\nThis next step takes the results of your script that you’ve “saved” and stores them permanently in your repository. It configures a bot user and then commits and pushes for you. The second to last line gives the commit message that the bot will use for each commit.\n      - name: Commit files\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n          git add --all\n          git commit -am \"add data\"\n          git push \nAgain, notice the indentation! This part of the script extends the current job further.\nThere are many useful workflows for which this step isn’t needed. For example, the tidymodels team uses a repository called extratests to run additional unit tests on some of their packages every night. The Action runs checks on their packages on a schedule and just leaves the results in the metadata for the Action rather than pushing to the repository.\n\n\nThat’s a wrap!\nThat’s it!\nAfter you push this workflow and all of the other files in your package, you’re good to go. I’d recommend waiting an hour or so before checking in on your repository to see if it’s working; sometimes it takes Actions a bit to get up and running.\nIn your GitHub repository, the “Actions” tab will show you information about how your workflow went. To help debug, I like to include a short script at the end of my workflows to tell me the packages I had installed and their versions.\n      - name: Session info\n        run: |\n          options(width = 100)\n          pkgs &lt;- installed.packages()[, \"Package\"]\n          sessioninfo::session_info(pkgs, include_base = TRUE)\n        shell: Rscript {0}\nGenerally, debugging Actions can be pretty tricky compared to code you’ve run locally. I’d recommend running your script in full with a fresh R environment before pushing your work out to GitHub, and once there, lean on Google heavily.😉 In addition to the error message you’re seeing, it’s often helpful to include r-lib/actions or github actions r in your search query.\nTo see the full version of the workflow I’ve outlined here, check it out here. A full example repository with all of the code and metadata needed is publicly available.\nI appreciate you reading, and I hope this was helpful!"
  },
  {
    "objectID": "blog/2021-06-10-ggplot-pipe-plus/index.html",
    "href": "blog/2021-06-10-ggplot-pipe-plus/index.html",
    "title": "Pipe-esque Programming with {ggplot2}’s Plus Operator",
    "section": "",
    "text": "Writing iterative code with {ggplot2}’s plus (+) operator rather than {magrittr}’s pipe (%&gt;%) was a tough transition my first time around.\nWhen working on a function—say, boop()—that takes the outputs of some other function—say, beep():\n…I usually take for granted that I’ll have access to whatever beep(1) outputted, and will be able to modify that thing, inside of the boop function.\nWith {ggplot2}’s plus operator, though, I wasn’t sure whether this was the case. I thought that ggplot layers, added with +, had to be able to operate somewhat independently. e.g., if I’m making some function super_fancy_layer that adds a layer to a ggplot:\n…I had never thought about how super_fancy_layer() might “access” information from the earlier lines. What if I want super_fancy_layer() to modify bits and pieces of the existing plot, depending on what’s in the plot already, rather than adding a layer on top of the whole thing?1\nIt turns out, the maintainers of {ggplot2} have put together a pretty neat system that allows developers to access and modify previous {ggplot2} output in defining new layers."
  },
  {
    "objectID": "blog/2021-06-10-ggplot-pipe-plus/index.html#the-plan",
    "href": "blog/2021-06-10-ggplot-pipe-plus/index.html#the-plan",
    "title": "Pipe-esque Programming with {ggplot2}’s Plus Operator",
    "section": "The plan",
    "text": "The plan\nBefore we get into the details, I should mention that this post assumes a solid background in R (specifically, functions) and some awareness of the S3 object system. I’ll do my best to clarify the bits that are important here, but here’s some more extensive writing on functions, and here’s more on the S3 system.\nI’ll start off highlighting the infrastructure that the {ggplot2} team has put together for developers to approach this challenge before writing a bit about how you can take advantage of it (with some additional pointers on integrating it in a package)."
  },
  {
    "objectID": "blog/2021-06-10-ggplot-pipe-plus/index.html#wheee",
    "href": "blog/2021-06-10-ggplot-pipe-plus/index.html#wheee",
    "title": "Pipe-esque Programming with {ggplot2}’s Plus Operator",
    "section": "Wheee",
    "text": "Wheee\nIt starts with the + operator we all know and (maybe) love. The + you use in {ggplot2} code, like you see above, is actually a “method.” That is, the meaning of + here is defined in the {ggplot2} source code, specifically in reference to gg objects. Here’s the current source code:\n\"+.gg\" &lt;- function(e1, e2) {\n  if (missing(e2)) {\n    abort(\"Cannot use `+.gg()` with a single argument. Did you accidentally put + on a new line?\")\n  }\n\n  # Get the name of what was passed in as e2, and pass along so that it\n  # can be displayed in error messages\n  e2name &lt;- deparse(substitute(e2))\n\n  if      (is.theme(e1))  add_theme(e1, e2, e2name)\n  else if (is.ggplot(e1)) add_ggplot(e1, e2, e2name)\n  else if (is.ggproto(e1)) {\n    abort(\"Cannot add ggproto objects together. Did you forget to add this object to a ggplot object?\")\n  }\n}\nHere, e1 is what’s on the left-hand side of the + and e2 is what’s on the right-hand side. So, in the code:\nggplot(mtcars, aes(x = hp, y = mpg)) + geom_point()\n…e1 is ggplot(mtcars, aes(x = hp, y = mpg)) and e2 is geom_point(). Another (goofy) way of writing the above code is:\n`+`(ggplot(mtcars, aes(x = hp, y = mpg)), geom_point())\nIn the definition of +.gg, you may notice that the function eventually returns the output of add_ggplot(e1, e2, e2name) if the thing on the left-hand side of the + is a ggplot. Okay, sure.\nHere’s the current definition of add_ggplot().\nadd_ggplot &lt;- function(p, object, objectname) {\n  if (is.null(object)) return(p)\n\n  p &lt;- plot_clone(p)\n  p &lt;- ggplot_add(object, p, objectname)\n  set_last_plot(p)\n  p\n}\nOkay, so–hmm. The part here doing the work of combining what used to be e1 and e2 is ggplot_add(object, p, objectname). What’s the definition of ggplot_add?\nggplot_add &lt;- function(object, plot, object_name) {\n  UseMethod(\"ggplot_add\")\n}\nHuzzah! Yippee!\nWhat this UseMethod means is that the definition of ggplot_add depends on what kind of thing object is–the thing on the right-hand side of the +. When I say what kind of thing, I mean the output of class(object). ggplot_add is exported with ggplot2, so, as developers on extensions of {ggplot2}, we get to decide what the class of our super_fancy_layer() function is, so we also get to decide how ggplot_add will work in our extensions!\nAlso, notice that plot (what used to be e1, or the thing on the left-hand side of the +) is also an input to ggplot_add and the output should be the result of adding those two things together. So, in implementing super_fancy_layer(),\n\nwe have access to what the plot previously “was”\nwe can modify that thing and output the whole plot rather than just a layer on top of it\n\nTo use a custom ggplot_add method in your own package, you’ll want to start with re-exporting the ggplot_add generic. (“Generic” is just a word for a function that uses this sort of UseMethod construction, “dispatching” to the method defined for the given input class.) To do so, add the following {roxygen2} lines somewhere in your R code and run devtools::document():\n#' @importFrom ggplot2 ggplot_add\n#' @export\nggplot2::ggplot_add\nNow, define your super_fancy_layer() function. The idea here is that, since our contents of super_fancy_layer() need access to the underlying plot data, we won’t actually define all of the function’s logic inside of super_fancy_layer() (since requiring the underlying plot data as an explicit function argument for each layer is bad ggplot form). Instead, just store what the user inputted and wait until they add the layer to a ggplot to actually do anything.\n#' @export\nsuper_fancy_layer &lt;- function(arg1 = 1, arg2 = 2) {\n  # store inputs in classed output that can \n  # be passed to a `ggplot_add` method\n  structure(\n    \"A super fancy layer.\", \n    class = \"fancy_layer\",\n    fn = \"super_fancy_layer_\",\n    arg1 = arg1,\n    arg2 = arg2\n  )\n}\nSo… pretty lame so far. If I call:\nsuper_fancy_layer()\n## [1] \"A super fancy layer.\"\n## attr(,\"class\")\n## [1] \"fancy_layer\"\n## attr(,\"fn\")\n## [1] \"super_fancy_layer_\"\n## attr(,\"arg1\")\n## [1] 1\n## attr(,\"arg2\")\n## [1] 2\nAll that I’ve done is bundled up what the user passed to me into a little fancy_layer object.\nclass(super_fancy_layer())\n## [1] \"fancy_layer\"\nI also added an attribute with the name fn set to \"super_fancy_layer_\", an arbitrary function name for the internal code you will write that actually implements the addition–more on that in a sec.\nNow, think back to how that + operator works–it calls add_ggplot, which calls ggplot_add, and the code for ggplot_add depends on the class of what’s on the right-hand side of the + in the original code. We set the class of super_fancy_layer() output to fancy_layer, so all we need to do now is define the instructions for adding fancy_layers to existing ggplots.\nOur definition for ggplot_add.fancy_layer will:\n\nextract the fn attribute from fancy_layer output\nextract arguments arg1 and arg2 from fancy_layer output\ncall fn with the arguments plot, arg1, and arg2\n\n#' @method ggplot_add fancy_layer\n#' @export\nggplot_add.fancy_layer &lt;- function(object, plot, object_name) {\n  # a method for the `+` operator for fancy_layer objects.\n  # - \"object to add\" (arguments to the RHS of the `+`)\n  # - plot is the existing plot (on the LHS of the `+`)\n  # - object_name is the unevaluated call on the RHS of the `+`\n  \n  # extract the `fn` attribute from `fancy_layer` output\n  fn &lt;- attr(object, \"fn\")\n  \n  # extract arguments `arg1` and `arg2` from `fancy_layer` output\n  fancy_args &lt;- attributes(object)[!names(attributes(object)) %in% \n                                   c(\"class\", \"fn\")]\n  \n  # call `fn` with the arguments `plot`, `arg1`, and `arg2`\n  new_plot &lt;- do.call(\n    fn,\n    c(list(plot), fancy_args)\n  )\n  \n  # return the new plot\n  new_plot\n}\nThe do.call lines are a programmatic way of writing super_fancy_layer_(plot, arg1 = 1, arg2 = 2). That fn attribute is really just a string, but if there’s a function by the name of the string, do.call will call that function. The magic here is that fn (in our case, \"super_fancy_layer_\") can reference a function containing code defining the result of adding the left and right hand sides of the + based on both of them, rather than just the right-hand side. That is, you can peek inside of—and modify—the existing plot inside of your layer function! So, define the function in that fn attribute as you will:\nsuper_fancy_layer_ &lt;- function(plot, arg1, arg2) {\n  # fancy code that modifies `plot` based on\n  # arg1 and arg2...\n  \n  return(new_plot)\n}\n…and you’re good to go. Crazy."
  },
  {
    "objectID": "blog/2021-06-10-ggplot-pipe-plus/index.html#bonus-points-a-print-method",
    "href": "blog/2021-06-10-ggplot-pipe-plus/index.html#bonus-points-a-print-method",
    "title": "Pipe-esque Programming with {ggplot2}’s Plus Operator",
    "section": "Bonus points: a print method",
    "text": "Bonus points: a print method\nGetting the ggplot_add method working is the biggest leap here, but my first move after making that happen was to put together a print method for that new custom layer object. The default output will print out all of the attributes you attached to the object, which could be a bit overwhelming and confusing for the user.\nsuper_fancy_layer()\n## [1] \"A super fancy layer.\"\n## attr(,\"class\")\n## [1] \"fancy_layer\"\n## attr(,\"fn\")\n## [1] \"super_fancy_layer_\"\n## attr(,\"arg1\")\n## [1] 1\n## attr(,\"arg2\")\n## [1] 2\nEep.\nTo hide those internals from users, you can define your own print method–what gets printed out if a user calls super_fancy_layer() without adding it to an existing ggplot object with +?\nHere’s what my print method looks like:\n#' @export\nprint.fancy_layer &lt;- function(x, ...) {\n  cat(x)\n}\nWith this loaded, printing fancy_layers is a bit less overwhelming (if a bit underwhelming):\nsuper_fancy_layer()\n## A super fancy layer.\nWoop woop. :-)"
  },
  {
    "objectID": "blog/2021-06-10-ggplot-pipe-plus/index.html#wrapping-up",
    "href": "blog/2021-06-10-ggplot-pipe-plus/index.html#wrapping-up",
    "title": "Pipe-esque Programming with {ggplot2}’s Plus Operator",
    "section": "Wrapping up",
    "text": "Wrapping up\nI hope this was helpful for those who are currently encountering this problem and interesting for those who may in the future. Thanks to the {ggplot2} maintainers for putting together this infrastructure (specifically, Thomas Lin Pedersen in 2017) and to Hiroaki Yutani for writing the blog post that initially tipped me off to this. To yall maintainers, my apologies if this post encourages some poor form."
  },
  {
    "objectID": "blog/2021-06-10-ggplot-pipe-plus/index.html#footnotes",
    "href": "blog/2021-06-10-ggplot-pipe-plus/index.html#footnotes",
    "title": "Pipe-esque Programming with {ggplot2}’s Plus Operator",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI write this blog post assuming that the reader may have stumbled here already with a problem to solve. If you’re curious when an issue like this may come up, my original use case for this kind of functionality was in a PR for {infer}, a package for {tidyverse}-aligned statistical inference. We had a {patchwork} (a few ggplots smushed together) object and wanted the layer function to act on and modify each patch in the patchwork one-by-one rather than the whole plot. You can see that PR here.↩︎"
  },
  {
    "objectID": "blog/2023-03-24-speedups-2023/index.html",
    "href": "blog/2023-03-24-speedups-2023/index.html",
    "title": "The tidymodels is getting a whole lot faster",
    "section": "",
    "text": "The tidymodels packages provide a consistent, expressive, and safe interface to all sorts of modeling algorithms in R.\nWhile we develop the infrastructure that users interact with directly, under the hood, we send calls out to other people’s modeling packages—or engines—that provide the actual implementations that estimate parameters, generate predictions, etc. The process looks something like this:\nWhen thinking about the time allotted to each of the three steps above, we refer to the “translate” steps in green as the tidymodels overhead. The time it takes to “translate” interfaces in steps 1) and 3) is within our control, while the time the modeling engine takes to do it’s thing in step 2) is not.\nLet’s demonstrate with an example classification problem. Generating some random data:\n…we’d like to model the class using the remainder of the variables in this dataset using a logistic regression. We can using the following code to do so:\nThe default engine for a logistic regression in tidymodels is stats::glm(). So, in the style of the above graphic, this code:\nAgain, we can control what happens in steps 1) and 3), but step 2) belongs to the stats package.\nThe time that steps 1) and 3) take is relatively independent of the dimensionality of the training data. That is, regardless of whether we train on one hundred or a million data points, our code (as in, the translation) takes about the same time to run. Regardless of training set size, our code pushes around small, relational data structures to determine how to correctly interface with a given engine. The time it takes to run step 2), though, depends almost completely on the size of the data. Depending on the modeling engine, modeling 10 times as much data could result in step 2) taking twice as long, or 10x as long, or 100x as long as the original fit.\nSo, while the absolute time allotted to steps 1) and 3) is fixed, the portion of total time to fit a model with tidymodels that is “overhead” depends on how quick the engine code itself is. How quick is a logistic regression with glm() on 100 data points?\nAbout a millisecond. That means that, if the tidymodels overhead is one second, we’ve made this model fit a thousand times slower!\nIn practice,\nSo, to fit a boosted tree model or a bagged neural network on 100,000 data points, step 2) might take a couple seconds. And steps 1) and 3) don’t care about the size of the data, so they still take a tenth of a second. No biggie—the overhead is negligible.\nBecause of that rationale, our development energy on the tidymodels has focused much more heavily on creating an expressive, safe, and principled interface to statistical models in R. We’ve spent a lot of time on this, and feel pretty good about where we’re at. As the collection of packages matures, we have had the development energy to consider how we may better position ourselves to support modeling approaches beyond prediction-forward machine learning.\nOne group of practicioners that we feel we’re quite near to supporting well are folks working on causal inference. In many ways, our tools already accommodate causal workflows, and many of the benefits of the tidymodels apply in that setting too. However, the tidymodels overhead becomes more painful in some typical causal settings—often quick model fits (think stats::lm() and stats::glm()) on small datasets (think 100-10,000 rows). Add in resampling, and, oof.\nThe last few weeks, I’ve been focusing on trimming down the tidymodels overhead as much as possible. Taken together with the work that the tidyverse and r-lib teams have carried out on their side, our efforts have been tremendously productive, and these changes will start hitting the CRAN versions of our packages in coming weeks.\nTo get a better sense of our progress, I put together some quick simulations to quantify how much a speedup we see."
  },
  {
    "objectID": "blog/2023-03-24-speedups-2023/index.html#parsnip",
    "href": "blog/2023-03-24-speedups-2023/index.html#parsnip",
    "title": "The tidymodels is getting a whole lot faster",
    "section": "parsnip",
    "text": "parsnip\nMost of this work has focused on the parsnip package, which takes care of the actual translation and calling steps. We thus see the biggest speedups there.\nWe’ll evaluate the speedup across unique combinations of:\n\nModel type: Logistic regression, decision tree, and boosted tree.\nNumber of rows in training data: 100, 1,000, 10,000, 100,000, and 1,000,000.\n\nFor each pairing, we’ll benchmark how long it takes to fit the model to the data with the following code:\nspecs &lt;- list(\n  logistic_reg = logistic_reg(\"classification\"),\n  decision_tree = decision_tree(\"classification\"),\n  boost_tree = boost_tree(\"classification\")\n)\n\nbenchmarks &lt;- \n  bench::press(\n    rows = 10^(2:6),\n    spec = specs,\n    {\n      d &lt;- sim_classification(rows)\n      bench::mark(\n        fit = fit(spec[[1]], class ~ ., d)\n      )\n    }\n  )\nI ran this code first with the developmental versions of our packages installed, and then a second time with the CRAN versions. We can then divide the time that the CRAN versions took by the time that the dev versions did to calculate the relative speedup. See the appendix for more information on installed versions + reproducibility.\nTo process the results from the developmental versions, we select a few columns and note that these results were generated with developmental versions:\ndev_benchmarks_parsnip &lt;-\n  benchmarks %&gt;%\n  select(rows, spec, median, mem_alloc) %&gt;%\n  mutate(spec = names(spec),\n         versions = \"dev\")\nThe results look like this:\nhead(dev_benchmarks_parsnip)\n## # A tibble: 6 × 5\n##      rows spec          median      mem_alloc  versions\n##     &lt;dbl&gt; &lt;chr&gt;         &lt;bench_tm&gt;  &lt;bnch_byt&gt; &lt;chr&gt;   \n## 1     100 logistic_reg  0.001618680    1920248 dev     \n## 2    1000 logistic_reg  0.003882228    3866448 dev     \n## 3   10000 logistic_reg  0.032714679   38333328 dev     \n## 4  100000 logistic_reg  0.382652980  382690832 dev     \n## 5 1000000 logistic_reg  3.136099840 3830722416 dev     \n## 6     100 decision_tree 0.003887764    1394376 dev\n\nAfter doing the same for the CRAN versions, we bind the dev and CRAN results together, do some cleaning up, and then divide the time to fit with CRAN versions by the time to fit with developmental versions:\nbenchmarks_parsnip &lt;-\n  bind_rows(dev_benchmarks_parsnip, cran_benchmarks_parsnip) %&gt;%\n  mutate(\n    spec = paste0(spec, \"()\"),\n    spec = factor(spec, levels = c(\"logistic_reg()\", \"decision_tree()\", \"boost_tree()\")),\n    median = as.numeric(median)\n  ) %&gt;%\n  arrange(rows, spec, versions) %&gt;%\n  group_by(rows, spec) %&gt;%\n  summarize(speedup = median[1] / median[2], .groups = \"drop\")\nPlotting:\nggplot(benchmarks_parsnip) +\n  aes(x = rows, y = speedup, color = spec) + \n  geom_line(linewidth = .9) +\n  scale_x_log10() +\n  scale_y_continuous(breaks = c(1, 5, 10, 15, 20)) +\n  labs(\n    x = \"Number of rows in training data\",\n    y = \"Factor of speedup (CRAN / dev)\",\n    color = \"Model\\ntype\",\n    title = \"The parsnip package is getting a whole lot faster\",\n    subtitle = \"Relative change in time to evaluate model fits\"\n  ) +\n  theme_light() +\n  theme(plot.subtitle = element_text(face = \"italic\")) +\n  scale_color_manual(values = c(\"#c4b291\", \"#588d75\", \"#c5784f\"))\n\nTwo things worth noting:\n\nFor smaller datasets and quicker model fits, we see 20x faster model fits!\nOur changes do little to affect fit times for larger datasets—step 2) accounts for nearly all of this total time.\n\nA natural question after seeing this graph might revolve a different ratio—fit time with dev parsnip vs. fit time with the engine itself.\nLet’s benchmark that real quick. We’ll carry out the same super quick model fit on a small dataset, once with the base R interface and once with ours.\nd &lt;- sim_classification(100)\n\nbench::mark(\n  engine     = stats::glm(class ~ ., family = stats::binomial, data = d),\n  tidymodels = fit(logistic_reg(), class ~ ., data = d),\n  check = FALSE\n) %&gt;% \n  select(expression, median)\n## # A tibble: 2 × 2\n##   expression   median\n##   &lt;bch:expr&gt; &lt;bch:tm&gt;\n## 1 engine       1.12ms\n## 2 tidymodels   1.72ms\nSo, tidymodels adds something like 50% overhead, worst-case scenario. We feel good about that.\nAgain, that overhead quickly disappears. With 10,000 rows:\nd &lt;- sim_classification(10000)\n\nbench::mark(\n  engine     = stats::glm(class ~ ., family = stats::binomial, data = d),\n  tidymodels = fit(logistic_reg(), class ~ ., data = d),\n  iterations = 100, check = FALSE\n) %&gt;% \n  select(expression, median)\n## # A tibble: 2 × 2\n##   expression   median\n##   &lt;bch:expr&gt; &lt;bch:tm&gt;\n## 1 engine       26.7ms\n## 2 tidymodels   27.1ms\nZoom zoom!"
  },
  {
    "objectID": "blog/2023-03-24-speedups-2023/index.html#tidymodels",
    "href": "blog/2023-03-24-speedups-2023/index.html#tidymodels",
    "title": "The tidymodels is getting a whole lot faster",
    "section": "tidymodels",
    "text": "tidymodels\nIn reality, the whole game of statistical modeling involves much more than a single model fit. Resampling model fits, generating predictions from them, calculating performance metrics based on those predictions, and all of the other intricacies of interactive modeling take lots of time, and we’ve worked on optimizing those steps too.\nThe following benchmarks are just like those above, except in this example, we resample the logistic regression model with 10-fold cross validation. Replacing fit() with fit_resamples() and switching out the data d for its folds:\nbenchmarks &lt;- \n  bench::press(\n    rows = 10^(2:6),\n    spec = specs,\n    {\n      d &lt;- sim_classification(rows)\n      folds &lt;- vfold_cv(d)\n      bench::mark(\n        fit = fit_resamples(spec[[1]], class ~ ., folds)\n      )\n    }\n  )\nTidying up with the same steps and then plotting:\n\nAgain, we see major speedups for smaller datasets, and the effect gradually disappears with larger ones as engine fit time trumps our overhead.\nThe same sort of benchmark I appended to the last section, where I compare “just the engine fit” to our wrapper, would be quite a bit more involved here. The line for what constitutes “overhead” vs “implementation” is a bit harder to draw, as we implement code for resampling data, computing performance metrics, etc. ourselves. The big ideas here are:\n\nWe’ve addressed obvious slowdowns in resample evaluation code, and\nUsers can expect further speedups in coming months, but, again,\nWith larger dataset sizes, model fit and prediction take up increasingly large proportions of the total evaluation time."
  },
  {
    "objectID": "blog/2023-03-24-speedups-2023/index.html#appendix",
    "href": "blog/2023-03-24-speedups-2023/index.html#appendix",
    "title": "The tidymodels is getting a whole lot faster",
    "section": "Appendix",
    "text": "Appendix\nI used the following code to install CRAN versions on Wednesday, March 22, 2023. The renv snapshot is saved as cran_pkgs.lock.\npak::pkg_install(c(\n  c(\"parsnip\", \"dials\", \"recipes\", \"workflows\", \n    \"yardstick\", \"rsample\", \"hardhat\", \"broom\", \"tune\"),\n  c(\"dplyr\", \"tibble\", \"purrr\", \"tidyr\"),\n  c(\"rlang\", \"tidyselect\", \"vctrs\")\n))\n\nrenv::snapshot(lockfile = \"cran_pkgs.lock\")\nSimilarly, for dev_pkgs.lock:\npak::pkg_install(c(\n  paste0(\"tidymodels/\", c(\"parsnip\", \"dials\", \"recipes\", \"workflows\", \n                          \"yardstick\", \"rsample\", \"hardhat\", \"broom\", \"tune\")),\n  paste0(\"tidyverse/\", c(\"dplyr\", \"tibble\", \"purrr\", \"tidyr\")),\n  paste0(\"r-lib/\", c(\"rlang\", \"tidyselect\", \"vctrs\"))\n))\n\nrenv::snapshot(lockfile = \"dev_pkgs.lock\")\nNote that the “CRAN” packages include dplyr v1.1.0 rather than v1.1.1, even though v1.1.1 was on CRAN by the time I ran these simulations. Folks on the tidyverse and r-lib teams (hi, Davis and Lionel!) have been super receptive and helpful in addressing performance hitches that contributed to our overhead, and dplyr 1.1.1 contains a good few changes made with an eye for helping our efforts here. Thanks, yall. So, for a “before” benchmark that better represents where we started from, I use dplyr v1.1.0."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Simon P. Couch",
    "section": "",
    "text": "I appreciate you dropping by.\n\n\nMy name’s Simon. I build tools for data scientists at Posit (formerly called RStudio).\nI studied statistics and sociology at Reed College, and had a brief stint in a PhD program in Biostatistics at Johns Hopkins. Throughout my career so far, I’ve had a deep and enduring interest in statistical software development—I think, done well, statistical software has an incredible impact on our ability to think intuitively about statistics and data science.\nMost days, I’m working on open source R packages for data analysis, machine learning, and LLM code-assist. I’m also working on a book called Efficient Machine Learning with R. My GitHub profile links out to some of the projects I spend my time with.\nI’m currently based out of Chicago, IL. When I’m not working, I enjoy cooking, hanging with my dog Millie, and playing American folk music.\nThis site is built with Quarto, and I’ve repo-dived on Silvia Canelón, Julia Silge, and Emil Hvitfeldt’s personal websites while tweaking and troubleshooting. The site is deployed using Netlify. My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-12-23-names/index.html",
    "href": "blog/2024-12-23-names/index.html",
    "title": "Which names that are also names of countries are most common?",
    "section": "",
    "text": "Sitting around the living room over the weekend, someone asked “Which names that are also names of countries are most common?” We all gave guesses, and then I gave in to the urge to reach for my laptop and generate an answer that was authoritative enough for our purposes.\nThe data I can easily access answers a question more like: which names that are also English names of countries were most common in the USA from 1880 to 2017? Before you read further, try to guess!"
  },
  {
    "objectID": "blog/2024-12-23-names/index.html#setup",
    "href": "blog/2024-12-23-names/index.html#setup",
    "title": "Which names that are also names of countries are most common?",
    "section": "Setup",
    "text": "Setup\nWe’ll use two CRAN packages to help answer this question:\n\n\nbabynames, which contains a data frame by the same name containing counts of babies by name by year in the US.\n\ncountrycode, which contains a data frame that includes, among other things, (English) names of countries.\n\n\ninstall.packages(c(\"countrycode\", \"babynames\"))\n\nAlso, of course:\n\ninstall.packages(c(\"tidyverse\"))\n\nLoading those up:\n\nlibrary(countrycode)\nlibrary(babynames)\nlibrary(tidyverse)\n\nHere’s what the data on baby names looks like:\n\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# ℹ 1,924,655 more rows\n\n\nAnd the country data:\n\ncodelist\n\n# A tibble: 291 × 624\n   ar5      cctld continent  country.name.de     country.name.de.regex\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;               &lt;chr&gt;                \n 1 ASIA     .af   Asia       Afghanistan         afghan               \n 2 EIT      .al   Europe     Albanien            albanien             \n 3 MAF      .dz   Africa     Algerien            algerien             \n 4 ASIA     .as   Oceania    Amerikanisch-Samoa  ^(?=.*amerik).*samoa \n 5 OECD1990 .ad   Europe     Andorra             andorra              \n 6 MAF      .ao   Africa     Angola              angola               \n 7 LAM      .ai   Americas   Anguilla            anguill?a            \n 8 LAM      .aq   Antarctica Antarktis           ^(?!.*franz).*antark…\n 9 LAM      .ag   Americas   Antigua und Barbuda antigua              \n10 LAM      .ar   Americas   Argentinien         argentin             \n# ℹ 281 more rows\n# ℹ 619 more variables: country.name.en &lt;chr&gt;,\n#   country.name.en.regex &lt;chr&gt;, country.name.fr &lt;chr&gt;,\n#   country.name.fr.regex &lt;chr&gt;, country.name.it &lt;chr&gt;,\n#   country.name.it.regex &lt;chr&gt;, cow.name &lt;chr&gt;, cowc &lt;chr&gt;,\n#   cown &lt;dbl&gt;, currency &lt;chr&gt;, dhs &lt;chr&gt;, ecb &lt;chr&gt;, eu28 &lt;chr&gt;,\n#   eurocontrol_pru &lt;chr&gt;, eurocontrol_statfor &lt;chr&gt;, …\n\n\ncountry.name.en is our ticket."
  },
  {
    "objectID": "blog/2024-12-23-names/index.html#the-reveal",
    "href": "blog/2024-12-23-names/index.html#the-reveal",
    "title": "Which names that are also names of countries are most common?",
    "section": "The reveal",
    "text": "The reveal\nWe can use some of the core verbs from dplyr to answer our question:\n\ntop_names &lt;-\n  babynames %&gt;%\n  # only keep names that are also English country names.\n  filter(name %in% codelist$country.name.en) %&gt;%\n  # then, for each name...\n  group_by(name) %&gt;%\n  # take the sum of the counts across years, and...\n  summarize(n = sum(n)) %&gt;%\n  # show the highest counts at the top.\n  arrange(desc(n))\n\ntop_names\n\n# A tibble: 64 × 2\n   name          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Jordan   499903\n 2 Chad     240662\n 3 Georgia  151662\n 4 Israel    60698\n 5 Kenya     26199\n 6 India     22154\n 7 Unknown   18723\n 8 Malaysia   7444\n 9 Ireland    5350\n10 China      4523\n# ℹ 54 more rows\n\n\nWas your guess in the top three?1"
  },
  {
    "objectID": "blog/2024-12-23-names/index.html#bonus-points",
    "href": "blog/2024-12-23-names/index.html#bonus-points",
    "title": "Which names that are also names of countries are most common?",
    "section": "Bonus points",
    "text": "Bonus points\nNow that we’re here, of course, we have to get up to a little tomfoolery. How are those top names trending?\n\nbabynames %&gt;%\n  # only keep names that are in the top 5 from the previous result\n  filter(name %in% top_names$name[1:5]) %&gt;%\n  # take a weighted average of the proportions across sex\n  group_by(year, name) %&gt;%\n  summarize(prop = (sum(n) / sum(n/prop))) %&gt;%\n  mutate(pct = prop * 100) %&gt;%\n  # plot that feller\n  ggplot() +\n  aes(x = year, y = pct, color = name) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Percent\", color = \"Name\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  theme(legend.position = c(0.15, 0.75))\n\n\n\n\n\n\n\nI initially found it somewhat surprising that there was so much variation in the proportion of Georgia’s up until 1920, and then suddenly not. The only way I could explain that sudden shift would be a huge increase in the number of people generally right around 1920—more people, less noisy proportion—but that didn’t seem reasonable to me.\nTurns out, that definitely happened:\n\nbabynames %&gt;% \n  group_by(year) %&gt;% \n  summarize(n = sum(n)) %&gt;% \n  ggplot() + \n  aes(x = year, y = n) + \n  geom_line() +\n  labs(x = \"Year\", y = \"Number of Babies\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\nNote this isn’t necessarily the number of babies born per year in the U.S. as the data includes only those names/sexes/years corresponding to at least 5 babies.\n\n\n\nI know very little about the history of the country that I live in. Alas, happy holidays, yall.🎄"
  },
  {
    "objectID": "blog/2024-12-23-names/index.html#footnotes",
    "href": "blog/2024-12-23-names/index.html#footnotes",
    "title": "Which names that are also names of countries are most common?",
    "section": "Footnotes",
    "text": "Footnotes\n\nI added this footnote so that you’d end up near the comments section. Let me know.↩︎"
  },
  {
    "objectID": "blog/2022-12-01-listening-2022/index.html",
    "href": "blog/2022-12-01-listening-2022/index.html",
    "title": "Analyzing my own music listening data with R and the tidyverse",
    "section": "",
    "text": "Aside from exchanging playlists with my partner every once in a while, I’m not much of a Spotify user. Around this time every year, though, all of my friends start posting their Spotify Wrapped, and I get jealous.\nI do get the Wrapped for Artists report from my high school band, and I’m pleased to report we had 13 total listens this year. Not quite the dopamine hit I’m looking for, though.\nMy preferred way of collecting music is via the Music app (formerly iTunes) on my computer. As in, .mp3s. I’m a simple feller.\nI noticed at some point a while back that when you right click the top bar, a dropdown appears with all of the different variables that you can display on your app screen and sort by (most notably, play count):\nSo, this information is somewhere.\nYou already know:"
  },
  {
    "objectID": "blog/2022-12-01-listening-2022/index.html#finding",
    "href": "blog/2022-12-01-listening-2022/index.html#finding",
    "title": "Analyzing my own music listening data with R and the tidyverse",
    "section": "Finding and cleaning the data",
    "text": "Finding and cleaning the data\nTurns out, the Apple folks made tracking down “the data” pretty straightforward. Just click\nMusic app &gt; File &gt; Library &gt; Export Library\nA pop-up will appear that allows you to pick where to save the Library.xml file. I write “the data” in quotes because it’s an .xml file. Yikes.\n\n\n\nA screenshot of an XML file containing the contents of my library. Pairs of ‘keys’ and ‘dictionaries’ are deeply nested inside of each other, containing metadata about all of the songs in my music library.\n\n\nThe nesting in this file is pretty gnarly, but the xml2 package provides some slick tools to read them into R and convert them to a list. Then, tibble::enframe() that list into a data frame, and tidy away. I won’t belabor that process here, but the source code for that tidying is available in an R package on my GitHub at https://github.com/simonpcouch/wrapped. You can install the package by running:\ndevtools::install_github(\"simonpcouch/wrapped\")\nThe wrap_library() function takes in the path to your Library.xml file, and the year of interest, and outputs a tidy data frame with your listening data. A la:\nwrap_library(\"Library.xml\", 2022L)\nThe source code is here, for those curious.\nI also packaged up some example output of this function with the wrapped data frame loaded with the package. It contains my listening data from 2022, which is a bit borked. 1\nlibrary(wrapped)\n\nwrapped\n## # A tibble: 594 × 8\n##       id track_title         artist album genre date_added skip_count play_count\n##    &lt;int&gt; &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n##  1 11452 Bag Of You          Mahal… Bag … R&B/… 2022-11-11          3        102\n##  2 11040 The Internet        Viole… If I… Indi… 2022-08-04          2         87\n##  3 11041 Settle              Viole… If I… Indi… 2022-08-04          2         87\n##  4 11170 Half My Life        Viole… Lone… Pop   2022-09-08          4         80\n##  5 11317 The Problem Song    Valley The … Indi… 2022-10-08         NA         79\n##  6 10976 WASTE TIME          Gwen … PHASE R&B/… 2022-07-21          3         74\n##  7 10899 Good Things         Ken Y… Ceru… Sing… 2022-06-04          1         61\n##  8 11268 pennies (voice mem… Tiny … penn… Sing… 2022-10-01          1         51\n##  9 11169 When I Come Home    Viole… Lone… Pop   2022-09-08          1         50\n## 10 10907 Small Doses         Ken Y… Ceru… Sing… 2022-06-04         NA         49\n## # … with 584 more rows\nNote that the data are already arranged in descending order of play count. I found myself arrange()ing by desc(play_count) after every summary, so though I’d skip that step when I could.\nFor easier printing in this blog post, I’ll rearrange this data to show the most commonly noted output:\nwrapped &lt;- \n  wrapped %&gt;%\n  select(-id) %&gt;%\n  relocate(date_added, skip_count, .after = everything()) %&gt;%\n  relocate(play_count, .before = everything())\n\nwrapped\n## # A tibble: 594 × 7\n##    play_count track_title          artist      album genre date_added skip_count\n##         &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;date&gt;          &lt;dbl&gt;\n##  1        102 Bag Of You           Mahalia     Bag … R&B/… 2022-11-11          3\n##  2         87 The Internet         Violet Ski… If I… Indi… 2022-08-04          2\n##  3         87 Settle               Violet Ski… If I… Indi… 2022-08-04          2\n##  4         80 Half My Life         Violet Ski… Lone… Pop   2022-09-08          4\n##  5         79 The Problem Song     Valley      The … Indi… 2022-10-08         NA\n##  6         74 WASTE TIME           Gwen Bunn   PHASE R&B/… 2022-07-21          3\n##  7         61 Good Things          Ken Yates   Ceru… Sing… 2022-06-04          1\n##  8         51 pennies (voice memo) Tiny Habits penn… Sing… 2022-10-01          1\n##  9         50 When I Come Home     Violet Ski… Lone… Pop   2022-09-08          1\n## 10         49 Small Doses          Ken Yates   Ceru… Sing… 2022-06-04         NA\n## # … with 584 more rows"
  },
  {
    "objectID": "blog/2022-12-01-listening-2022/index.html#analyzing-it",
    "href": "blog/2022-12-01-listening-2022/index.html#analyzing-it",
    "title": "Analyzing my own music listening data with R and the tidyverse",
    "section": "Analyzing it",
    "text": "Analyzing it\nOnce the data is in a tidy format, a little bit of dplyr goes a long way.\nThe blessing of posting on my own platform, as well, is that I can annotate the results with fun commentary that no one cares about.\n\nTop songs:\nwrapped %&gt;%\n  select(track_title, artist, play_count) %&gt;%\n  head(10)\n## # A tibble: 10 × 3\n##    track_title          artist       play_count\n##    &lt;chr&gt;                &lt;chr&gt;             &lt;dbl&gt;\n##  1 Bag Of You           Mahalia             102\n##  2 The Internet         Violet Skies         87\n##  3 Settle               Violet Skies         87\n##  4 Half My Life         Violet Skies         80\n##  5 The Problem Song     Valley               79\n##  6 WASTE TIME           Gwen Bunn            74\n##  7 Good Things          Ken Yates            61\n##  8 pennies (voice memo) Tiny Habits          51\n##  9 When I Come Home     Violet Skies         50\n## 10 Small Doses          Ken Yates            49\nI bought that Mahalia song probably three weeks ago. I recently went on a road trip that took about 40 hours of driving, and this song was repeated 5 times on a 40-song playlist that I spun most of the drive. Even then, I was surprised to see this up top.\nAnd, yall! Violet Skies! She opened for another group that my partner and I saw recently, and I didn’t know who she was before that show, but wow.\n\n\nTop artists:\nwrapped %&gt;%\n  group_by(artist) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  head()\n## # A tibble: 6 × 2\n##   artist       play_count\n##   &lt;chr&gt;             &lt;dbl&gt;\n## 1 Violet Skies        594\n## 2 Ken Yates           326\n## 3 Watchhouse          320\n## 4 Gwen Bunn           269\n## 5 JID                 177\n## 6 Seaforth            174\n:)\n\n\nTop genres:\nOne of my first steps after buying a new record is to edit it’s metadata to fit into one of a few pre-defined genres. Every categorization is likely a hot take, but it does make for a nice summary:\nwrapped %&gt;%\n  group_by(genre) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  head()\n## # A tibble: 6 × 2\n##   genre                  play_count\n##   &lt;chr&gt;                       &lt;dbl&gt;\n## 1 Singer-Songwriter/Folk       1714\n## 2 Indie/Alternative             987\n## 3 R&B/Soul                      805\n## 4 Country                       547\n## 5 Rap/Hip-Hop                   251\n## 6 Pop                           222\nPickin’ and grinnin’.\nIt also looks like I’ve forgotten to re-genre some of those albums, resulting in a pretty steep play count dropoff after the first six.\nwrapped %&gt;%\n  group_by(genre) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  slice(7:10)\n## # A tibble: 4 × 2\n##   genre       play_count\n##   &lt;chr&gt;            &lt;dbl&gt;\n## 1 R&B                109\n## 2 Jazz                44\n## 3 Blues               19\n## 4 Hip-Hop/Rap         13\n\n\nTop albums:\nwrapped %&gt;%\n  group_by(album, artist) %&gt;%\n  summarize(play_count = sum(play_count, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(play_count)) %&gt;%\n  head()\n## # A tibble: 6 × 3\n##   album              artist       play_count\n##   &lt;chr&gt;              &lt;chr&gt;             &lt;dbl&gt;\n## 1 If I Saw You Again Violet Skies        372\n## 2 Watchhouse (Duo)   Watchhouse          320\n## 3 Cerulean           Ken Yates           303\n## 4 PHASE              Gwen Bunn           269\n## 5 Lonely             Violet Skies        222\n## 6 The Forever Story  JID                 177\nI was surprised to see that Watchhouse only made two appearances in these results, but, I’ll take it. Grateful for all of these albums."
  },
  {
    "objectID": "blog/2022-12-01-listening-2022/index.html#bonus-points",
    "href": "blog/2022-12-01-listening-2022/index.html#bonus-points",
    "title": "Analyzing my own music listening data with R and the tidyverse",
    "section": "Bonus points",
    "text": "Bonus points\nUnlike Spotify, I do not have graphic design chops to display my summaries with. As for UI, it may seem that all I have in my toolkit is a call to an HTML-pretty-printing library, and that’s true. I do have the raw data, though, so I may as well flex on being able to analyze this myself.\n\nHiding things\nJust for funsies, I divided my play counts for songs of one genre by three (and took the floor() function on the output).\nAny statisticians in the crowd? Scavenger hunt: which genre was that?\n\n\nMost skipped\nThe Music app stores all sorts of things about what/how I listen to music, and one of them I thought would be interesting is my most skipped song.\nwrapped %&gt;%\n  arrange(desc(skip_count)) %&gt;%\n  head() %&gt;%\n  select(1:3)\n## # A tibble: 6 × 3\n##   play_count track_title               artist      \n##        &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;       \n## 1         28 Jupiter                   Violet Skies\n## 2         17 Over The Valley           Violet Skies\n## 3         39 Best of the Broken Things Ken Yates   \n## 4          6 Born On Valentine's Day   Violet Skies\n## 5         80 Half My Life              Violet Skies\n## 6         30 What I Get for Loving You Seaforth\nFrom looking at these, I’m guessing the most prominent ones here are the songs I like least on the albums I like the most; if I shuffle the albums these songs are from, I can definitely imagine myself skipping several of these.\n\n\nNew music friday\nOne of my favorite Friday morning traditions is to take half an hour to try and track down any new music from my favorite artists. I often, though, don’t take the time to wake up before starting work and make that happen. I wondered how pronounced that effect would be in this data:\nwrapped %&gt;%\n  mutate(weekday = wday(date_added, label = TRUE)) %&gt;%\n  ggplot(aes(x = weekday)) + \n  geom_bar()\n\nYeah!!! Looks like I decide to sleep in most Fridays, but do tend to get my redemption the day after."
  },
  {
    "objectID": "blog/2022-12-01-listening-2022/index.html#spotify-abstainers-rejoice",
    "href": "blog/2022-12-01-listening-2022/index.html#spotify-abstainers-rejoice",
    "title": "Analyzing my own music listening data with R and the tidyverse",
    "section": "Spotify abstainers rejoice",
    "text": "Spotify abstainers rejoice\nWhile putting together this post, I’ve developed an interest in finding a music listening app that surveils me even more closely. A listen-by-listen level of observation with this data would be so fun to play around with. Alas.\nThanks for dropping by. If you’re also an R and Music app user, give the {wrapped} package a go, and let me know if you share your analyses anywhere!\n\nFootnotes\n(1) I switched jobs in May, and migrated computers shortly after. I did so in a silly way, and lost all of the metadata for my music library. So, almost all of my songs say they were added to my library in mid-May. So, for simplicity, I’ve kept only songs that I’ve purchased since that date, and my listening statistics thus only account for the latter half of the year. So it goes—I’m excited for next year’s output!"
  },
  {
    "objectID": "blog/2023-08-04-parallel-racing/index.html",
    "href": "blog/2023-08-04-parallel-racing/index.html",
    "title": "Optimizing model parameters faster with tidymodels",
    "section": "",
    "text": "Especially for large amounts of data or large grids of parameters, the time and resources needed to tune statistical models can quickly become a barrier to getting machine learning models deployed. Many examples online demonstrating how to tune hyperparameters with tidymodels use tune_grid() on one CPU core. Making use of parallel processing and using a near-drop-in replacement for tune_grid() can speed up hyperparameter tuning by 20-30x!"
  },
  {
    "objectID": "blog/2023-08-04-parallel-racing/index.html#setup-grid-search",
    "href": "blog/2023-08-04-parallel-racing/index.html#setup-grid-search",
    "title": "Optimizing model parameters faster with tidymodels",
    "section": "Setup: grid search",
    "text": "Setup: grid search\nIn many examples online demonstrating machine learning with tidymodels, grid search via tune_grid() is the workhorse behind tuning hyperparameters. tune_grid() computes a set of performance metrics for a set of candidate model parameters across resamples of data; the parameters that result in the most performant models can then be used to train a model on the entire training set. We’ll quickly put together an example grid search with tune_grid() to benchmark how long it takes before introducing two strategies to speed the process up—parallel processing and racing via ANOVA.\nWe’ll make use of the ames housing data included as part of the tidymodels meta-package to demonstrate hyperparameter tuning with grid search.\nlibrary(tidymodels)\n\names\n## # A tibble: 2,930 × 74\n##    MS_SubClass            MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape\n##  * &lt;fct&gt;                  &lt;fct&gt;            &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;    \n##  1 One_Story_1946_and_Ne… Resident…          141    31770 Pave   No_A… Slightly…\n##  2 One_Story_1946_and_Ne… Resident…           80    11622 Pave   No_A… Regular  \n##  3 One_Story_1946_and_Ne… Resident…           81    14267 Pave   No_A… Slightly…\n##  4 One_Story_1946_and_Ne… Resident…           93    11160 Pave   No_A… Regular  \n##  5 Two_Story_1946_and_Ne… Resident…           74    13830 Pave   No_A… Slightly…\n##  6 Two_Story_1946_and_Ne… Resident…           78     9978 Pave   No_A… Slightly…\n##  7 One_Story_PUD_1946_an… Resident…           41     4920 Pave   No_A… Regular  \n##  8 One_Story_PUD_1946_an… Resident…           43     5005 Pave   No_A… Slightly…\n##  9 One_Story_PUD_1946_an… Resident…           39     5389 Pave   No_A… Slightly…\n## 10 Two_Story_1946_and_Ne… Resident…           60     7500 Pave   No_A… Regular  \n## # ℹ 2,920 more rows\n## # ℹ 67 more variables: Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;,\n## #   Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;,\n## #   Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;, Year_Built &lt;int&gt;,\n## #   Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;,\n## #   Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;,\n## #   Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, …\nThe data set contains information on 2,930 properties in Ames, Iowa. We’ll model the log home sale price using predictors related a home’s location, size, and condition.\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\nFirst, splitting the data into training and testing sets and then resampling the training set using v-fold cross-validation:\nset.seed(1)\names_split &lt;- initial_split(ames, strata = \"Sale_Price\")\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\nset.seed(2)\names_folds &lt;- vfold_cv(ames_train, v = 20)\nThen, we’ll define a boosted tree ensemble model. We first load the bonsai package, which defines parsnip wrappers for the gradient boosting framework LightGBM (among others), and then define a model specification using parsnip.\nlibrary(bonsai)\n\nbt_spec &lt;-\n  boost_tree(learn_rate = tune(), stop_iter = tune(), trees = 1000) %&gt;%\n  set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n  set_mode(\"regression\")\nWe tune a number of main model parameters as well as the engine parameter num_leaves. Rather than tuning the number of trees, we tune stop_iter, the number of boosting iterations without improvement before stopping training.\nWe’ll also use a recipe to pre-process the data. This one is adapted from the book Tidy Modeling with R:\nbt_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.05) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = tune())\nNote that we tune the degrees of freedom deg_free parameter of the natural spline step.\nBundling the model and preprocessing together into a workflow:\nbt_wflow &lt;- workflow(bt_rec, bt_spec)\nEach argument we’ve tagged to tune() is now represented in the workflow. We can summarize that set of parameters needing tuning by extracting them from the workflow:\nextract_parameter_set_dials(bt_wflow)\n## Collection of 4 parameters for tuning\n## \n##  identifier       type    object\n##  learn_rate learn_rate nparam[+]\n##   stop_iter  stop_iter nparam[+]\n##  num_leaves num_leaves nparam[+]\n##    deg_free   deg_free nparam[+]\nA grid search involves taking candidate values for each of these parameters and evaluating their resulting performance against resamples of the data. A model configuration is a workflow with a specific value sampled for each tuning parameter; we’re searching for the model configuration that results in the best possible performance. We’ll carry out a grid search across 50 model configurations with tidymodels using tune_grid() and time how long it takes using system.time().\nset.seed(3)\n\nbt_time_grid &lt;- system.time(\n  bt_res_grid &lt;- tune_grid(bt_wflow, ames_folds, grid = 50)\n)\nbt_res_grid is an object that gives us insight into various aspects of the tuning process like resampling variation, performance metrics, and issues in model fitting.\nbt_res_grid\n## # Tuning results\n## # 20-fold cross-validation \n## # A tibble: 20 × 4\n##    splits             id     .metrics           .notes          \n##    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n##  1 &lt;split [2087/110]&gt; Fold01 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  2 &lt;split [2087/110]&gt; Fold02 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  3 &lt;split [2087/110]&gt; Fold03 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  4 &lt;split [2087/110]&gt; Fold04 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  5 &lt;split [2087/110]&gt; Fold05 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  6 &lt;split [2087/110]&gt; Fold06 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  7 &lt;split [2087/110]&gt; Fold07 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  8 &lt;split [2087/110]&gt; Fold08 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  9 &lt;split [2087/110]&gt; Fold09 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 10 &lt;split [2087/110]&gt; Fold10 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 11 &lt;split [2087/110]&gt; Fold11 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 12 &lt;split [2087/110]&gt; Fold12 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 13 &lt;split [2087/110]&gt; Fold13 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 14 &lt;split [2087/110]&gt; Fold14 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 15 &lt;split [2087/110]&gt; Fold15 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 16 &lt;split [2087/110]&gt; Fold16 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 17 &lt;split [2087/110]&gt; Fold17 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 18 &lt;split [2088/109]&gt; Fold18 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 19 &lt;split [2088/109]&gt; Fold19 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 20 &lt;split [2088/109]&gt; Fold20 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\nVarious helper functions allow us to quickly learn from this output. For example, plotting the resampled performance for each tuning parameter:\nautoplot(bt_res_grid)\n\nOr, picking out the best tuning parameter combination according to the root mean squared error (RMSE):\ncollect_metrics(bt_res_grid) %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean)\n## # A tibble: 50 × 10\n##    learn_rate stop_iter num_leaves deg_free .metric .estimator   mean     n\n##         &lt;dbl&gt;     &lt;int&gt;      &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n##  1    0.00713        19         45       15 rmse    standard   0.0751    20\n##  2    0.0211          7         17        2 rmse    standard   0.0759    20\n##  3    0.00960        16         82       12 rmse    standard   0.0760    20\n##  4    0.0129         12         65        4 rmse    standard   0.0763    20\n##  5    0.00317         5         37       11 rmse    standard   0.0764    20\n##  6    0.0416          3         72       13 rmse    standard   0.0784    20\n##  7    0.0494         11         57        9 rmse    standard   0.0801    20\n##  8    0.00198        15         71        6 rmse    standard   0.0811    20\n##  9    0.00473         7          6       10 rmse    standard   0.0812    20\n## 10    0.0921          6         53       12 rmse    standard   0.0816    20\n## # ℹ 40 more rows\n## # ℹ 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;\nOur best validation set RMSE averaged out to 0.075.\nFor each of the 50 model configurations, the configuration was fitted 20 (once for each fold) times. That’s 1000 model fits!\nIn total, these results took 1005 seconds, or about 17 minutes, to generate. This isn’t terribly long, but our input data is quite small compared to that from many of the real-world applications of machine learning.\ntidymodels includes support for two optimizations that will allow us to tune this model much more quickly!"
  },
  {
    "objectID": "blog/2023-08-04-parallel-racing/index.html#parallel-processing",
    "href": "blog/2023-08-04-parallel-racing/index.html#parallel-processing",
    "title": "Optimizing model parameters faster with tidymodels",
    "section": "Parallel processing",
    "text": "Parallel processing\nThe first optimization is parallel processing. At the moment, I’m writing this blog post on a computer with 10 cores. Without any additional configuration, R will only make use of one of those cores to carry out computations. Parallel processing tools will allow me to allot all of the computations under the hood in tune_grid() to best make use of all of the cores available to me.\ntidymodels supports a variety of parallel processing backends. I’ll set one up using the parallel and doParallel packages:\nlibrary(doMC)\nregisterDoMC(cores = 10)\nRunning the same tuning code as before, now, tune_grid() will pick up on our newly registered backend and distribute its computations across the available cores:\nset.seed(4)\n\nbt_time_par &lt;- system.time(\n  bt_res_par &lt;- tune_grid(bt_wflow, ames_folds, grid = 50)\n)\nIn total, these results took 136 seconds to generate! This is about 7.4 times faster than the single-core approach. There’s a bit of overhead associating with sending computations out to different cores and then compiling them back together, and some of the computations needed to tune hyperparameters can’t be parallelized, so the factor of speedup when using parallel processing while tuning is slightly less than the factor increase in number of cores.\nAlready, a 7.4x speedup is a gamechanger for many machine learning workflows! This optimization assumes you have access to unutilized computing resources, though, which may not be the case for you; this next one doesn’t require any extra computing power."
  },
  {
    "objectID": "blog/2023-08-04-parallel-racing/index.html#racing",
    "href": "blog/2023-08-04-parallel-racing/index.html#racing",
    "title": "Optimizing model parameters faster with tidymodels",
    "section": "Racing",
    "text": "Racing\nIn these previous examples, we’ve evaluated 50 model configurations against 20 resamples, resulting in 1000 model fits. We can use an approach called racing to cut down on that number of model fits.\nRacing via ANOVA models, implemented in the tune_race_anova() function in finetune, works by fitting each model configuration to only a few of the resamples and then, at each additional resample, removing model configurations that are unlikely to be more performant than the others.\nlibrary(finetune)\nVisualization helpers in the finetune package are super effective for developing intuition on this process, so let’s go ahead and tune using racing and then use the output to learn more about the process. First, we’ll bump the number of cores back to one so that we can observe the effect of only racing:\nregisterDoMC(cores = 1)\nNow, tuning using racing:\nset.seed(5)\n\nbt_time_race &lt;- system.time(\n  bt_res_race &lt;- tune_race_anova(bt_wflow, ames_folds, grid = 50)\n)\nThe resulting object bt_res_race looks quite similar to the typical grid search output:\nbt_res_race\n## # Tuning results\n## # 20-fold cross-validation \n## # A tibble: 20 × 5\n##    splits             id     .order .metrics           .notes          \n##    &lt;list&gt;             &lt;chr&gt;   &lt;int&gt; &lt;list&gt;             &lt;list&gt;          \n##  1 &lt;split [2087/110]&gt; Fold01      3 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  2 &lt;split [2087/110]&gt; Fold05      1 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  3 &lt;split [2087/110]&gt; Fold10      2 &lt;tibble [100 × 8]&gt; &lt;tibble [0 × 3]&gt;\n##  4 &lt;split [2087/110]&gt; Fold16      4 &lt;tibble [8 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n##  5 &lt;split [2087/110]&gt; Fold15      5 &lt;tibble [6 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n##  6 &lt;split [2087/110]&gt; Fold11      6 &lt;tibble [4 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n##  7 &lt;split [2087/110]&gt; Fold04      7 &lt;tibble [4 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n##  8 &lt;split [2087/110]&gt; Fold02     14 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n##  9 &lt;split [2087/110]&gt; Fold03     19 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 10 &lt;split [2087/110]&gt; Fold06     15 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 11 &lt;split [2087/110]&gt; Fold07     11 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 12 &lt;split [2087/110]&gt; Fold08     16 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 13 &lt;split [2087/110]&gt; Fold09     20 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 14 &lt;split [2087/110]&gt; Fold12     10 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 15 &lt;split [2087/110]&gt; Fold13      8 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 16 &lt;split [2087/110]&gt; Fold14     13 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 17 &lt;split [2087/110]&gt; Fold17      9 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 18 &lt;split [2088/109]&gt; Fold18     18 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 19 &lt;split [2088/109]&gt; Fold19     12 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\n## 20 &lt;split [2088/109]&gt; Fold20     17 &lt;tibble [2 × 8]&gt;   &lt;tibble [0 × 3]&gt;\nThe plot_race() helper visualizes the racing process:\nplot_race(bt_res_race) + labs(x = \"Resample #\")\n\nIn this plot, we see 50 differently colored lines, each corresponding to a model configuration. For a given resample, each model configuration has an associated RMSE, plotted on the y axis.\nIn racing, resamples are assigned a random order and then evaluated in order. The x axis in this case thus roughly corresponds to time elapsed.\nMoving from left to right, we initially see that every model configuration was evaluated with the first three resamples. After those first three resamples (called the “burn-in” period), the racing procedure stops and evaluates the distributions of RMSEs for each model configuration. Using a repeated measures ANOVA model, we can evaluate whether the distributions of RMSEs from some of the model configurations are not statistically significantly different from the current “best” RMSE distribution. The model configurations resulting in distributions of RMSEs that are not statistically significantly different from the current best, as well as the one resulting in the best RMSE distribution, are resampled again; the rest are set aside and not evaluated against any more resamples.\nIntuitively, this makes sense; in the plot above, those upper-most lines are not horizontally centered in the same place as the lowest ones. Each point on that plot is a model fit, which takes lots of time to generate. If we feel confident that a model configuration defines a model that isn’t as performant as the current best one, why continue fitting it to more and more resamples? We can instead reroute those computing resources to evaluating the models that seem like contenders with the current best one.\nSo, after the burn-in period, we determine that many of the model configurations are no good, and evaluate only the remaining ones against the fourth resample. This adds a point to each of the distributions of errors corresponding to each model configuration, and those distributions are again passed to the repeated measures ANOVA model, which might find a couple more model configurations to remove. And so on. Ultimately, we end up only fully resampling (i.e. evaluating against all 20 resamples) a small fraction of the model configurations we initially started with.\nThe analogous plot for a regular grid search, like that generating bt_res_grid, would just have 50 lines stretching all the way across the x axis. Every model configuration is fully resampled:\n\nA little silly.\nNot having to carry out all of those additional model fits saves us a bunch of time! In total, these results took 189 seconds to generate, about 5.3 times faster than the single-core, grid search approach.\nDo we see any loss in performance, though? No free lunch, Simon! Well, kind of. With some probability (see the racing control settings), the repeated measures ANOVA model will mistakenly boot a model configuration that was actually associated with performance comparable to the “best” model configuration. Note too, though, that evaluating all model configurations across all resamples doesn’t mean we can guarantee we’ve associated the true “best” model with the most performant metric. Variation abounds! So, I don’t lose sleep over it.\nAgain, on its own, this method is really powerful in cutting down on time-to-tune. In combination with parallel processing, we see marked speedups."
  },
  {
    "objectID": "blog/2023-08-04-parallel-racing/index.html#all-together-now",
    "href": "blog/2023-08-04-parallel-racing/index.html#all-together-now",
    "title": "Optimizing model parameters faster with tidymodels",
    "section": "All together, now!",
    "text": "All together, now!\nI’ll quickly spin up that parallel cluster again and then run the same code:\nregisterDoMC(cores = 10)\n\nset.seed(6)\n\nbt_time_race_par &lt;- system.time(\n  bt_res_race_par &lt;- tune_race_anova(bt_wflow, ames_folds, grid = 50)\n)\nThese results took 47 seconds to generate, 21.5 times faster than the single-core, conventional grid search approach. Scootin’! As the help-file for tune_race_anova() reads:\n\nThere is a compounding effect of racing and parallel processing but its magnitude depends on the type of model, number of resamples, number of tuning parameters, and so on.\n\nThis blog post is adapted from that help-file, which includes benchmarks from tuning a different model on a different data set, resulting in a similarly impressive speedup. Check that page out for another example!\nHappy modeling. :)"
  },
  {
    "objectID": "blog/2025-04-15-gpt-4-1/index.html",
    "href": "blog/2025-04-15-gpt-4-1/index.html",
    "title": "How good are the GPT 4.1 models at writing R code?",
    "section": "",
    "text": "Yesterday, OpenAI dropped a new series of models called GPT 4.1, 4.1 mini, and GPT 4.1 nano. This line from their release post, specifically, caught my eye:\nIt’s no surprise to me that OpenAI’s newest drop tops benchmark after benchmark. That said, when I see news of new models beating out Claude Sonnet by various measures, I usually wait a week before coming to any conclusions; many developers seem to feel that the Claude series of models have some secret sauce, and I’m among them. Seeing this explicit focus on real-world coding and instruction-following piqued my curiosity, so I’m bypassing my usual “wait a week” policy to see what’s up.\nAs it happens, I’ve been working on a new tool called vitals for large language model evaluation in R. The package is still pretty early on in it’s development and is changing rapidly–so much so that its name has changed in the two weeks since I last wrote about it on this blog–but I’ll use it here to evaluate these models on an R coding benchmark."
  },
  {
    "objectID": "blog/2025-04-15-gpt-4-1/index.html#introducing-vitals",
    "href": "blog/2025-04-15-gpt-4-1/index.html#introducing-vitals",
    "title": "How good are the GPT 4.1 models at writing R code?",
    "section": "Introducing vitals",
    "text": "Introducing vitals\nvitals is an R port of the widely adopted Python framework Inspect. While the package doesn’t integrate with Inspect directly, it allows users to interface with the Inspect log viewer and shares much of its grammar and philosophy.\nvitals describes LLM evals in three core components:\n\n\nDatasets contain a set of labelled samples. Datasets are just a tibble with columns input and target, where input is a prompt and target is either literal value(s) or grading guidance.\n\nSolvers evaluate the input in the dataset and produce a final result (hopefully) approximating target. In vitals, the simplest solver is just an ellmer chat (e.g. ellmer::chat_claude()) wrapped in generate(), i.e. generate(ellmer::chat_claude())), which will call the Chat object’s $chat() method and return whatever it returns.\n\nScorers evaluate the final output of solvers. They may use text comparisons, model grading, or other custom schemes to determine how well the solver approximated the target based on the input.\n\nIn this blog post, we’ll apply a solver powered by four different models to a dataset of R coding problems. Our baseline will be Claude 3.7 Sonnet, as this is my daily driver for coding assistance and a peer to GPT 4.1 in pricing. Then, I’ll also add GPT-4o, as I know this has been the model of choice for many other folks. Finally, I’ll include the three new models for the GPT series: 4.1, 4.1 mini, and 4.1 nano.\nIn ellmer, here’s how we define those model connections:\n\nlibrary(ellmer)\n\nsonnet_3_7 &lt;- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\n\ngpt_4o &lt;- chat_openai(model = \"gpt-4o\")\n\ngpt_4_1 &lt;- chat_openai(model = \"gpt-4.1\")\ngpt_4_1_mini &lt;- chat_openai(model = \"gpt-4.1-mini\")\ngpt_4_1_nano &lt;- chat_openai(model = \"gpt-4.1-nano\")\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re interested in how Gemini’s newest 2.5 Pro release stacks up on this eval, check out this post from two weeks ago.\n\n\nNote that I needed to configure a ANTHROPIC_API_KEY and OPENAI_API_KEY to connect to these models, respectively. These new models are quite cheap compared to Claude 3.7 Sonnet and GPT-4o! Their pricing per million tokens is as follows\n\n\n# A tibble: 5 × 3\n  Name              Input Output\n  &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; \n1 Claude 3.7 Sonnet $3.00 $15.00\n2 GPT-4o            $3.75 $15.00\n3 GPT-4.1           $2.00 $8.00 \n4 GPT-4.1 mini      $0.40 $1.60 \n5 GPT-4.1 nano      $0.10 $0.40 \n\n\nAltogether, the data underlying this blog post took around $3 USD to generate."
  },
  {
    "objectID": "blog/2025-04-15-gpt-4-1/index.html#an-r-eval-dataset",
    "href": "blog/2025-04-15-gpt-4-1/index.html#an-r-eval-dataset",
    "title": "How good are the GPT 4.1 models at writing R code?",
    "section": "An R Eval dataset",
    "text": "An R Eval dataset\n\nlibrary(vitals)\nlibrary(tidyverse)\n\nWe’ll use a dataset that ships with vitals called are, or “An R Eval.” From the are docs:\n\nAn R Eval is a dataset of challenging R coding problems. Each input is a question about R code which could be solved on first-read only by human experts and, with a chance to read documentation and run some code, by fluent data scientists. Solutions are in target and enable a fluent data scientist to evaluate whether the solution deserves full, partial, or no credit.\n\n\nglimpse(are)\n\nRows: 26\nColumns: 7\n$ id        &lt;chr&gt; \"after-stat-bar-heights\", \"conditional-grouped-sum…\n$ input     &lt;chr&gt; \"This bar chart shows the count of different cuts …\n$ target    &lt;chr&gt; \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n…\n$ domain    &lt;chr&gt; \"Data analysis\", \"Data analysis\", \"Data analysis\",…\n$ task      &lt;chr&gt; \"New code\", \"New code\", \"New code\", \"Debugging\", \"…\n$ source    &lt;chr&gt; \"https://jrnold.github.io/r4ds-exercise-solutions/…\n$ knowledge &lt;list&gt; \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"…\n\n\nAt a high level:\n\n\ntitle: A unique identifier for the problem.\n\ninput: The question to be answered.\n\ntarget: The solution, often with a description of notable features of a correct solution.\n\ndomain, task, and knowledge are pieces of metadata describing the kind of R coding challenge.\n\nsource: Where the problem came from, as a URL. Many of these coding problems are adapted “from the wild” and include the kinds of context usually available to those answering questions.\n\nNotably, these coding problems look like a typical chat, so the eval doesn’t measure instruction-following / structured output specifically.\nFor the purposes of actually carrying out the initial evaluation, we’re specifically interested in the input and target columns. Let’s print out the first entry in full so you can get a taste of a typical problem in this dataset:\n\ncat(are$input[1])\n\nThis bar chart shows the count of different cuts of diamonds,\nand each bar is stacked and filled according to clarity:\n\n```\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity))\n```\n\nCould you change this code so that the proportion of diamonds\nwith a given cut corresponds to the bar height and not the\ncount? Each bar should still be filled according to clarity.\n\n\nHere’s the suggested solution:\n\ncat(are$target[1])\n\nPreferably:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = after_stat(count) /\nsum(after_stat(count)), fill = clarity))\n```\n\nThe dot-dot notation (`..count..`) was deprecated in ggplot2\n3.4.0, but it still works:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill =\nclarity))\n```\n\nSimply setting `position = \"fill\" will result in each bar\nhaving a height of 1 and is not correct.\n\n\nFor now, are was publicly shared after the knowledge cutoff of each of these models, so the answers to these questions (likely) aren’t yet incorporated into the models’ weights."
  },
  {
    "objectID": "blog/2025-04-15-gpt-4-1/index.html#a-baseline-model",
    "href": "blog/2025-04-15-gpt-4-1/index.html#a-baseline-model",
    "title": "How good are the GPT 4.1 models at writing R code?",
    "section": "A baseline model",
    "text": "A baseline model\nLLM evaluation with vitals happens in two main steps:\nFirst, use Task$new() to situate a dataset, solver, and scorer in a Task. Tasks are R6 objects that define important methods and data structures for LLM evaluation. Below, I use generate() as a solver, currently the only built-in solver supplied by the package. Think of it like Chat objects’ $chat() method with some bells and whistles—parallel requests, some nice progress functionality, and lots of logging. generate() returns a function that has one argument, solver_chat, which takes an ellmer Chat; you can set a default Chat by supplying it to generate() itself. The scorer, model_graded_qa(), uses model grading (or “LLM-as-a-judge”) to score the solver’s responses. Different models exhibit different behaviors as judges, so I use the same model (Claude Sonnet 3.7) as the judge regardless of which model is being evaluated.\n\nare_task &lt;- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n\nAn evaluation task AnREval.\n\n\nSecond, use Task$eval() to evaluate the solver, evaluate the scorer, and then explore a persistent log of the results in an interactive viewer.\n\nare_sonnet_3_7 &lt;- are_task$clone()\nare_sonnet_3_7$eval(solver_chat = sonnet_3_7)\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also run $eval() step-by-step, if you want; $eval() just calls $solve(), $score(), $log(), and $view() in sequence.\n\n\nAfter evaluation, the task contains information from the solving and scoring steps. Here’s what the model responded to that first question with:\n\ncat(are_sonnet_3_7$samples$result[1])\n\nTo change the bar chart from showing counts to showing\nproportions of diamonds with each cut, I'll modify the code to\nuse proportions instead of raw counts while keeping the\nclarity breakdown within each bar.\n\nHere's the modified code:\n\n```r\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity),\nposition = \"fill\") +\nlabs(y = \"Proportion\")\n```\n\nThis change:\n\n1. Adds `position = \"fill\"` to the `geom_bar()` function,\nwhich normalizes each bar to have the same height (1.0 or\n100%) and shows the proportional breakdown of clarity\ncategories within each cut.\n\n2. Adds `labs(y = \"Proportion\")` to relabel the y-axis, since\nit now represents proportions rather than counts.\n\nEach bar will now have the same height, representing 100% of\ndiamonds with that cut, and the colored segments within each\nbar will show the proportional distribution of clarity\ncategories for that particular cut.\n\n\nThen, since we’ve supplied model_graded_qa() as the scorer, Claude Sonnet 3.7 will be supplied the input question, the grading guidance, and the solver’s output, and asked to determine whether the solution is incorrect, partially correct, or correct. Here’s Claude’s grading transcript for the solver’s first answer:\n\ncat(are_sonnet_3_7$samples$scorer_chat[[1]]$last_turn()@text)\n\nI need to analyze whether the submitted answer correctly\naddresses the task according to the criterion.\n\nThe submission suggests using `position = \"fill\"` in\n`geom_bar()`, which normalizes each bar to have the same\nheight (1.0) and shows the proportional distribution of\nclarity categories within each cut.\n\nHowever, according to the criterion, this approach is\nexplicitly stated as incorrect. The criterion specifies that\nwe need to use either:\n1. `after_stat(count) / sum(after_stat(count))` in newer\nversions of ggplot2, or\n2. `..count.. / sum(..count..)` in older versions of ggplot2\n\nThe reason this is different from `position = \"fill\"` is that\n`position = \"fill\"` normalizes each cut category individually,\nshowing the proportion of different clarity values within each\ncut. In contrast, the requested solution shows the proportion\nof each cut relative to the total number of diamonds, while\nstill maintaining the clarity breakdown within each bar.\n\nThe submitted solution does not match what was specifically\nrequested. The criterion explicitly states that using\n`position = \"fill\"` is not correct for this task.\n\nGRADE: I\n\n\nvitals ships with the Inspect Log Viewer, a small .js app that allows you to interactively explore evaluation logs. Especially the first few times you run an eval, the tool is super helpful for uncovering unexpected behavior in solving and scoring. I’ve embedded the viewer in this post so you can check out the problems in An R Eval and how effectively Claude Sonnet 3.7 handled them:\n\n\n\n\n\nI’d encourage you to poke around in this app! You’ll certainly see some bugs that I’ve still yet to work out and some surprising behavior from the scorer, but there’s lots to be learned about how these models work from evaluation logs."
  },
  {
    "objectID": "blog/2025-04-15-gpt-4-1/index.html#evaluating-the-rest",
    "href": "blog/2025-04-15-gpt-4-1/index.html#evaluating-the-rest",
    "title": "How good are the GPT 4.1 models at writing R code?",
    "section": "Evaluating the rest",
    "text": "Evaluating the rest\nWe can evaluate the remaining models by cloning the original task and running $eval() with a new solver chat. First, to evaluate the previous GPT (non-thinking) generation, GPT-4o:\n\nare_gpt_4o &lt;- are_task$clone()\nare_gpt_4o$eval(solver_chat = gpt_4o)\nsave(are_gpt_4o, file = \"are_gpt_4o.rda\")\n\nFrom here, it’s pretty rote. Evaluating each of GPT 4.1, 4.1 mini, and 4.1 nano on this dataset:\n\nare_gpt_4_1 &lt;- are_task$clone()\nare_gpt_4_1$eval(solver_chat = gpt_4_1)\n\n\nare_gpt_4_1_mini &lt;- are_task$clone()\nare_gpt_4_1_mini$eval(solver_chat = gpt_4_1_mini)\n\n\nare_gpt_4_1_nano &lt;- are_task$clone()\nare_gpt_4_1_nano$eval(solver_chat = gpt_4_1_nano)\n\nI’ve also situated the logs for the above evaluations in the above app—just click the three stacked bars in the top right of the app to check out the logs for the remaining models."
  },
  {
    "objectID": "blog/2025-04-15-gpt-4-1/index.html#analysis",
    "href": "blog/2025-04-15-gpt-4-1/index.html#analysis",
    "title": "How good are the GPT 4.1 models at writing R code?",
    "section": "Analysis",
    "text": "Analysis\nAt evaluation time, vitals does a naive accuracy calculation that you can see displayed in the app, but in general is quite restrained in its analysis functionality. Instead, the package aims to get analysts to Happy Data Frame Land as quickly as possible using vitals_bind():\n\nare_eval &lt;- \n  vitals_bind(\n    `Claude Sonnet 3.7` = are_sonnet_3_7,\n    `GPT-4o` = are_gpt_4o,\n    `GPT-4.1` = are_gpt_4_1,\n    `GPT-4.1 mini` = are_gpt_4_1_mini,\n    `GPT-4.1 nano` = are_gpt_4_1_nano,\n  ) %&gt;%\n  rename(model = task) %&gt;%\n  mutate(\n    model = factor(model, levels = c(\n      \"Claude Sonnet 3.7\", \n      \"GPT-4o\",\n      \"GPT-4.1\", \n      \"GPT-4.1 mini\",\n      \"GPT-4.1 nano\"\n    ))\n  )\n\nare_eval\n\n# A tibble: 390 × 5\n   model             id                          epoch score metadata\n   &lt;fct&gt;             &lt;chr&gt;                       &lt;int&gt; &lt;ord&gt; &lt;list&gt;  \n 1 Claude Sonnet 3.7 after-stat-bar-heights          1 I     &lt;tibble&gt;\n 2 Claude Sonnet 3.7 after-stat-bar-heights          2 I     &lt;tibble&gt;\n 3 Claude Sonnet 3.7 after-stat-bar-heights          3 I     &lt;tibble&gt;\n 4 Claude Sonnet 3.7 conditional-grouped-summary     1 P     &lt;tibble&gt;\n 5 Claude Sonnet 3.7 conditional-grouped-summary     2 P     &lt;tibble&gt;\n 6 Claude Sonnet 3.7 conditional-grouped-summary     3 C     &lt;tibble&gt;\n 7 Claude Sonnet 3.7 correlated-delays-reasoning     1 P     &lt;tibble&gt;\n 8 Claude Sonnet 3.7 correlated-delays-reasoning     2 C     &lt;tibble&gt;\n 9 Claude Sonnet 3.7 correlated-delays-reasoning     3 P     &lt;tibble&gt;\n10 Claude Sonnet 3.7 curl-http-get                   1 I     &lt;tibble&gt;\n# ℹ 380 more rows\n\n\nIn this dataset, each row represents a single time a solver is invoked to answer a question:\n\nmodel gives the model used to solve a given question\nid gives the question id\nepoch identifies the run/resample of the given question\nscores shows whether the scoring model (Claude Sonnet 3.7) identified the solver’s answer as Incorrect, Partially Correct, or Correct. It’s an ordinal factor with I &lt; P &lt; C.\nmetadata is a list column containing just about all of the information that vitals collects during the evaluation process.\n\nWe’re interested in which of these three models are right more often. We have 26 unique questions, each resampled across 3 epochs for each of 5 models. For a cursory analysis, we could do the canonical Bar Chart Dodged By Model visualization:\n\nare_eval %&gt;%\n  mutate(\n    score = fct_rev(score),\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = score, fill = model)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_manual(values = c(\n    \"Claude Sonnet 3.7\" = \"#d6941a\", \n    \"GPT-4o\" = \"#0f4c81\", \n    \"GPT-4.1\" = \"#4f86c6\", \n    \"GPT-4.1 mini\" = \"#6a9ed4\",  \n    \"GPT-4.1 nano\" = \"#89b9e2\" \n  )) +\n  labs(\n    x = \"Score\", y = \"Count\", fill = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"While the newest GPT 4.1 series models tend to solve R coding problems\\nmore effectively than GPT-4o, they still seem to lag behind Claude Sonnet 3.7.\"\n  ) +\n  theme(plot.subtitle = element_text(face = \"italic\"))\n\n\n\n\n\n\n\nCould the differences we’re seeing be attributed to random noise, though? We can use a hierarchical modeling technique called a mixed model to model the probability of each score (i.e., correct, etc.) as a function of the LLM. In this case, observations are not independent; some questions may be harder than others, and we’re repeating each question multiple times since we’ve set epochs = 3. A random intercept on the question id can help account for this variation. Since score is ordinal, we use a cumulative link mixed model rather than the usual suspect lme4::glmer():\n\nlibrary(ordinal)\n\nare_mod &lt;- clmm(score ~ model + (1|id), data = are_eval)\n\n\nsummary(are_mod)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  390  -268.62 551.24 272(1363) 1.69e-05 5.8e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 13       3.605   \nNumber of groups:  id 26 \n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \nmodelGPT-4o        -2.3531     0.4359  -5.398 6.72e-08 ***\nmodelGPT-4.1       -1.2544     0.4116  -3.048 0.002307 ** \nmodelGPT-4.1 mini  -1.6051     0.4135  -3.882 0.000104 ***\nmodelGPT-4.1 nano  -1.6902     0.4195  -4.029 5.60e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -2.3073     0.7963  -2.897\nP|C   0.1779     0.7782   0.229\n\n\nFirst, let’s take a look at the Coefficients table. We have coefficients for each model other than Claude Sonnet 3.7, which is our “reference” model. Negative Estimates indicate lower odds of achieving higher rating categories, and the Pr(&gt;|z|) values to their right show the p-values associated with those coefficients. We have evidence here that Claude Sonnet 3.7 is the strongest contender on this eval. While these estimates show that GPT 4.1 is closest to Claude Sonnet 3.7’s performance, followed by GPT 4.1 mini, 4.1 nano, and then 4o, we haven’t tested whether those pairwise differences could be attributed to random noise.\nThe Threshold coefficients describe whether ratings of Incorrect vs. Partially Correct and Partially Correct vs. Correct are meaningfully different from each other. The thresholds establish the baseline “difficulty” of achieving each category on the grading scale; more negative values for a pair of grades indicate that moving between those grades is relatively easy. If we saw that both coefficients here were quite negative, we could conclude that the rating system has a strong tendency toward higher ratings overall. In our case, those ratings seems relatively balanced.\nFinally, the substantial Random effects value here shows that there’s substantial heterogeneity in question difficulty that’s being captured by the model. We can visualize these question-level effects:\n\n\n\n\n\n\n\n\nEach of the rows here is a given question, where smaller random intercept estimates indicate that a question is more difficult. The most challenging sample was “after-stat-bar-heights” where, across all LLMs and epochs, 0 of 15 scores were categorized as correct. As this eval’s author, this is an indication to me that I should audit these questions and determine if they’re answerable at all; it’s fine if these are just hard questions, but if there’s not enough information in the question to actually answer it, or if the grading guidance is incorrect, this is a bug in the eval dataset rather than a measure of these models’ coding ability.\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye out for a vitals vignette with a more thorough model-based analysis than this one in the near future.\n\n\nAltogether:\n\nThe GPT 4.1 series of models does seem to improve on GPT-4o for solving R coding problems.\nClaude Sonnet 3.7 still outperforms GPT-4o and the GPT 4.1 series of models on R coding.\nAt least for this sort of problem, the GPT 4.1 nano model seems to pack quite the punch for its price point.\n\nGiven this set of releases’ apparent focus on instruction-following and the relatively strong performance of the nano model here, I’m now curious if GPT 4.1 nano (or even mini) would make for a good model to underlie the chores and gander packages, which require a model that’s very good at pattern-matching and instruction-following and don’t necessarily rely on extensive coding prowess otherwise.\n\nThank you to Max Kuhn for advising on the model-based analysis here."
  },
  {
    "objectID": "blog/2023-07-28-moving-on-s23/index.html",
    "href": "blog/2023-07-28-moving-on-s23/index.html",
    "title": "Moving On From Baltimore",
    "section": "",
    "text": "2 years ago, I sent out a post around the time I graduated from college. I wrote about my gratitude for my time in Oregon, my excitement to start a PhD program in Biostatistics at Johns Hopkins, and the joy of raising my then-very-young dog, Millie. Soon after, in summer 2021, I made the road trip to Baltimore, MD.\nMy time in Baltimore didn’t go as expected. Coming out of my first semester, after a tough housing situation, a new medical condition to come to terms with, and disillusion with the program at Hopkins, I decided to take a year of leave to get my feet under me. Leaving the program and forfeiting my NSF Graduate Research Fellowship felt like both a great loss and a huge risk; I invested a lot of time and energy and ego into the pursuit of becoming a “statistician,” and had been quite lucky while doing so, and I wasn’t sure what work-life looked like after letting that go.\nA few months later, I started full-time on the tidymodels team at RStudio (soon-to-be Posit). I had interned and contracted with the team before, and landing the gig felt too good to be true. The work felt a lot better to me and, at the end of 2022, I decided to officially drop out of the PhD program at Hopkins. I’m grateful to feel well-supported in my development as a statistical thinker and tool builder at Posit, and to really believe in what we’re making together.\nI often feel like I’m supposed to think of my time at Hopkins as a failure. It was surely not the place for me. At the same time, if I hadn’t tried it, I would definitely be wondering around this time, a couple years after graduating, whether I ought to pursue that path; I feel like my time in the PhD program was a success in that I now know much better what I do—and do not—want. I also met life-long friends, got to check out a lot of the US Northeast, and found a wonderful partner.\nA couple months ago, I tagged along with that partner in moving to Chicago, IL. It’s greener, it’s easier to get around, the weather has been awesome1, and Millie has a big-ass park to run around in. I like it here.\nI’ll end this one the same way that I ended that post two years ago:\nThings are so different now."
  },
  {
    "objectID": "blog/2023-07-28-moving-on-s23/index.html#footnotes",
    "href": "blog/2023-07-28-moving-on-s23/index.html#footnotes",
    "title": "Moving On From Baltimore",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt will get really fucking cold. I know. :)↩︎"
  },
  {
    "objectID": "blog/2025-04-11-chores/index.html",
    "href": "blog/2025-04-11-chores/index.html",
    "title": "Introducing chores",
    "section": "",
    "text": "Note\n\n\n\nThe following is a cross-post of a post I put together for the Posit Blog; you can read that post here.\nWhat’s a 1-minute data science task you do all of the time? Could you teach another data scientist to do it with a couple paragraphs of explanation and an example or two? If so, you might benefit from checking out chores, a new package that helps you with tedious but hard-to-automate tasks.\nchores followed up on the initial release of ellmer, a package that makes it easy to use large language models (LLMs) from R. The package connects ellmer to your source editor in RStudio and Positron via a collection of chore helpers."
  },
  {
    "objectID": "blog/2025-04-11-chores/index.html#example",
    "href": "blog/2025-04-11-chores/index.html#example",
    "title": "Introducing chores",
    "section": "Example",
    "text": "Example\nAs an R package developer, one thing I do many times a day is write R function documentation using roxygen2. For example, imagine I’m working on this function:\n\n\nA screenshot of an RStudio session.\n\nThe function grabs the value of an environmental variable, providing a nicely formatted error message if it can’t find it. For example, the HOME environmental variable shows my home folder on macOS:\n\nkey_get(\"HOME\")\n\n[1] \"/Users/simoncouch\"\n\n\nThis variable doesn’t exist, though:\n\nkey_get(\"IM_A_SILLY_GOOSE\")\n\nError:\n! Can't find env var `IM_A_SILLY_GOOSE`.\n\n\nThis function is ready to export from my package. So, the next step is to write inline documentation using roxygen2. Before I can write about how this function is connected to other topics in my package and any interesting notes about how it can be used, there’s 100-characters-or-so of boilerplate that basically follows directly from the source code of the function. e.g. there are two arguments—nameand error_call—and one of them has a default argument, and so on. That will take me a minute or so to type out, and I won’t really enjoy doing so.\nWhat if I didn’t have to? The chores package supplies a helper for this task: scaffolding out minimal roxygen2 documentation.\n\nIn the above video, I:\n\nSelect my function,\nPress a keyboard shortcut,\nSelect the “roxygen” helper from a dropdown, and\nPress [Enter]\n\nAt that point, inline roxygen2 comments begin streaming into my document about my function. Notably, the output is quite minimal; the @param entries just give the argument name and a description of their type, there’s a brief @returns section, and an @export tag. These are the bits of roxygen2 boilerplate that I would likely have written almost exactly the same if I had done so myself. Once this boilerplate is in place, I can check over it and add notes on the interesting parts of what the function does. This is a minute worth of work I wouldn’t particularly enjoy doing—and that would otherwise be quite difficult to automate without LLMs—done for me."
  },
  {
    "objectID": "blog/2025-04-11-chores/index.html#how-it-works",
    "href": "blog/2025-04-11-chores/index.html#how-it-works",
    "title": "Introducing chores",
    "section": "How it works",
    "text": "How it works\nThe chores package manages a directory of markdown files, each of which contains instructions on how to do a certain task, or “prompts.” Here’s the current prompt associated with the roxygen chore:\n\nShow the code: roxygen-chore-prompt.md# Templating function documentation\n\nYou are a terse assistant designed to help R package developers quickly template out their function documentation using roxygen2. Given some highlighted function code, return minimal documentation on the function's parameters and return type. Beyond those two elements, be sparing so as not to describe things you don't have context for. Respond with *only* R `#'` roxygen2 comments---no backticks or newlines around the response, no further commentary.\n\nFor function parameters in `@params`, describe each according to their type (e.g. \"A numeric vector\" or \"A single string\") and note if the parameter isn't required by writing \"Optional\" if it has a default value. If the parameters have a default enum (e.g. `arg = c(\"a\", \"b\", \"c\")`), write them out as 'one of `\"a\"`, `\"b\"`, or `\"c\"`.' If there are ellipses in the function signature, note what happens to them. If they're checked with `rlang::check_dots_empty()` or otherwise, document them as \"Currently unused; must be empty.\" If the ellipses are passed along to another function, note which function they're passed to.\n\nFor the return type in `@returns`, note any important errors or warnings that might occur and under what conditions. If the `output` is returned with `invisible(output)`, note that it's returned \"invisibly.\"\n\nHere's an example:\n\n# given:\nkey_get &lt;- function(name, error_call = caller_env()) {\n  val &lt;- Sys.getenv(name)\n  if (!identical(val, \"\")) {\n    val\n  } else {\n    if (is_testing()) {\n      testthat::skip(sprintf(\"%s env var is not configured\", name))\n    } else {\n      cli::cli_abort(\"Can't find env var {.code {name}}.\", call = error_call)\n    }\n  }\n}\n\n# reply with:\n#' Get key\n#'\n#' @description\n#' A short description...\n#' \n#' @param name A single string.\n#' @param error_call A call to mention in error messages. Optional.\n#'\n#' @returns \n#' If found, the value corresponding to the provided `name`. Otherwise,\n#' the function will error.\n#'\n#' @export\n\nAnother:\n\n# given:\nchat_perform &lt;- function(provider,\n                         mode = c(\"value\", \"stream\", \"async-stream\", \"async-value\"),\n                         turns,\n                         tools = list(),\n                         extra_args = list()) {\n\n  mode &lt;- arg_match(mode)\n  stream &lt;- mode %in% c(\"stream\", \"async-stream\")\n\n  req &lt;- chat_request(\n    provider = provider,\n    turns = turns,\n    tools = tools,\n    stream = stream,\n    extra_args = extra_args\n  )\n\n  switch(mode,\n    \"value\" = chat_perform_value(provider, req),\n    \"stream\" = chat_perform_stream(provider, req),\n    \"async-value\" = chat_perform_async_value(provider, req),\n    \"async-stream\" = chat_perform_async_stream(provider, req)\n  )\n}\n\n# reply with:\n#' Perform chat\n#'\n#' @description\n#' A short description...\n#' \n#' @param provider A provider.\n#' @param mode One of `\"value\"`, `\"stream\"`, `\"async-stream\"`, or `\"async-value\"`.\n#' @param turns Turns.\n#' @param tools Optional. A list of tools.\n#' @param extra_args Optional. A list of extra arguments.\n#'\n#' @returns \n#' A result.\n#'\n#' @export\n\n# given:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    cli::cli_abort(\"Function must have at least two arguments.\", .internal = TRUE)\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\n\n# reply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\n\nWhen two functions are supplied, only provide documentation for the first function, only making use of later functions as additional context. For example:\n\n# given:\ncheck_args &lt;- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names &lt;- names(formals(fn))\n  if (length(arg_names) &lt; 2) {\n    error_less_than_two_args()\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\n\nerror_less_than_two_args &lt;- function(call = caller_env()) {\n  cli::cli_abort(\"Function must have at least two arguments.\", call = call, .internal = TRUE)\n}\n \n# reply with:\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\n\n\nWhen a prompt file lives in the chores directory, the name of the file appears in that dropdown shown in the video. Then, when you select a chore from the dropdown, its associated prompt is submitted to the model along with the code selection you’ve made using ellmer. The response from the model is then written to your file as it’s received.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re an ellmer user, here’s what it’d look like if you wrote the above request out manually:\n\nlibrary(ellmer)\n\nch &lt;- chat_claude(system_prompt = the_prompt_above)\n \nch$chat(\"The selected code.\")"
  },
  {
    "objectID": "blog/2025-04-11-chores/index.html#whats-possible",
    "href": "blog/2025-04-11-chores/index.html#whats-possible",
    "title": "Introducing chores",
    "section": "What’s possible",
    "text": "What’s possible\nThe roxygen helper is one of three helpers supplied with the chores package. There’s also a testthat helper, which transitions unit testing code to the third edition of testthat, and a cli helper, which transitions erroring code to use the cli package. These three helpers that ship with the package are focused on R package development, as that’s where I’ve gotten the most mileage out of the interface.\nThat said, the package allows you to write your own helpers, and all that that takes is a couple paragraphs and an example or two in a markdown file. The chores gallery highlights various custom helpers folks have written since the package came to be:\n\nYour browser does not support iframes.\n\nFor example, Hannah Frick wrote a helper to migrate code chunk options from R Markdown to Quarto format, and Hadley Wickham wrote a helper to polish NEWS changelogs before package releases. If you write your own helper prompt and would like it to be featured in the gallery, I’d be glad to review your pull request!"
  },
  {
    "objectID": "blog/2025-04-11-chores/index.html#other-ellmer-delights",
    "href": "blog/2025-04-11-chores/index.html#other-ellmer-delights",
    "title": "Introducing chores",
    "section": "Other ellmer delights",
    "text": "Other ellmer delights\nWorking on ellmer-based packages really helped me begin to wrap my head around what LLMs are capable of. Provided the right pieces of context and given access to the right tools, they can make the practice of developing software a much more enjoyable experience. It’s been neat to see others in the #rstats community making use of ellmer, too; I wanted to call out a few ellmer-related bits and bobs that I’ve come across recently.\nFor one, Dr. Athanasia Mo Mowinckel wrote up some great reflections on the struggles of using tools like chores with local ollama models in a recent blog post. We’re not quite there yet, but I’m hopeful we will be soon.\n\nAnother one is crias, a turn-based adventure game from Frank Hull. In his words:\n\nIt is built with the idea of using an LLM as a “Game Moderator”. Currently has one GM called “Pachamama”. The other aspect is using LLMs as players in your “party”. Many LLMs have names like “llama”, “vicuna”, “alpaca”; this game is built on the idea of leading your “party” aka “herd” in a llama-centric journey set in an Andean-inspired world.\n\n\nNot to toot my own horn, but I’ve also written about another ellmer-based tool that I built recently—gander—on a post on the Posit Blog. gander is a coding assistant that knows about the objects in your R environment, resulting in more accurate completions.\nI’ve had a lot of fun working with ellmer and am excited to share about more ellmer-based tools soon."
  },
  {
    "objectID": "blog/2025-01-30-llm-biases/index.html",
    "href": "blog/2025-01-30-llm-biases/index.html",
    "title": "Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements",
    "section": "",
    "text": "I’ve been spending some time recently learning about LLMs evaluating output from LLMs, or as its referred to in the literature, “LLM-as-a-judge.” That is, after asking a question to an LLM and receiving an answer, both the question and answer are provided to another language model and that model is asked to somehow judge whether the provided response was satisfactory.\nThere are all sorts of considerations that come into play when designing LLM-as-a-judge systems; LLMs can exhibit many of the same cognitive biases that humans do when judging paired comparisons.1 For example, when asked to choose between two responses, many models will prefer the one presented first, prefer the longer of the two, or prefer the one it generated itself; we’ll look into each of these effects in this blog post. In this post, I’ll examine some data I generated while prototyping out some code for evaluating ellmer outputs. That code isn’t quite usable generally yet, but the kinds of data one can pull out of it already is super interesting.2"
  },
  {
    "objectID": "blog/2025-01-30-llm-biases/index.html#the-data",
    "href": "blog/2025-01-30-llm-biases/index.html#the-data",
    "title": "Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements",
    "section": "The data",
    "text": "The data\nEach row in the dataset is the result of a paired comparison of two model responses. For example, consider the following question asked of a language model:\n\nMake the bars side-by-side rather than stacked on top of each other:\nggplot(mtcars) + \n  aes(x = cyl, fill = factor(vs)) + \n  geom_bar()\n\nThat model’s system prompt has been set to the following:\n\nWhen asked a question about R code, reply with only the code needed to answer the question. No exposition, no backticks.\n\nIdeally, I’d like the model to respond with changes that implement the requested change and only the requested change. No exposition, no additional formatting, so something like this:\nggplot(mtcars) + \n  aes(x = cyl, fill = factor(vs)) +\n  geom_bar(position = \"dodge\")\nNow, I ask a few different models—OpenAI’s GPT-4o, Anthropic’s Claude Sonnet 3.5, and a locally hosted Qwen 2.5 model—to respond to this query. (This is an admittedly arbitrary bunch. Just what I had configured at the moment.) Then, I take a bunch of paired comparisons, where e.g. a response from GPT-4o is compared to one from Claude. Those comparisons are evaluated by letting each of GPT-4o, Claude, and Qwen take their turn as a “judge,” where they’re prompted to choose one or the other based on a reference “target” and some criteria. In this example, I blended the prompts from two recent research papers (Ye et al. 2024; Schroeder and Wood-Doughty 2024):\n\nPlease act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user’s instructions and best resembles the provided target response.\nYour evaluation should consider factors such as the helpfulness, relevance, accuracy, conciseness, and level of detail of their responses, though be terse and don’t provide your reasoning, just your judgement.\n\nEnsure that the order in which the responses were presented does not influence your decision.\nDo not allow the length of the responses to influence your evaluation.\nDo not favor certain names of the assistants.\nBe as objective as possible.\n\nOutput your final verdict by strictly following this format: “Best Response: [[letter]]”. Include the brackets, so “Best Response: [[A]]” if assistant A is better, “Best Response: [[B]]” if assistant B is better.\n&lt;userInput&gt;\nMake the bars side-by-side rather than stacked on top of each other:\nggplot(mtcars) + aes(x = cyl, fill = factor(vs)) + geom_bar()\n&lt;/userInput&gt;\n&lt;targetResponse&gt;\nggplot(mtcars) + \n   aes(x = cyl, fill = factor(vs)) + \n   geom_bar(position = \"dodge\")\n&lt;/targetResponse&gt;\n&lt;assistantResponseA&gt;\nggplot(mtcars) + \n  aes(x = cyl, fill = factor(vs)) + \n  geom_bar(position = position_dodge())\n&lt;/assistantResponseA&gt;\n&lt;assistantResponseB&gt;\nggplot(mtcars) + \n  aes(x = cyl, fill = factor(vs)) + \n  geom_bar(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    x = \"Number of Cylinders\",\n    y = \"Count\",\n    fill = \"Engine Type\"\n  ) +\n  theme_minimal()\n&lt;/assistantResponseB&gt;\n\n\n\n\n\n\n\nWarning\n\n\n\nI’ve added some code fences inside the XML tags in this example prompt just for readability, but they wouldn’t actually appear in the prompt (so that the models could penalize models that include code fences despite being asked not to).\n\n\nNote that, in this prompt, I don’t note which models each response comes from, instead referring to them as “Response A” and “Response B.” This is intended to guard against the most blatant “That result is labeled Claude and I am Claude” kind of self-enhancement bias.\n\nlibrary(tidyverse)\n\n\nload(\"data/ggplot2_evals.rda\")\n\nglimpse(ggplot2_evals)\n\nRows: 882\nColumns: 9\n$ config_a       &lt;chr&gt; \"OpenAI gpt-4o\", \"OpenAI gpt-4o\", \"OpenAI gpt…\n$ config_b       &lt;chr&gt; \"Qwen2.5 14b\", \"Qwen2.5 14b\", \"Qwen2.5 14b\", …\n$ judge          &lt;chr&gt; \"Claude Sonnet 3.5\", \"OpenAI gpt-4o\", \"Qwen2.…\n$ choice         &lt;chr&gt; \"OpenAI gpt-4o\", \"OpenAI gpt-4o\", \"OpenAI gpt…\n$ input          &lt;chr&gt; \"Add a square in each box to show the mean:\\n…\n$ target         &lt;chr&gt; \"ggplot(mtcars, aes(x = factor(cyl), y = mpg)…\n$ response_a     &lt;chr&gt; \"ggplot(mtcars, aes(x = factor(cyl), y = mpg)…\n$ response_b     &lt;chr&gt; \"ggplot(mtcars, aes(x = factor(cyl), y = mpg)…\n$ response_judge &lt;chr&gt; \"Best Response: [[A]]\", \"Best Response: [[A]]…\n\n\nEach row in this data represents one paired comparison on an LLM eval task centered on writing, revising, and debugging ggplot2 code. Column-wise:\n\n\nconfig_a and config_b are descriptions of the models used to generate a response\n\njudge is the model used to judge the responses from config_a and config_b\n\n\nchoice is the value of config_a or config_b chosen by the judge, or NA if it didn’t make a decision.\n\ninput is the user request, target is the “ideal” output, and response_* variables represent the raw output from each model.\n\nI let three models take a go at the eval and gave the same three each a turn at being the judge:\n\nsort(unique(ggplot2_evals$config_a))\n\n[1] \"Claude Sonnet 3.5\" \"OpenAI gpt-4o\"     \"Qwen2.5 14b\"      \n\nsort(unique(ggplot2_evals$judge))\n\n[1] \"Claude Sonnet 3.5\" \"OpenAI gpt-4o\"     \"Qwen2.5 14b\"      \n\n\nIf you’re interested in poking at this data yourself, the raw data is here. It’s worth saying that this data is very limited in its practical utility, and the analyses in this blog post are just for funsies."
  },
  {
    "objectID": "blog/2025-01-30-llm-biases/index.html#position-bias",
    "href": "blog/2025-01-30-llm-biases/index.html#position-bias",
    "title": "Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements",
    "section": "Position bias",
    "text": "Position bias\nLLMs tend to exhibit position bias when making paired comparisons, where they tend to prefer the first response presented to them over the second (or vice versa) despite response quality (Wang et al. 2023). For the purposes of this experiment, I just assign one model to be config_a and the other config_b randomly, though there’s some research out there proposing better ways to address this issue. We can see though that each of these three models seem to demonstrate substantial position bias:\n\nggplot2_evals %&gt;%\n  mutate(\n    choice_position = if_else(choice == config_a, \"First\", \"Second\")\n  ) %&gt;%\n  ggplot() +\n  aes(x = judge, fill = choice_position) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Judge\", y = \"Proportion\", fill = \"Position\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA more rigorous way to quantify this would be position consistency, where we run the comparison with both orders and see how often the model prefers the same response either way (Gu et al. 2025)."
  },
  {
    "objectID": "blog/2025-01-30-llm-biases/index.html#verbosity-bias",
    "href": "blog/2025-01-30-llm-biases/index.html#verbosity-bias",
    "title": "Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements",
    "section": "Verbosity bias",
    "text": "Verbosity bias\nAnother bias that many LLMs seem to demonstrate is verbosity bias, where longer responses are preferred over shorter ones (Ye et al. 2024). One way we could measure this is how often in this data models choose the response that has more characters.\n\nggplot2_evals %&gt;%\n  mutate(\n    chose_a = choice == config_a,\n    a_longer_than_b = nchar(response_a) &gt; nchar(response_b),\n    choice_length = \n      if_else(\n        (a_longer_than_b & chose_a) | (!a_longer_than_b & !chose_a),\n        \"Longer\",\n        \"Shorter\"\n      )\n  ) %&gt;%\n  ggplot() +\n  geom_bar(position = \"fill\") +\n  aes(x = judge, fill = choice_length) +\n  labs(x = \"Judge\", y = \"Proportion\", fill = \"Preferred Length\")\n\n\n\n\n\n\n\nAt least by this simplistic metric, verbosity bias in the “longer is better” direction doesn’t seem to be the case.3 At the same time, the models are definitely preferring the shorter responses over the longer ones in this data, though this may just be the (desired) effect of models choosing the most minimal solution to the problem."
  },
  {
    "objectID": "blog/2025-01-30-llm-biases/index.html#sec-self-enhancement-bias",
    "href": "blog/2025-01-30-llm-biases/index.html#sec-self-enhancement-bias",
    "title": "Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements",
    "section": "Self-enhancement bias",
    "text": "Self-enhancement bias\nAnother interesting effect is self-enhancement bias, where models are likely to prefer their own answer over one supplied by another model, even when they don’t know where a given response arises from (Ye et al. 2024).\n\nggplot2_evals %&gt;%\n  rowwise() %&gt;%\n  mutate(judged_itself = judge %in% c(config_a, config_b)) %&gt;%\n  filter(judged_itself) %&gt;%\n  mutate(\n    choice_self = if_else(judge == choice, \"Itself\", \"Other Model\")\n  ) %&gt;%\n  ggplot() +\n  aes(x = judge, fill = choice_self) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Judge\", y = \"Proportion\", fill = \"Choice\")\n\n\n\n\n\n\n\nIn this case, we really don’t see that at all. It may be the case in this example that, given there are generally a limited set of ways to get to the “right” answer in this eval, models don’t have much of an opportunity to display their “taste,” which is presumably the thing they’re exhibiting when they display this bias."
  },
  {
    "objectID": "blog/2025-01-30-llm-biases/index.html#okay-but-who-won",
    "href": "blog/2025-01-30-llm-biases/index.html#okay-but-who-won",
    "title": "Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements",
    "section": "Okay, but who won?",
    "text": "Okay, but who won?\nThis data really isn’t good enough to state we’ve done anything quite resembling an “eval,” but we can at least check out which model gave the preferred response most often:\n\nggplot2_evals %&gt;%\n  ggplot() +\n  aes(x = choice) +\n  geom_bar() +\n  labs(x = \"Judge\", y = \"# Times Response Was Preferred\")\n\n\n\n\n\n\n\nOkay, Qwen!"
  },
  {
    "objectID": "blog/2025-01-30-llm-biases/index.html#footnotes",
    "href": "blog/2025-01-30-llm-biases/index.html#footnotes",
    "title": "Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you’re interested in learning about this, I put together this primer.↩︎\nThis post is based on output generated using the code in the commit message here.↩︎\nYe et al. (2024) use a much more nuanced analysis method here.↩︎"
  },
  {
    "objectID": "blog/2021-08-21-infer-v-1-0-0/index.html",
    "href": "blog/2021-08-21-infer-v-1-0-0/index.html",
    "title": "{infer} v1.0.0 is on CRAN",
    "section": "",
    "text": "In Fall 2017, during my first semester at Reed, I took an introductory statistics course. A prospective biology major, I was told learning how to code would be helpful later on in my coursework. I used R for the first time during the labs for that class, getting to know packages like {ggplot2} and {dplyr}. Another cornerstone of that class was {infer}, a package whose ubiquity I assumed was similar to {ggplot2} and {dplyr}. At the time, the thought that packages changed over time, or were written by people, or even had source code, had probably yet to occur to me.\nDuring the spring semester, I somehow heard through the math department grapevine that our professor, Andrew Bray, had himself written the package (with help from many co-authors, of course). Our cohort had been the first to learn about inference with {infer} as an aid—a package that was, at the time, just a repository on Andrew’s GitHub profile, yet to be sent off to CRAN.\nIn the years that followed, {infer} changed a lot. The package’s first CRAN release came in early 2018, and its core authors worked through dozens of releases, adding new test statistics, improving visualization methods, and so on.\nI changed a lot during those years, too. I got really into the whole R thing (goodbye, biology major!), traveled to many places I’d never dreamed of traveling to, and met a lot of folks with whom I made memories I’ll never forget. But, yeah—the R thing!\nAndrew helped me make my first pull request to {infer} in the fall of my sophomore year. I made some edits to the vignettes and asked “is this right?” maybe 40 times before ultimately submitting the PR. The {infer} dev team, characteristically, was incredibly thoughtful, kind, and helpful in bringing my submission up to snuff. Over time, I became a bit cozier with R and the flow of collaborative package development, ultimately signing on as an intern (and then contractor) with RStudio, taking over as maintainer of {broom}, and writing the {stacks} package.\nIn full circle, this last summer, I was supported by package co-author Mine Çetinkaya-Rundel and RStudio to wrap up some loose ends on the {infer} package. The package, after several semesters utilized in many classrooms outside of Reed’s walls, had proved to be a valuable tool for teaching statistical inference intuitively. At the same time, its shortcomings as a pedagogical tool were well-acknowledged, and a “production release” of the package would need to reckon with these shortcomings. While the package expressed randomization-based inference on one or two variables well, its functionality could not take an introductory statistics class from start to finish; lack of support for multiple explanatory variables and smooth, theory-based distributions meant that instructors needed one-off solutions from different packages to finish out a full semester.\nSo—we did the thing! A good few months of thoughtful discussions resulted in a grammar that feels good. No silver bullets, but nevertheless an interface that is principled and unified. I spent much of the summer working on this release, and it was wrapped up and shipped off to CRAN last week. Huzzah!\nI’m super grateful for the opportunity to work on this project, and wrote a good bit about the package and what’s new in v1.0.0 in a blog post on the tidyverse blog. I’m hopeful that this release will “fill the gaps” in teaching an introductory statistics class with {infer} as an aid, start to finish.\nA genuine thank you to all of those in the greater {infer} orbit. I’m better for knowing each of you.\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2021-05-13-dev-docs-p4/index.html",
    "href": "blog/2021-05-13-dev-docs-p4/index.html",
    "title": "Big Things (Developer Documentation pt. 4)",
    "section": "",
    "text": "This is the last of four blog posts on the development process of the {stacks} package, excerpted from my Reed senior thesis project Tidy Model Stacking with R.\nPart 1: Introduction\nPart 2: Splitting Things Up\nPart 3: Naming Things\nThe final blog post in this series is somewhat less argumentative than the first two, but speaks to Reiderer’s concept of “developer documentation” much the same. Rather than (explicitly) defending my API choices, I’d like to provide some context for some of the more wiggly elements of {stacks}’ infrastructure. That is, some components of the package’s implementation are weird—some may say hacky, if feeling particularly violent—and the reasoning for these choices often tracks back to the same issue; things are Big.\nBig things. I mean this both in the sense that many of the inputs to {stacks} functions are almost inevitably very large data objects requiring a non-negligible amount of computing time to generate and that {stacks} itself takes a long time to do its thing, returning data objects that are even larger than it was supplied. The tools of the R package development trade are varyingly equipped to accommodate such things—those of the Big variety—and thus, some wiggliness is required. I’ll continue using Big to evoke some intersection of these two qualities, both for notational convenience and goofiness.\nThere are a few things I’m referring to when I say “tools of the R package development trade.” Initially, I mean R itself and the tools provided by the R core team to articulate what a proper package looks like and check that this is the case. Namely, the R CMD check set of checks exhaustively defines the bounds of a “valid” R package. Further, I also mean the Comprehensive R Archive Network (CRAN), a centralized repository that curates and hosts thousands of the most widely used packages in the R community, as well as the team of volunteers supporting it. The CRAN team also contributes and maintains its own extensions to R CMD check for packages hosted in its repositories, asserting guidelines that improve maintainability (and thus user experience) for both the CRAN team and contributed package maintainers (Claes, 2014).\nEach of the tools mentioned above are tremendously positive forces in the R community. At the same time, they must make assertions (whether explicit or implicit) about the “smells and feels” of R packages that are one-size-fits-all, so to speak, in order to shepherd the homogeneity required to articulate coherent bounds on what an R package is (Bryan, 2018). In the case of {stacks}, some of these assertions introduce the need for particularly wiggly workarounds.\nThe following bounds, excerpted from the CRAN repository policy, introduce the need for the vast majority of these wiggles:\nAgain, I contend that the above restrictions are justified and necessary given the scope of the CRAN team’s resources. Beyond making the maintenance of CRAN a more feasible task, too, these restrictions also provide expectations from which popular package development tools can draw from in determining functionality.\nI’ll begin this by demonstrating more precisely what I mean by Big. Then, in the following section, I will illustrate more clearly the tension between this Bigness and the aforementioned bounds. In the following sections, I describe three strategies for reckoning with this tension—wiggling thoughtfully, if you will."
  },
  {
    "objectID": "blog/2021-05-13-dev-docs-p4/index.html#footnotes",
    "href": "blog/2021-05-13-dev-docs-p4/index.html#footnotes",
    "title": "Big Things (Developer Documentation pt. 4)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe code mentioned here will be available when the full thesis is made available–I’ll make sure to note here when that’s the case!↩︎"
  },
  {
    "objectID": "blog/2020-11-30-stacks-0-1-0/index.html",
    "href": "blog/2020-11-30-stacks-0-1-0/index.html",
    "title": "{stacks} v0.1.0 is on CRAN!",
    "section": "",
    "text": "Yall!\n\n\n\nOver the past few months, I’ve been focused on the development of {stacks}, an R package for tidymodels-aligned stacked ensemble modeling. This package began as an internship project at RStudio, and is now the focus of my undergraduate thesis. Today, the package was released on CRAN.\nYou can install the package with the following code:\ninstall.packages(\"stacks\")\nTo load the package:\nlibrary(stacks)\nTo learn more:\n\nThe official release blog post can be found here\nThe package’s website is here\nI wrote up a blog post with an example of using the package to build an ensemble here\n\nThank you to Julie Jung for contributing the package’s hex sticker as well as research data used in examples throughout the package. Also, thanks to those who have tested and provided feedback on the developmental versions of the package over the last few months!\nHappy modeling. 🙂\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2020-10-11-intro/index.html",
    "href": "blog/2020-10-11-intro/index.html",
    "title": "The Blog Thing! We’re Doing It!",
    "section": "",
    "text": "Hi yall!\nFor a while now, I’ve wished I had a place to share some short posts on what I’m up to related to R, data science, and statistics. I decided I’d take a Sunday to get this subdomain up and running! This setup uses blogdown and is based on a fork of Julia Silge’s website!\nThere’s more information about myself and what I work on on my main website.\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2024-03-14-oracle/index.html",
    "href": "blog/2024-03-14-oracle/index.html",
    "title": "Run an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS",
    "section": "",
    "text": "I recently sunk a few days into getting an Oracle database deployed on MacOS with an M1 chip in a docker container via Docker Desktop. The few solutions that I found recommended using the Docker Desktop alternative Colima and/or using publicly available, community-contributed images; I had various troubles getting these solutions to work, and found myself missing the bells and whistles of Docker Desktop along the way. This morning, I finally got this database deployed by building the image myself from the official Oracle source, in Docker Desktop rather than Colima, and thought it’d be worth writing up how I did so, especially given the countless GitHub issue comments, StackOverflow posts, and forum discussion I came across from others in my situation.\nWhat ended up sending me down the right path was this FAQ answer on the official Oracle source repository:\nIf this is enough information to get you going, then off you go! It took me a couple hours to iron out the details, and I documented my steps as I went."
  },
  {
    "objectID": "blog/2024-03-14-oracle/index.html#download-oracle-database",
    "href": "blog/2024-03-14-oracle/index.html#download-oracle-database",
    "title": "Run an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS",
    "section": "Download Oracle Database",
    "text": "Download Oracle Database\nDownload Oracle Database 19c, listed as Oracle Database 19c (19.19) for LINUX ARM (aarch64). No need to unzip the result .zip file. You’ll need a (free) Oracle account to do so.\nFrom what I understand, this is the only version of Oracle Database supported on Apple Silicon chips."
  },
  {
    "objectID": "blog/2024-03-14-oracle/index.html#clone-oracles-docker-image-source",
    "href": "blog/2024-03-14-oracle/index.html#clone-oracles-docker-image-source",
    "title": "Run an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS",
    "section": "Clone Oracle’s docker image source",
    "text": "Clone Oracle’s docker image source\nThen, clone the oracle/docker-images repository. The repository contains Dockerfiles and samples to build Docker images for a bunch of Oracle’s products, but we’re specifically interested in Oracle Database.\nIf you’re an R user, you can run the following to do so:\nusethis::create_from_github(\"oracle/docker-images\")\nIn your favorite IDE, navigate to your cloned repository. (In RStudio, usethis will do this automatically.)\nIn the repository folder, navigate to OracleDatabase/SingleInstance/dockerfiles/19.3.0 and paste the .zip file you just downloaded there. The install script expects that .zip file to be named LINUX.ARM64_1919000_db_home.zip, as it should be by default."
  },
  {
    "objectID": "blog/2024-03-14-oracle/index.html#build-the-image",
    "href": "blog/2024-03-14-oracle/index.html#build-the-image",
    "title": "Run an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS",
    "section": "Build the image",
    "text": "Build the image\nChange your working directory to OracleDatabase/SingleInstance/dockerfiles/, using cd OracleDatabase/SingleInstance/dockerfiles/ if you have the docker-images repository as your current working directory. Then, run the following in Terminal:\n./buildContainerImage.sh -v 19.3.0 -e\n\n-v 19.3.0 specifies that we’re building the image for version 19.3.0, the version of Oracle Database we’ve downloaded.\n-e specifies that we want to build the image for the Enterprise Edition, which is the only release that’s currently supported.\n\nIf you encounter errors, you can check logs in the Builds tab of Docker Desktop. A relatively thorough FAQ on debugging this build can be found here.\nOnce the build script completes, you should see something like the following in your terminal:\n  Oracle Database container image for 'ee' version 19.3.0 is ready to be extended: \n    \n    --&gt; oracle/database:19.3.0-ee\n\n  Build completed in 127 seconds."
  },
  {
    "objectID": "blog/2024-03-14-oracle/index.html#running-the-image",
    "href": "blog/2024-03-14-oracle/index.html#running-the-image",
    "title": "Run an Oracle Database with Docker Desktop on ARM (M1, M2, M3) MacOS",
    "section": "Running the image",
    "text": "Running the image\nYou can run the built image by navigating to Images in Docker Desktop and clicking the run button on the oracle/database entry. Set the ORACLE_PWD environmental variable to any value to set the default SYS, SYSTEM, and PDBADMIN passwords. To do so using docker run, write:\ndocker run -d --name oracle -e ORACLE_PWD=YourPass321 oracle/database:19.3.0-ee\n\n-d indicates that you’ll run in detached mode so that you have access to your terminal.\n--name indicates that the container will be named oracle.\n-e indicates that the following key=value will be set as an environmental variable.\noracle/database:19.3.0-ee is the name of your image. If you see a pull access denied for ... error, check the output of docker images in Shell to find the correct ID for your image.\n\nThe container will then be visible in the Containers tab of Docker Desktop. After a few minutes, you should see the following in your container’s logs:\n#########################\nDATABASE IS READY TO USE!\n#########################\nAt this point, you’re ready to go. :)\n\n\n\n\n\n\nNote\n\n\n\nAt the time of writing, you will not be able to connect to this database through ODBC, as Oracle Instant Client does not support macOS aarch64."
  },
  {
    "objectID": "blog/2023-10-11-submodels-rabbit-hole/index.html",
    "href": "blog/2023-10-11-submodels-rabbit-hole/index.html",
    "title": "Down the submodels rabbit hole with tidymodels",
    "section": "",
    "text": "library(tidymodels)"
  },
  {
    "objectID": "blog/2023-10-11-submodels-rabbit-hole/index.html#the-setup",
    "href": "blog/2023-10-11-submodels-rabbit-hole/index.html#the-setup",
    "title": "Down the submodels rabbit hole with tidymodels",
    "section": "The setup",
    "text": "The setup\nA familiar pastime—plotting mtcars:\nggplot(mtcars) +\n  aes(x = hp, y = mpg, color = factor(cyl)) +\n  geom_point()\n\nThe fuel efficiency of a car tends to drop off as the horsepower increases, and more cylinders tend to be associated with higher horsepower. Cozy.\nSuppose we wanted to fit a machine learning model using all of the variables in mtcars to best predict the observed miles per gallon. I hear this is not a well-understood modeling problem.\nform &lt;- mpg ~ .\nWe can make use of a nearest neighbors model, evaluating multiple values of neighbors against resamples to see which one results in the best performance. For a given value of neighbors n and a given observation i, the model finds the n observations in the training data with predictor values closest to i and takes their average to come up with a prediction for the mpg of i. A tidymodels model specification for this model looks like:\nspec &lt;- nearest_neighbor(\"regression\", neighbors = tune())\nMarking the neighbors argument tune() tells the tune package to try out different values for n and see what happens.\nWe can pass these objects to tune to evaluate several values of n against resamples of the mtcars data:\nset.seed(1)\nres &lt;-\n  tune_grid(\n    workflow(form, spec),\n    resamples = vfold_cv(mtcars, v = 3),\n    control = control_grid(extract = extract_fit_engine)\n  )\nWe’ve passed tune_grid() three arguments:\n\nThe first argument is a workflow, combining the formula preprocessor form with the model specification spec.\nThe second argument, resamples, defines a resampling scheme for mtcars.1\nThe third, control, is sprinkled in for funsies. You wouldn’t need to include that argument in many tuning problems, but I’m doing so here to demonstrate something wacky later on. extract = extract_fit_engine tells tune to run the extract_fit_engine() function each time it fits a new model and return that along with the outputted results. For each fitted workflow—presumably, generated from fixing n and training the model—we want to extract the underlying engine fit. tidymodels doesn’t implement training routines for models ourselves, instead relying on modeling engines (in this case, the kknn package) to do so.\n\nOkay.\nres\n## # Tuning results\n## # 3-fold cross-validation \n## # A tibble: 3 × 5\n##   splits          id    .metrics          .notes           .extracts       \n##   &lt;list&gt;          &lt;chr&gt; &lt;list&gt;            &lt;list&gt;           &lt;list&gt;          \n## 1 &lt;split [21/11]&gt; Fold1 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [8 × 3]&gt;\n## 2 &lt;split [21/11]&gt; Fold2 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [8 × 3]&gt;\n## 3 &lt;split [22/10]&gt; Fold3 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [8 × 3]&gt;\nWe gave a go at tuning this model! We can look at the metrics associated with each model fit to see how different values of numbers performed. Each column in res prefixed with a . has a collection function associated with it that binds together results in a convenient way. So, for .metrics, that’s collect_metrics():\ncollect_metrics(res) %&gt;%\n  select(-.estimator)\n## # A tibble: 16 × 6\n##    neighbors .metric  mean     n std_err .config             \n##        &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n##  1         2 rmse    2.89      3  0.240  Preprocessor1_Model1\n##  2         2 rsq     0.776     3  0.109  Preprocessor1_Model1\n##  3         5 rmse    3.01      3  0.396  Preprocessor1_Model2\n##  4         5 rsq     0.802     3  0.0782 Preprocessor1_Model2\n##  5         6 rmse    3.02      3  0.382  Preprocessor1_Model3\n##  6         6 rsq     0.804     3  0.0796 Preprocessor1_Model3\n##  7         8 rmse    3.06      3  0.402  Preprocessor1_Model4\n##  8         8 rsq     0.802     3  0.0831 Preprocessor1_Model4\n##  9         9 rmse    3.09      3  0.431  Preprocessor1_Model5\n## 10         9 rsq     0.799     3  0.0853 Preprocessor1_Model5\n## 11        11 rmse    3.17      3  0.495  Preprocessor1_Model6\n## 12        11 rsq     0.793     3  0.0871 Preprocessor1_Model6\n## 13        13 rmse    3.23      3  0.562  Preprocessor1_Model7\n## 14        13 rsq     0.789     3  0.0866 Preprocessor1_Model7\n## 15        14 rmse    3.27      3  0.596  Preprocessor1_Model8\n## 16        14 rsq     0.787     3  0.0864 Preprocessor1_Model8\nEach row describes the results for a model configuration, .config. The thing that determines a model configuration in our example is the number of neighbors n. Each neighbor is associated with an out-of-sample error metric. Two different metrics, actually, because tidymodels is 😇👼😇.\nA plot might be a bit more illustrative:\nautoplot(res)\n\nWe tried out values for n ranging from 2 to 14. With respect to root mean squared error, at least, more neighbors meant more error.\nNice! Maybe a bit less cozy, but moderately so.\nOff the deep end we go! Let’s use another collection function, collect_extracts(), to bind together the extracted objects:\ncollect_extracts(res)\n## # A tibble: 24 × 4\n##    id    neighbors .extracts  .config             \n##    &lt;chr&gt;     &lt;int&gt; &lt;list&gt;     &lt;chr&gt;               \n##  1 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model1\n##  2 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model2\n##  3 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model3\n##  4 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model4\n##  5 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model5\n##  6 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model6\n##  7 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model7\n##  8 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model8\n##  9 Fold2        14 &lt;trn.kknn&gt; Preprocessor1_Model1\n## 10 Fold2        14 &lt;trn.kknn&gt; Preprocessor1_Model2\n## # ℹ 14 more rows\nUm… not cozy! This output recycles the same neighbors value for every config. Buggy!🐛\nFor a while, we thought so too. This comment lived inside of our source code for years:\n\n\n\nA screenshot of a code snippet in tidymodels source code. A comment, reading: “FIXME: I think this might be wrong?”\n\n\nEmbarrassing. I promise we don’t let bugs hang around that long, usually.\nExcept!!! Except… the feller need not be fixed. This output is right, kind of."
  },
  {
    "objectID": "blog/2023-10-11-submodels-rabbit-hole/index.html#submodels",
    "href": "blog/2023-10-11-submodels-rabbit-hole/index.html#submodels",
    "title": "Down the submodels rabbit hole with tidymodels",
    "section": "Submodels",
    "text": "Submodels\nIn machine learning, on the way to fitting one model, we often end up training many.\nFor example, when training a nearest neighbors model with 5 neighbors, we need to take the average of the outcomes from the 5 closest points to the one we’re trying to predict. One way we could go about that is:\n\ncalculating the distance of all the points in the training set to the one we want to predict,\nsorting by the distance, and then\ntaking the average of the 5 points with the smallest distance.\n\n 1. and 2. are the hard parts. Note that 1. and 2., though, don’t care about the value of n. In the process of training that model for the 5 closest points, we did all of the work we’d need to do to find the 4 closest, or 2 closest, or 10 closest.\ntidymodels takes advantage of this trick to quickly evaluate many models at once. Under the hood, tune_grid() just trains one model, with the value of neighbors with the highest n. Then, using that one model, we can easily generate predictions from all of the models with fewer neighbors and calculate metrics using those predictions. So, the submodel is a model configuration that we haven’t trained, per se, but are still able to generate predictions from. In res, we only trained the model configuration with 14 neighbors, and the rest are submodels."
  },
  {
    "objectID": "blog/2023-10-11-submodels-rabbit-hole/index.html#tuning-with-submodels",
    "href": "blog/2023-10-11-submodels-rabbit-hole/index.html#tuning-with-submodels",
    "title": "Down the submodels rabbit hole with tidymodels",
    "section": "Tuning with submodels",
    "text": "Tuning with submodels\nOkay, back to res. collect_metrics() associates each value of neighbors with a .config entry:\ncollect_metrics(res) %&gt;%\n  select(-.estimator)\n## # A tibble: 16 × 6\n##    neighbors .metric  mean     n std_err .config             \n##        &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n##  1         2 rmse    2.89      3  0.240  Preprocessor1_Model1\n##  2         2 rsq     0.776     3  0.109  Preprocessor1_Model1\n##  3         5 rmse    3.01      3  0.396  Preprocessor1_Model2\n##  4         5 rsq     0.802     3  0.0782 Preprocessor1_Model2\n##  5         6 rmse    3.02      3  0.382  Preprocessor1_Model3\n##  6         6 rsq     0.804     3  0.0796 Preprocessor1_Model3\n##  7         8 rmse    3.06      3  0.402  Preprocessor1_Model4\n##  8         8 rsq     0.802     3  0.0831 Preprocessor1_Model4\n##  9         9 rmse    3.09      3  0.431  Preprocessor1_Model5\n## 10         9 rsq     0.799     3  0.0853 Preprocessor1_Model5\n## 11        11 rmse    3.17      3  0.495  Preprocessor1_Model6\n## 12        11 rsq     0.793     3  0.0871 Preprocessor1_Model6\n## 13        13 rmse    3.23      3  0.562  Preprocessor1_Model7\n## 14        13 rsq     0.789     3  0.0866 Preprocessor1_Model7\n## 15        14 rmse    3.27      3  0.596  Preprocessor1_Model8\n## 16        14 rsq     0.787     3  0.0864 Preprocessor1_Model8\n…and yet collect_extracts() associates all of those .configs with the same neighbors value:\ncollect_extracts(res)\n## # A tibble: 24 × 4\n##    id    neighbors .extracts  .config             \n##    &lt;chr&gt;     &lt;int&gt; &lt;list&gt;     &lt;chr&gt;               \n##  1 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model1\n##  2 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model2\n##  3 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model3\n##  4 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model4\n##  5 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model5\n##  6 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model6\n##  7 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model7\n##  8 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model8\n##  9 Fold2        14 &lt;trn.kknn&gt; Preprocessor1_Model1\n## 10 Fold2        14 &lt;trn.kknn&gt; Preprocessor1_Model2\n## # ℹ 14 more rows\nBoth of these are right, the second one maybe a bit more opinionated.\nRemember that, to evaluate this model, tune only had to fit it once per resample. n = 14 was the largest value of n that we evaluated, so we just fit that model, then generate predictions for all of the models using that one fit, and then calculate metrics from those predictions. We thus have honest-to-god predictions and metrics from several models that we didn’t actually fit, directly at least. The .config entries in this case are associated with predictions.\nHow about the extracted objects, though? Since we only fitted one workflow, we call extract_fit_engine(x) on that one workflow, and that result applies to every model configuration. The .config entries in this case are associated with fits.\nMetrics come from predictions, of which there are many. Extracts come from fitted models, of which there is only one.\nOnce we put a finger to what was going on here, the tidymodels team ultimately decided this is probably the right way to go about it, UI-wise. The output raises an eyebrow, though it really ought to. If a user extracts values from the fitted workflows assuming there really are several models fitted with different values of n, they may erroneously make conclusions about those extracted values. If a user does notice this effect, though, and concludes that it’s okay that the extracted values arise from the same models, the output looks exactly the way it would (other than hyperparameter values) if there were no submodel trick at play; all of the benefits of type stability apply."
  },
  {
    "objectID": "blog/2023-10-11-submodels-rabbit-hole/index.html#other-options",
    "href": "blog/2023-10-11-submodels-rabbit-hole/index.html#other-options",
    "title": "Down the submodels rabbit hole with tidymodels",
    "section": "Other Options",
    "text": "Other Options\nSo, we ultimately opted not to make any changes to functionality here, just improving the documentation we have. Some other options we considered, labeled with the conclusions we ultimately came to about each of them:\n\nMisleading\nWe could backtrack in the source code and make sure that every neighbors entry is paired with the .config entry it’s associated with elsewhere in the output. For example, the collect_extracts(res) results from above might look something like:\ncollect_extracts(res)\n## # A tibble: 24 × 4\n##    id    neighbors .extracts  .config             \n##    &lt;chr&gt;     &lt;int&gt; &lt;list&gt;     &lt;chr&gt;               \n##  1 Fold1         2 &lt;trn.kknn&gt; Preprocessor1_Model1\n##  2 Fold1         5 &lt;trn.kknn&gt; Preprocessor1_Model2\n##  3 Fold1         6 &lt;trn.kknn&gt; Preprocessor1_Model3\n##  4 Fold1         8 &lt;trn.kknn&gt; Preprocessor1_Model4\n##  5 Fold1         9 &lt;trn.kknn&gt; Preprocessor1_Model5\n##  6 Fold1        11 &lt;trn.kknn&gt; Preprocessor1_Model6\n##  7 Fold1        13 &lt;trn.kknn&gt; Preprocessor1_Model7\n##  8 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model8\n##  9 Fold2         2 &lt;trn.kknn&gt; Preprocessor1_Model1\n## 10 Fold2         4 &lt;trn.kknn&gt; Preprocessor1_Model2\n## # ℹ 14 more rows\nThis would help the user in not needing to raise an eyebrow early on, but might obscure that there’s a little bit of tomfoolery going on here. If that user then goes on to process the extracted objects in some way, assuming that they really are fitted with the number of neighbors listed in the output, they might be led to incorrect conclusions.\n\n\nPain In The Ass\nWe could also backtrack and only return rows with the .config indicating the one model that was actually fit. For example:\ncollect_extracts(res)\n## # A tibble: 24 × 4\n##    id    neighbors .extracts  .config             \n##    &lt;chr&gt;     &lt;int&gt; &lt;list&gt;     &lt;chr&gt;               \n##  1 Fold1        14 &lt;trn.kknn&gt; Preprocessor1_Model8\n##  2 Fold2        14 &lt;trn.kknn&gt; Preprocessor1_Model8\n##  3 Fold3        14 &lt;trn.kknn&gt; Preprocessor1_Model1\nThis would probably raise a user’s eyebrow as quickly as our actual approach, and would be a reasonable way to go about this. At the same time, though, for an analyst who may then go on to join this output to other objects from the tuning results (like the collect_metrics() output), the specification of those joins changes depending on whether there are submodels present or not. Also, if that analyst determines that it is fine that the extracted object for a given .config arose from a different model fit, re-associating that extracted object with all of the configurations that it’s predictions are associated with elsewhere in output is a headache.\n\n\nSlow\nWe could also, whenever a user indicates that they want to extract objects when using a tuning approach that involves submodels, ignore the submodel trick and move forward with “re-fitting” models we don’t necessarily need to. This would allow for totally type-stable output! Every row of extracted output does indeed correspond to a unique fitted workflow, and can be associated with the same .config and hyperparameters it would be elsewhere. This would be slooow though, and we like to go fast.\n\n\nCumbersome and buggy\nWe could also tamper with the fitted workflow to look like it’s a model fit that arose from a different set of hyperparameters, so that when users extract things from it, it walks and talks like output that would have arisen from approach 3. The trouble here, though, is that users can extract whatever they want from extracted objects, and there’s no way we could anticipate all the things they might want to extract and thus the “patches” we’d need to make to the object. Those patches could very well be as computationally expensive as re-fitting the workflow and extracting from it.\n\nSo, option zero it is. Raise the curious-looking thing, document it well, and become excited enough about software to write a blog post.\nHappy modeling.🙂"
  },
  {
    "objectID": "blog/2023-10-11-submodels-rabbit-hole/index.html#footnotes",
    "href": "blog/2023-10-11-submodels-rabbit-hole/index.html#footnotes",
    "title": "Down the submodels rabbit hole with tidymodels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is a silly one! 3 folds is quite few—in practice, we’d recommend at least 5, and more if your data allows.↩︎"
  },
  {
    "objectID": "blog/2021-05-11-dev-docs-p2/index.html",
    "href": "blog/2021-05-11-dev-docs-p2/index.html",
    "title": "Splitting Things Up (Developer Documentation pt. 2)",
    "section": "",
    "text": "This is the second of four blog posts on the development process of the {stacks} package, excerpted from my Reed senior thesis project Tidy Model Stacking with R.\nPart 1: Introduction\nPart 3: Naming Things\nPart 4: Big Things\n\nUndoubtedly, one of the most challenging parts of developing {stacks} was determining what working with the package should “feel” like. The foremost challenge here is striking a balance between ease of use and statistical expressiveness. On one end of this spectrum, the package could encompass its operations within one function, accommodating many input types and obscuring statistical complexities for the benefit of code conciseness. On the other, each requisite statistical operation could require explicit code and only accommodate input types reflecting an opinionated statistical practice.\nWhile the latter end of this spectrum is a bit harder to conceptualize, and feels more ridiculous at its extremes, implementations closer to the former are rather common. How “magical” could the functionality in this package be? What does “model stacking made easy” look like? Perhaps the clearest illustration of the benefits of {stacks}’ approach—an initialization function and three “core verbs”—is to begin with an approach on the more magical end of this spectrum. Considering such an approach, I’ll identify some of the more major drawbacks to user understanding and the intermediate operations that are made inaccessible. Then, I’ll uncouple operations that thus ought to be distinct, iterating on this “conscious uncoupling” process until I’ve arrived at a happy medium between conciseness and verbosity (Bryan, 2019). I use the word “syntax” to refer a set of functions, grouping some set of operations in more or less coarse grammars, that could encapsulate the functionality of a model ensembling package. For example, the syntax ultimately implemented in the package consists of stacks, add_candidates, blend_predictions, and fit_members. Generally, discussion of the names of these functions will be left to the next blog post.\n\nSuper auto ML AI magic\nTo start, then, we’ll consider one function—we can call it stack_models—that simply calls the current core verbs in succession. Its definition might look something like the following. (This code is for the purpose of illustration only, and does not work as presented. See the workflow_set S3 method of add_candidates for an iterated implementation of add_candidates that could be utilized in this kind of construction.)\n# initializes a data stack, adds all candidate members,\n# fits the meta-learner on their predictions, and then\n# fits all members with non-zero stacking coefficients\nstack_models &lt;- function(...) {\n  stacks() %&gt;%\n    purrr::reduce(\n      ...,\n      add_candidates\n    ) %&gt;%\n    blend_predictions() %&gt;%\n    fit_members()\n}\nThe purrr::reduce construction here iteratively passes each element of ..., a sequence of model definitions, to add_candidates.\nThe process of fitting an ensemble model with this code would be quite concise, to say the least. With the necessary model definitions created—say model1 and model2—the API would present as such.\n# fit an ensemble model with model1 and model2\nstack_models(\n  model1,\n  model2\n)\nWhat sacrifice does this kind of abstraction implicate?\nTo begin answering this question, we’ll start with the obfuscation of what were ultimately the last two core verbs of the package—blend_predictions and fit_members. The blend_predictions function fits the meta-learner (an elastic net model) on the predictions from each candidate member, determining how to best combine their outputs. Candidate members with non-zero meta-learner coefficients are model stack members, and need to be trained on the full training set rather than the resamples used to construct the data stack.\nIn some cases, though, the step of training a member on the full training set is substantially more computationally intensive than training the analogous candidate on the resampled data used to construct the data stack.\nMore concretely, consider a \\(10\\)-fold cross-validation resampling scheme. The training data is partitioned into \\(10\\) folds of equal size. Then, the candidate model configuration is trained on \\(9\\) of the folds, producing \\(9\\) resampled models. Each of these resampled models then predicts the values for the \\(10\\)th fold—the validation set, or partition of the training folds used for model validation—and {stacks} summarizes those models’ predictions, forming that candidates’ predictions for the validation set.\nNote that each of these \\(9\\) models is trained on a dataset with cardinality equal to a tenth of the cardinality of the full training set. The computational complexity of the training step for many statistical models is “worse” than linear as a function of the cardinality of the dataset; that is, training some statistical models on a dataset with \\(100\\) rows takes much more than \\(10\\) times as long to train that same model on a dataset with \\(10\\) rows.\nConsider, for instance, a thin-plate spline model. The time to fit this model is proportional to the cube of the cardinality of the training set (Hastie, 2009). In our example, then, training on the \\(10\\)-row training set takes \\(10^3\\) time units, and training on the \\(100\\)-row training set takes \\(100^3 = 10^6\\) time units. Thus, training on the full \\(100\\)-row training set takes \\(10^6/10^3 = 1,000\\) times as long as training on the \\(10\\)-row fold of the training set. Thus, training this model on our full \\(100\\)-row training set would take over \\(100\\) times as long as training \\(9\\) models on the resampled \\(10\\)-row folds and summarizing their outputs.\nOther examples of models that scale “worse” than linearly include many varieties of neural networks, K-nearest neighbors, and other discriminant analysis techniques like support vector machines (Hastie, 2009).\nIn the context of {stacks}, this recognition of differences in runtime is crucial. After fitting the meta-learner on the candidates, one can evaluate the degree of penalization chosen and its effect on member selection. If the practitioner specifies a very small penalty, or the grid search specified results in a very small penalty, then most all of the candidate members will need to be fitted—a sometimes profoundly computationally intensive task. Delineating between the steps of training the meta-learner and then training the candidate members that it “selects” gives the practitioner a chance to evaluate her chosen degree of regularization and its impact on model selection. A critical choice of penalty value can often result in a much more parsimonious (and realistically trainable) set of selected member models, saving the practitioner valuable computing time.\nMinimally, then, the package ought to delineate between these two training steps.\n\n\nTwenty four karat magic in the air\nInstead, consider an approach that includes two functions: one that adds a set of candidate members and fits a meta-learner to them, and another that fits the candidates with non-zero stacking coefficients on the full training sets. That approach might look something like this:\n# initializes a data stack, adds all candidate members,\n# and fits the meta-learner on their predictions\nprepare_stack &lt;- function(...) {\n  stacks() %&gt;%\n    purrr::reduce(\n      ...,\n      add_candidates\n    ) %&gt;%\n    blend_predictions()\n}\n\n# the current fit_members function is left as is\nfit_members &lt;- fit_members\nThe process of fitting an ensemble model with this approach would be still be a good bit more concise than the current syntax. This approach, as well, gives the practitioner a moment to consider the computational implications of the chosen penalty before fitting members on the full training set. With the same model1 and model2 model definitions, the syntax would present as such.\n# fit an ensemble model with model1 and model2\nprepare_stack(\n  model1,\n  model2\n) %&gt;%\n  # after evaluating computational considerations,\n  # fit all members with non-zero stacking coefficients\n  fit_members()\nHowever, for one, this approach introduces yet another computational redundancy; the time to add candidates to a data stack, in some use cases, is not trivial.\n\n\nThe obfuscation of add_candidates and blend_predictions in the prepare_stack syntax introduces a similar computational inconsideration that the stack_models approach did. The synthesis of operations introduced in prepare_stack means that if a practicioner fits the meta-learner and finds the grid search specified to be unsatisfactory, she must perform the add_candidates step again, even if the candidate models are the same.\nWhile add_candidates performs a number of operations, its runtime is roughly equivalent to the runtime for prediction for the supplied model definition(s); add_candidates collects predictions on the validation set from each resampled model and summarizes them to generate the vector of validation set predictions for the candidate member. As was the case in the section above, the computational complexity of this operation depends on the specific candidate model types in question. Even when a model’s time-to-predict scales linearly with the cardinality of a dataset, though, this runtime is not negligible, and ought not be carried out more than once if possible.\nTo demonstrate this cost in computing time, we’ll turn example modeling objects used in an earlier part of the thesis. (The code to compute these objects is not currently publicly available; I’ll drop a link here when that’s no longer the case.) In that portion, we generated a k-nearest neighbors (knn_res), ordinary least squares (lin_reg_res), and support vector machine (svm_res) model to predict the sale price of homes in Ames, Iowa.\nInitially, we compute the time to collect the predictions from each of these model definitions using the system.time function.\ntime_to_add &lt;- system.time(\n  ames_st &lt;- \n    stacks() %&gt;%\n    add_candidates(knn_res) %&gt;%\n    add_candidates(lin_reg_res) %&gt;%\n    add_candidates(svm_res)\n)\n\names_st\n## # A data stack with 3 model definitions and 11 candidate members:\n## #   knn_res: 4 model configurations\n## #   lin_reg_res: 1 model configuration\n## #   svm_res: 6 model configurations\n## # Outcome: Sale_Price (numeric)\ntime_to_add[[\"elapsed\"]]\n## [1] 2\nThe time to add these candidate members was 1.6 seconds. Now, evaluating the runtime for fitting the meta-learner for one possible penalty value:\ntime_to_fit &lt;- system.time(\n  ames_st &lt;- \n    ames_st %&gt;%\n    blend_predictions(penalty = .1)\n)\n\names_st\n## ── A stacked ensemble model ─────────────────────────────────────\n## \n## Out of 11 possible candidate members, the ensemble retained 1.\n## Penalty: 0.1.\n## Mixture: 1.\n## \n## The 1 highest weighted members are:\n## # A tibble: 1 x 3\n##   member          type       weight\n##   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;\n## 1 lin_reg_res_1_1 linear_reg  0.371\n## \n## Members have not yet been fitted with `fit_members()`.\ntime_to_fit[[\"elapsed\"]]\n## [1] 10\nThe time to fit the meta-learner in this case was 9.5 seconds. At first glance, the time to add candidates may seem negligible in comparison, given that it took only 16.9% of the time needed to fit the meta-learner.\nHowever, note that the proposed model stack is being evaluated on only 11 potential members and cross-validation folds of roughly 450 rows. While the computational complexity of adding additional candidates depends on the computational complexity of prediction for a given candidate, the runtime to add n candidates can be reasonably assumed to scale roughly linearly with n. (That is, we assume that a “randomly chosen” statistical model will take just as long to compute predictions as the next.)\nOn the other hand, in theory, the time to fit an elastic net model scales with the square of the number of predictors (Zou, 2005). However, the implementation of cyclical coordinate descent in fitting the elastic net meta-learner with {glmnet} substantially impacts the computational complexity of this operation (Friedman, 2010). The figure below shows the reality of how {glmnet} fit times scale with the number of predictors. (The source code for generating this figure was originally in the thesis appendix, and will be posted when the full thesis is available.)\n\n\n\nFigure 1: A plot showing the time to fit an elastic net model in seconds, averaged across 30 simulations, as a function of the number of rows and predictors. While the relationship is surely not perflectly linear, it is near so; in most applied use cases, the time to fit the elastic net meta-learner can be reasonably estimated with a linear approximation on the number of predictors.\n\n\nWhile the runtime of the elastic net fitting operation does not scale perfectly linearly with the number of predictors, in most applied model stacking settings, a linearity assumption is appropriate for estimating the time to fit such a model. The “predictors,” in the case of fitting the meta-learner, are the candidate model outputs. Thus, while a 16.9% increase seems negligible in the context of this example, that proportion roughly holds for significantly more computationally intensive ensembling use cases. For many applied use cases of model ensembling (i.e. on the scale of 10-100 candidate members being evaluated for training on much larger datasets than in this example), the need to redundantly add candidates each time a practitioner would like to refine her grid search in fitting the meta-learner becomes more cumbersome.\nGiven this, then, {stacks}’ syntax ought to delineate between the steps of adding candidate models and blending their predictions.\n\n\nDo you believe in magic?\nAs a result of the above considerations, we’ll instead consider a syntax with three distinct steps:\n\nInitialize a stack and add candidate members with prepare_candidates\nFit the meta-learner on those candidates’ predictions to generate stacking coefficients with blend_predictions\nFit the candidates with non-zero stacking coefficients on the full training set with fit_members\n\nWorking from the current syntax, such an approach might look something like this:\n# initializes a data stack, adds all candidate members,\n# and fits the meta-learner on their predictions\nprepare_candidates &lt;- function(...) {\n  stacks() %&gt;%\n    purrr::reduce(\n      ...,\n      add_candidates\n    )\n}\n\n# the current blend_predictions function is left as is\nblend_predictions &lt;- blend_predictions\n\n# the current fit_members function is left as is\nfit_members &lt;- fit_members\nWith the same model1 and model2 model definitions, the syntax would present as such.\n# add model1 and model2 candidates\nprepare_candidates(\n  model1,\n  model2\n) %&gt;%\n  # fit the meta-learner\n  blend_predictions() %&gt;%\n  # after evaluating relevant computational considerations,\n  # fit all members with non-zero stacking coefficients\n  fit_members()\nTo recognize the problem(s) that such a syntax leaves unaddressed, we must look to the intersection of tidy design principles and data pedagogy.\nIn many ways, the approach to add candidate members implemented in prepare_candidates presents like a “vectorized” alternative to add_candidates. In communities of statistical computing, vectorization is often used colloquially to communicate that a function is able to iteratively perform computations on an input if that input has length greater than one. More exactly, a function \\(f\\) is vectorized over a vector input \\(x\\) if and only if \\(g_j(f(x)) = f(g_j(x))~\\forall~j\\), where \\(g_j\\) takes in some subsettable object \\(x\\) and outputs the \\(j\\)-th element. It follows, then, that \\(f\\) may be vectorized over \\(x\\) if \\(g_j\\) is well-defined for all \\(~j~\\in~ \\{1,~2,~...,~\\max(\\vert x \\vert,~ \\vert f(x)\\vert)\\}\\), where \\(\\vert \\vert\\) denotes the number of subsettable elements indexed by \\(j\\).\nIn R, if \\(x\\) is an S3 object, \\(g_j\\) can be written as g_j &lt;- function(x) {x[[j]]}. For context, I note some common objects subsettable with this syntax and the results of subsetting them.\n\ndata.frame: Subsetting the jth element of a data frame, or its subclasses, returns the jth column of the data frame. For this section, tune_results objects and data_stacks are notable subclasses of data frames. tune_results objects encapsulate model definitions and do not have a number of columns sensitive to the number of candidate members specified therein. data_stack objects collate validation set predictions from each candidate member and have a number of columns one greater than the number of candidates in the regression setting, or one greater than the number of candidates \\(\\times\\) number of possible outcomes in the classification setting.\nlist: Subsetting the jth element of a list returns the jth element of the list, which can be an R object with any number of dimensions or class structures. Note that data.frames are a list subclass, though are usually not colloquially referred to as such.\natomic vector: Subsetting the jth element of an atomic vector returns the jth element of the vector, which must be an atomic element. Note that (non-atomic) lists are vectors, though are usually not colloquially referred to as such.\n\nAddition to one is an example of a vectorized operation.\n# construct a subsettable vector x\nx &lt;- c(1, 2, 3)\n\n# define a function f\nadd_one &lt;- function(x) {1 + x}\n\n# check the definition for j = 3\nadd_one(x[[3]]) == add_one(x)[[3]]\n## [1] TRUE\nIn the case of prepare_candidates, then, consider a vector x of two model definitions. a and b are abstracted representations of model definitions specifying one and two candidate members, respectively.\na &lt;- \"a model definition specifying one candidate member\"\nb &lt;- \"a model definition specifying two candidate members\"\n\nx &lt;- c(a, b)\n\nx\n## [1] \"a model definition specifying one candidate member\" \n## [2] \"a model definition specifying two candidate members\"\nThus, \\(g_j\\) is well-defined only for \\(j \\in \\{1,~2\\}\\) where \\(g_1(x) = a\\) and \\(g_2(x) = b\\). Note, though, that we can only subset \\(x\\) to extract model definitions—candidate members themselves are not subsettable elements of a or b.\nThis example may present as overly-simplistic, but encapsulates the principal issue with proposing that add_candidates may be vectorizable over model definitions. Regardless of how many candidate members a model definition specifies, its fundamental “elements” (i.e. the results of subsetting the tune_results object) are the same. That is, the length of an tune_results model definition object is not determined by the number of candidate members. In this way, there is no way to “subset out” a specific candidate member from neither a nor b.\nIf the output of prepare_candidates was also subsettable by model definition, this may not be an issue. However, the output of any call to prepare_candidates is a data_stack object with a number of columns one greater than the number of candidates specified in ..., where the first column always contains the true validation set outcomes from the shared resamples used to define each model definition. The “elements” of this object, then, are the candidate members in question. Model definitions may specify any number of candidate members.\nMore concretely, let \\(f\\) be prepare_candidates and \\(g_j\\) be defined by g_j &lt;- function(x) {x[[j]]}. Then \\(f(a)\\) is a data frame with two subsettable columns, where \\(g_1(f(a))\\) returns the true validation set outcomes and \\(g_2(f(a))\\) returns the validation set predictions for the candidate member specified by a. Similarly, \\(g_1(f(b))\\) returns the true validation set outcomes, and \\(g_2(f(b))\\) and \\(g_3(f(b))\\) return the validation set predictions for the first and second candidate members specified by b, respectively. Finally, \\(g_1(f(x))\\) again returns the true validation set outcomes, \\(g_2(f(x))\\) returns the validation set predictions for the candidate member specified by a, and \\(g_3(f(x))\\) and \\(g_4(f(x))\\) return the validation set predictions for the first and second candidate members specified by b, respectively.\nThe prepare_candidates approach, then, is only vectorized in the colloquial sense of the word. I argue, further, that not only is this pseudo-vectorization ultimately not helpful for user experience, but actively detracts from a practitioner’s ability to add candidates in context.\nTo demonstrate, let’s do something wrong. In addition to the k-nearest neighbors (knn_res), ordinary least squares (lin_reg_res), and support vector machine (svm_res) models from earlier, let’s introduce a neural network model definition as nn_res, and suppose that it was somehow ill-specified for inclusion in an ensemble in its construction.\nBoth the syntax in question and the current syntax would fail in attempting to add candidates.\n# the syntax in question\nprepare_candidates(\n  knn_res,\n  lin_reg_res,\n  nn_res,\n  svm_res\n)\n\n# the current syntax\nstacks() %&gt;%\n  add_candidates(knn_res) %&gt;%\n  add_candidates(lin_reg_res) %&gt;%\n  add_candidates(nn_res) %&gt;%\n  add_candidates(svm_res)\n{stacks} may or may not handle the error introduced while adding the neural network candidates gracefully. While it would be somewhat more painful to construct helpful errors from the developer’s perspective in a prepare_candidates-style implementation, this is not my point. Suppose that the clarity of the error—and {stacks}’ gracefulness in handling it—is the same between these two syntax options. What debugging strategies do these syntaxes accommodate?\nA notable strategy, and one which is common among many practitioners using tidy package ecosystems, owes itself to the composability design principle of tidy packages. In the current syntax, since discrete operations are separated into discrete calls, a practitioner may iteratively comment out a line of code (and its associated pipe), rerun the code, and check for an error until she has identified the source of the error.\n# if an error arose somewhere in this pipeline...\nstacks() %&gt;%\n  add_candidates(knn_res) %&gt;%\n  add_candidates(lin_reg_res) %&gt;%\n  add_candidates(nn_res) %&gt;%\n  add_candidates(svm_res)\n\n# ... one could identify the source by commenting out\n# calls to add_candidates in succession, like so\nstacks() %&gt;%\n  add_candidates(knn_res) %&gt;%\n  add_candidates(lin_reg_res) %&gt;%\n  add_candidates(nn_res) # %&gt;%\n  # add_candidates(svm_res)\n\nstacks() %&gt;%\n  add_candidates(knn_res) %&gt;%\n  add_candidates(lin_reg_res) # %&gt;%\n  # add_candidates(nn_res) %&gt;%\n  # add_candidates(svm_res)\nUpon the realization that the last iteration of the pipeline runs correctly, the practitioner now knows that the nn_res model definition is the source of the relevant error, and can troubleshoot further.\nThis debugging strategy is common enough among users of tidy ecosystems that several web extensions and RStudio Add-ins have been developed to accommodate and improve the effectiveness of the strategy, such as Miles McBain’s {breakerofchains} or Nischal Shrestha’s {DataTutor}. Given that the generation of inputs to {stacks} functions requires intimate familiarity with both the {tidyverse} and {tidymodels} ecosystems, accommodating this strategy is surely helpful.\n\n\nNow that the magic has gone\nGiven the above discussion, then, we arrive at a syntax that is not far from that ultimately implemented in the package. Transitioning add_candidates from its pseudo-vectorized alternative, we arrive at the following syntax.\n# an alternative formulation of add_candidates that can \n# initialize a data stack if one is not already present, \n# and otherwise performs the functionality of add_candidates\nform_candidates &lt;- function(data_stack = NULL, ...) {\n  if (is.null(data_stack)) {\n    data_stack &lt;- stacks()\n  }\n  \n  data_stack %&gt;%\n    add_candidates(...)\n}\n\n# the current blend_predictions function is left as is\nblend_predictions &lt;- blend_predictions\n\n# the current fit_members function is left as is\nfit_members &lt;- fit_members\nThis syntax is nearly identical to that utilized in the package, with the exception of the coupling of the stacks initialization and add_candidates steps. I refer to this coupling as form_candidates, where the function can initialize a data stack if one is not supplied, and otherwise works in the same way that add_candidates does.\nWith the same model1 and model2 model definitions, the syntax would present as such.\n# add model1 and model2 candidates\nform_candidates(model1) %&gt;%\n  form_candidates(model2)\n  # fit the meta-learner\n  blend_predictions() %&gt;%\n  # after evaluating relevant computational considerations,\n  # fit all members with non-zero stacking coefficients\n  fit_members()\nA ready rebuttal one might have in reaction to my proposal that this syntax is not appropriate is “you chose a bad name for form_candidates!” This argument is correct, and leads to my reasoning well. To name a function, one must articulate what it does. What does form_candidates—or a function by a different name that does the same thing—do? Especially given the argument that add_candidates cannot be properly vectorized, the action that form_candidates performs is dependent on the form of its inputs. I don’t mean this technically, for there are formal mechanisms (that is, object orientation) to accommodate operations whose exact implementation must be sensitive to the form of its input. Rather, colloquially, form_candidates performs two distinct operations on two different types of things, and there are not verbs that can describe the action being carried out insensitively to the form of the input.\n\nIf provided nothing as its first argument, form_candidates creates a data stack and then appends the second argument to the data stack.\nIf provided a data stack as its first argument, form_candidates appends the second argument to the data stack provided.\n\nWhat word can describe creation from nothing and addition to something simultaneously? For this reason, I argue that the form_candidates approach is an overextension of object orientation.\nInstead, then, the package ought to delineate between these steps of creating and appending, assigning more appropriate and evocative names to each operation. This first step involves the creation of a minimal instance of a data_stack which can be later appended to and manipulated by other functions in the package. Luckily, users of the {stacks} package have a mental map, formed while learning the APIs of many of {stacks}’ dependencies and otherwise related packages, from which they can draw to understand the appearance and role of such a function. In numerous packages in the {tidyverse}, {tidymodels}, and adjacent ecosystems, a function by the same (or similar) name of the package returns a minimal instance of an object class defined in the package, serving as a starting point for code that will ultimately add to or modify that object. Notable examples include {ggplot2} and {tibble} from the tidyverse, {workflows} and {recipes} from the {tidymodels}, and {data.table} and {gt} elsewhere.\n\n\nThe real magic was the friends we made along the way\nAnd thus, here we are; we’ve landed at the syntax that we ultimately implemented in the package. Maybe you saw that coming. To construct an ensemble using our old friends model1 and model2, we can use the following code.\nstacks() %&gt;%\n  add_candidates(model1) %&gt;%\n  add_candidates(model2) %&gt;%\n  blend_predictions() %&gt;%\n  fit_members()\nWhile this syntax is surely less concise than those considered so far, it offers a number of advantages over the one-function stack_models approach:\n\nThe syntax offers moments for pause to a practitioner who would like to evaluate her choice of grid search specification. She need not redundantly perform the steps of adding candidates to a data stack nor fitting candidates on the full training set, both of which are computationally intensive enough to warrant careful consideration.\nThe syntax roughly reflects the way these operations would be explained colloquially, and thus owes itself to function and argument names that are coherent and comprehensive. It is for this reason that we did not consider more verbose syntaxes.\nThe syntax takes seriously the technical and pedagogical implications of an implementation that presents as vectorized but ultimately promotes a misunderstanding of the form of its inputs.\n\nThe real super auto AI machine learning magic truly was the friends we made along the way. Here’s to more keystrokes.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2021-05-12-dev-docs-p3/index.html",
    "href": "blog/2021-05-12-dev-docs-p3/index.html",
    "title": "Naming the Things (Developer Documentation pt. 3)",
    "section": "",
    "text": "This is the third of four blog posts on the development process of the {stacks} package, excerpted from my Reed senior thesis project Tidy Model Stacking with R.\nPart 1: Introduction\nPart 2: Splitting Things Up\nPart 4: Big Things\n\nOver the course of development of {stacks}, we considered many “trios” of names for the core functions.\nThe core functions in the trio perform the following operations, respectively:\n\nAdd candidate member models to a data stack, proposing them for inclusion in an ensemble model. This function was ultimately named add_candidates().\nFit a “meta-learner” that combines the predictions from each of the candidates members in the data stack. This function was ultimately named blend_predictions().\nFit each of the candidate members with nonzero stacking coefficients (i.e. coefficients from the meta-learner, one for each candidate member) on the whole training set. This function was ultimately named fit_members().\n\nTo learn more about how each of these functions are used in practice, check out the Getting Started vignette.\nWe felt relatively sure that each trio ought to share some sort of basic grammatical structure. For example, each function name could be composed of:\n\nonly a verb, e.g. add, blend, & fit\na verb and a direct object, e.g. add_candidates, blend_predictions, & fit_members\na prefix and a verb, e.g. stack_add, stack_blend, & stack_fit\nall three, e.g. stack_add_candidates, stack_blend_predictions, & stack_fit_members\n\nThe verbs and direct objects in question and the meanings they imply, as well, left us with many options to consider. (The name of the package itself going without saying.)\nI argue, though, that this question is not only an aesthetic one, but one whose answer implicates a number of technical challenges and implications for data pedagogy. To demonstrate, I will consider a few of the later iterations of these sets of function names. I refer to these sets as “grammars” rather than “syntaxes” (as in the previous blog post) in order to accentuate that I am no longer considering the coarseness with which the requisite operations for fitting an ensemble model are grouped into functions—there will always be three distinct functions in this section—but the names given to each function.\n\nadd, blend, & fit\nFor users of IDEs without auto-complete features (and non-IDE users), there’s much to say for a grammar as concise as add, blend, and fit.\nIn RStudio, as well as many other popular IDEs, the full names of possible functions, methods, and other objects in the environment will be suggested at the tooltip after typing three or more letters. The extent to which such a feature is helpful, primarily, is a function of how many more keystrokes a function name will require than three.\nFor R programmers who do not use such an IDE, though, minimizing the number of keystrokes necessary to write code using a given package is surely a helpful thing to keep in mind as a package developer. The number of {stacks} users who do not develop in RStudio may seem small, especially given that {stacks} is situated in the RStudio-funded {tidymodels} package ecosystem. However, I contend that {stacks} ought to be sensitive to the needs of these users, for several reasons. Namely,\n\nAt the time of writing, a significant number of RStudio features are inaccessible for blind developers and data scientists. As such, many blind R developers are not RStudio users, making use of a wide array of alternative development environments. As Jooyoung Seo noted in his rstudio::global(2021) talk, “Data science requires insight, not sight” (Seo, 2021). A positive user experience with {stacks} ought not to require sight as well.\nRStudio-funded open source package developers ought to think carefully about the tension between their obligation to RStudio as a company and the spirit of open source. Is an R software package truly “free and open source” if a positive user experience is contingent upon the usage of a closed-source IDE built by a for-profit company? In reality, there are all sorts of complications to implying such a tension, and many potential conflicts of interest that one might deduce would follow from this relationship do not hold true in practice. (Said more plainly, it’s my experience that RStudio genuinely cares about supporting free and open source software users and developers.) That said, I generally feel R package design can anticipate the addition of RStudio Addins (user-contributed extensions to the IDE) for the improvement of user experience, but ought not admit itself to a poor user experience outside of RStudio due only to inconsiderate API design.\n\nIn any case, the inputs to {stacks} functions are the culmination of often hundreds of lines of code. {stacks} is situated in a software ecosystem (the tidymodels) that prioritizes intentionality and decision-making over code brevity. Writing code in such an ecosystem thus relies heavily on IDE tools such as auto-complete (as well as the homegrown {usemodels} tool for generating boilerplate code) to write code. The result is code that is, from one perspective, expressive and principled, and from the other, long. As such, the gains in truncating function names to as few characters as is proposed here might be more easily offset by the challenges such a grammar implies.\nIndeed, this grammar introduces several technical challenges. For one, such a grammar does little to clarify the distinction between the blend and fit steps. Fundamentally, both of these steps involve fitting a statistical model—the former step fits the meta-learner model that will combine predictions from each member, while the latter fits the member models whose predictions will be combined by the meta-learner.\nThus, if only one of these methods were to be named fit, the distinction between which of these steps ought to be fit or blend would be arbitrary and likely misleading.\nFurther, add and fit (and, arguably, any verb with five or fewer characters) are both function names that are useful in numerous data analysis and modeling settings beyond model stacking. Thus, functions with these names are exported in numerous widely used packages, in some cases introducing namespace conflicts. Namespace conflicts arise when two or more packages supply a function—say, add—of the same name, such that the behavior of calling add without explicitly specifying the namespace (i.e. stacks::add) ultimately depends on which package has been loaded more recently. In package development, namespace conflicts ought to be avoided as much as possible.\nHowever, while the behavior of a function likely ought not to depend on the order of recently loaded packages, it could very well be useful for the behavior of a function to depend on the structure of what is passed to it. This is the premise of object-orientated programming, for which the most mainstream implementation in R software is the S3 system.\nAs an example, if I pass a data_stack object to a function called fit, the most sensical thing to do would probably be to fit the meta-learner that ultimately will inform how to best combine the predictions contained in the data stack. However, this is probably not also the case for a model_stack object. In the case of a model stack, the meta-learner has already been fitted, so a fit function applied to a model_stack probably ought to fit the member models whose predictions will be combined by the meta-learner.\nTo implement this kind of framework, one could define a generic function, fit, which just looks at the class of what’s passed to it, and then dispatches to the correct method, or implementation of fit as applied to a specific object class. In the case of fit, specifically, the {generics} package supplies a number of helpful function generics, including fit. Thus, to implement this kind of framework in {stacks}, the package would re-export the {generics} package’s fit generic function and then implement fit methods for the data_stack and model_stack classes. Such an implementation would result in {stacks} pipelines appearing something like the following.\nstacks() %&gt;%\n  add(candidate1) %&gt;%\n  add(candidate2) %&gt;%\n  fit() %&gt;%\n  fit()\nUnderstanding this code requires a strong understanding of S3 object-orientation, as well its application within the package specifically, in order for a practitioner to know how many times to she ought to call fit and which operations were happening with each call. This need not be a requisite for fitting ensemble models in R. Even then, though, this grammar results in the juxtaposition of an iterative add method, which does not alter the object class, and two fit methods, one of which alters the object class and neither of which can be called iteratively.\nOne could use two non-fit verbs for these two steps, but our creativity failed us in coming up with appropriate names given that approach.\n\n\n\nadd_candidates, blend_predictions, & fit_members\nWhile working with a grammar like add_candidates, blend_predictions, & fit_members is surely made easier by an auto-complete feature, it only takes advantage of one of two notable benefits to auto-complete. If a practitioner remembers the name of the function she is planning on using, auto-complete is very much helpful here; after typing the first three characters, the large majority of keystrokes can then be eliminated through the use of the tooltip. However, this grammar does not make effective use of the tooltip in the case that the practitioner does not remember the name of the function she intends to use. Grammars making use of prefixing, where functions with shared functionality are prefixed with the same three (or more) characters, only ask that a practitioner can recall that one prefix in order to see the possible options suggested. In addition to saving keystrokes, then, such grammars can uniquely provide access to short-form, “fly-by” documentation—the grammars discussed in the following two subsections are such examples.\nThis grammar also introduces a somewhat strange interface in regard to the first argument supplied by the practitioner. Per tidyverse design principles, core functions meant to be called in succession ought to be “pipable”—that is, the first argument (supplied via the {magrittr} pipe %&gt;% operator) to each function ought to be the output of the function called previously. The grammar in consideration presents as such:\nstacks() %&gt;%\n  add_candidates(...) %&gt;%\n  # ... %&gt;%\n  blend_predictions() %&gt;%\n  fit_members()\nAligning with this convention results in inconsistent relationships between the direct object in the function name and the argument to which that direct object refers. Walking through the initialization function and three core verbs, consider:\n\nThe stacks initialization function does not take arguments.\nThe first input to add_candidates must then be the output of stacks, which is a data_stack object. Since add_candidates is called iteratively, the output of add_candidates can also be inputted to add_candidates. Thus, rather than the candidates, the first argument to add_candidates is the object to which the candidates are added, followed by the candidates themselves.\nThe input to blend_predictions is the output of add_candidates, which is also a data_stack object. The function does not require any other arguments. Thus, in this case, the direct object (“predictions”) indeed refers to the data stack, which is passed through the pipe operator. Thus, the direct object in this function name refers to a different argument, by position. Further, to be internally consistent with add_candidates, the first argument is called data_stack rather than predictions, reflecting the internally defined class of the object supplied as the first argument.\nThe input to fit_members is the output of blend_predictions, which is a model_stack object. The function does not accept any other arguments. Thus, in this case, the members referred to in the function name are an element of the first argument, named model_stack, but not the first argument itself. Thus, the direct object in this function name technically does not refer to an argument at all.\n\nOne alternative to this set of direct objects would be to refer to the object class (reflected in the argument names) of the inputs, i.e. add_data_stack, blend_data_stack, fit_model_stack. This approach implicates a few challenges, though.\n\nThe noted dissonance in argument order—notably, the data stack is not the thing being added—is more immediate.\nSuffixing the name of a function with an object class results in a function name that is confusingly similar to that which would result from defining an S3 method that would presumably serve the same purpose. For example, a fit method for a model_stack object would be written as fit.model_stack.\nThis grammar could be confusing for a practitioner who does not yet know the underlying object classes (data_stack and model_stack) and the abstract object types they represent (data stacks and model stacks). Notably, it is unclear whether _stack is itself an independent suffix, possibly referring to a noun, verb, or some abstract package identifier. Does add_data_stack refer to adding data to a stack, adding a data stack, stacking data to add to something, or adding data to something in the context of {stacks}? Similar questions could be asked of the two fitting functions.\n\nThe contradictions of this sort of grammar may not even occur to some practitioners. “Not that deep,” if you will. For others (including the authors), this grammar has been difficult.\n\n\nstack_add, stack_blend, & stack_fit\nA grammar composed of stack_add, stack_blend, & stack_fit is subject to many of the pitfalls noted for add, blend, & fit. Notably, this grammar does not contribute to distinguishing a necessarily arbitrary delineation between two steps to fitting an ensemble model that are, in essence, model fitting steps.\nHowever, this grammar makes use of both of the potential benefits of auto-complete functionality noted in the above section; not only does the practitioner benefit from auto-complete through a decrease in the number of keystrokes, but she also need only remember one prefix in order to quickly see the names of all functions matching that prefix.\nTo demonstrate, consider a package {beepbopboop} that implements three operations that can be carried out on two scalars: beep, bop, and boop. The figure below represents the effect of common prefixing and suffixing on the user experience of {beepbopboop} while working in RStudio.\n\n\n\nFigure 1: Beginning function and method names with a common prefix allows for usage of auto-complete and the tooltip not only to decrease keystrokes, but also to jog the practitioners’ memory regarding each of the possible operations that can be carried out with a package.\n\n\nIn this more general example, too, we must consider that there are many operations which can be carried out on two scalars. Need a practitioner remember that the beep, bop, and boop methods come from the beepbopboop package specifically? If so, this sort of prefixing/suffixing could be helpful in clarifying the source of the beep, bop, and boop methods. If not, the introduction of package-specific prefixes or suffixes is unnecessarily verbose.\nIn {stacks}, though, that prefixing comes at a cost. The prefix likely ought to be some iteration of “stack” in order to most memorable in combination with the name of the package. However, placing “stack” at the beginning of function names owes itself to misreading “stack” as a verb. Even then, “stack” is hypersaturated with meanings in the package already—between the name of the package itself, the initialization function, the object classes, and its colloquial usage in “model stacking” and “stacked ensemble learning,” assigning another specific meaning to the word would be ill-considered. This usage also fails to differentiate between model and data stacks, obfuscating the change in object classes after fitting the meta-learner.\nAs for stack_add_candidates and friends, while this grammar lends itself to both of the benefits of auto-complete mentioned for the current names, it is vulnerable to the same pitfalls of those discussed in the current and prefixed names. For users in IDEs without auto-complete and non-IDE users, this grammar is quite cumbersome.\n\n\nWhere we landed\nIn short, the following considerations were relevant to ultimately determining the names of the core functions in {stacks}.\n\nHow evocative are the function names of the tasks being performed by the functions?\nDoes the grammar make use of auto-complete to improve access to documentation?\nFor practitioners in IDEs without auto-complete and non-IDE practitioners, how cumbersome is the grammar to type?\nDoes the grammar introduce namespace conflicts within relevant package ecosystems? If one or more functions must be S3 methods, are their meanings compatible with the common interpretation of the function?\nIs the grammar internally consistent in the manner in which it references function arguments?\nIf the grammar includes some iteration or abbreviation of “stack,” what part of speech might it be interpreted as? What meanings of the word elsewhere in the package might it obfuscate?\n\nOnly some of these questions are relevant to/well-defined for each of the four grammars considered above. Further, none of the grammars in question can provide each of the advantages described above. The table below outlines some of the considerations mentioned in this section.\n\n\n\n\n\n\n\n\nGrammar =======================================================: add, blend, & fit\nPros ====================================================: - Concise\nCons | ==========================================================:+ - Does not make use of auto-complete/tooltip- Namespace conflicts likely- Obfuscates two distinct model fitting steps |\n\n\n\n\nadd_candidates, blend_predictions, & fit_members\n\nMakes use of auto-complete- Delineates between distinct model fitting steps | - Does not make use of tooltip- Necessarily inconsistent in its grammatical structure\n\n\n\nstack_add, stack_blend, & stack_fit\n\nMakes use of auto-complete and tooltip\n\n\nFurther overloads “stack” terminology- Obfuscates two distinct model fitting steps |\n\n\n\n\nThe grammar that we landed on is largely an artifact of the ecosystem in which the package is situated, surely with some elements of arbitrary personal preference and random chance.\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2025-04-01-gemini-2-5-pro/index.html",
    "href": "blog/2025-04-01-gemini-2-5-pro/index.html",
    "title": "How Good Is Gemini 2.5 Pro at Writing R Code?",
    "section": "",
    "text": "Since Gemini 2.5 Pro Experimental’s release last week, I’ve been seeing a lot of hype claiming that the model is the new state of the art. I’ve been wondering—how good is this model at writing R code?\nAs it happens, I’ve been working on a new tool for large language model evaluation in R. The package is still pretty early on in it’s development, but I figured this might be a good opportunity to kick the tires publicly."
  },
  {
    "objectID": "blog/2025-04-01-gemini-2-5-pro/index.html#introducing-rinspect",
    "href": "blog/2025-04-01-gemini-2-5-pro/index.html#introducing-rinspect",
    "title": "How Good Is Gemini 2.5 Pro at Writing R Code?",
    "section": "Introducing rinspect",
    "text": "Introducing rinspect\nrinspect is an R port of the widely adopted Python framework Inspect. While the package doesn’t integrate with Inspect directly, it allows users to interface with the Inspect log viewer and shares much of its grammar and philosophy.\nrinspect describes LLM evals in three core components:\n\n\nDatasets contain a set of labelled samples. Datasets are just a tibble with columns input and target, where input is a prompt and target is either literal value(s) or grading guidance.\n\nSolvers evaluate the input in the dataset and produce a final result (hopefully) approximating target. In rinspect, the simplest solver is just an ellmer chat (e.g. ellmer::chat_claude()) wrapped in generate(), i.e. generate(ellmer::chat_claude())), which will call the Chat object’s $chat() method and return whatever it returns.\n\nScorers evaluate the final output of solvers. They may use text comparisons, model grading, or other custom schemes to determine how well the solver approximated the target based on the input.\n\nIn this blog post, we’ll apply a solver powered by four different models to a dataset of R coding problems. Google’s Gemini 2.5 Pro Experimental will be the first. We’ll compare that model to its previous generation, Gemini 2.0 Flash. Otherwise, it’s a bit difficult to determine reasonable “peers” to the model without knowing what the model’s API cost will be, but we could use OpenAI’s o3-mini-high and Anthropic’s Claude Sonnet 3.7 Extended Thinking as peer models since Google used them in the linked release post. We’ll then use the same scorer to evaluate how well each model performed.\n\n\n\n\n\n\nImportant\n\n\n\nI was unable to get Claude Sonnet 3.7’s “thinking” enabled properly with ellmer; an apples-to-apples comparison would use that setting.\n\n\nIn ellmer, here’s how we define those model connections:\n\nlibrary(ellmer)\n\ngemini_2_5_pro &lt;- chat_gemini(model = \"gemini-2.5-pro-exp-03-25\")\ngemini_2_0_flash &lt;- chat_gemini(model = \"gemini-2.0-flash\")\no3_mini_high &lt;- chat_openai(\n  model = \"o3-mini\", \n  api_args = list(reasoning_effort = \"high\")\n)\nclaude_sonnet_3_7 &lt;- chat_claude(model = \"claude-3-7-sonnet-latest\")\n\nNote that I needed to configure a GOOGLE_API_KEY, OPENAI_API_KEY, and ANTHROPIC_API_KEY to connect to these models, respectively. Gemini 2.5 Pro Experimental is currently free but heavily rate-limited. Here are the prices per million tokens for the other models1:\n\n\n# A tibble: 3 × 3\n  Name              Input Output\n  &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; \n1 Gemini 2.0 Flash  $0.10 $0.40 \n2 ChatGPT o3-mini   $1.10 $4.40 \n3 Claude 3.7 Sonnet $3.00 $15.00"
  },
  {
    "objectID": "blog/2025-04-01-gemini-2-5-pro/index.html#an-r-eval-dataset",
    "href": "blog/2025-04-01-gemini-2-5-pro/index.html#an-r-eval-dataset",
    "title": "How Good Is Gemini 2.5 Pro at Writing R Code?",
    "section": "An R Eval dataset",
    "text": "An R Eval dataset\n\nlibrary(rinspect)\nlibrary(tidyverse)\n\nWe’ll use a dataset that ships with rinspect called are, or “An R Eval.” From the are docs:\n\nAn R Eval is a dataset of challenging R coding problems. Each input is a question about R code which could be solved on first-read only by human experts and, with a chance to read documentation and run some code, by fluent data scientists. Solutions are in target and enable a fluent data scientist to evaluate whether the solution deserves full, partial, or no credit.\n\n\nglimpse(are)\n\nRows: 28\nColumns: 7\n$ title     &lt;chr&gt; \"after-stat-bar-heights\", \"conditional-grouped-sum…\n$ input     &lt;chr&gt; \"This bar chart shows the count of different cuts …\n$ target    &lt;chr&gt; \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n…\n$ domain    &lt;chr&gt; \"Data analysis\", \"Data analysis\", \"Data analysis\",…\n$ task      &lt;chr&gt; \"New code\", \"New code\", \"New code\", \"Debugging\", \"…\n$ source    &lt;chr&gt; \"https://jrnold.github.io/r4ds-exercise-solutions/…\n$ knowledge &lt;list&gt; \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"…\n\n\nAt a high level:\n\n\ntitle: A unique identifier for the problem.\n\ninput: The question to be answered.\n\ntarget: The solution, often with a description of notable features of a correct solution.\n\ndomain, task, and knowledge are pieces of metadata describing the kind of R coding challenge.\n\nsource: Where the problem came from, as a URL. Many of these coding problems are adapted “from the wild” and include the kinds of context usually available to those answering questions.\n\nFor the purposes of actually carrying out the initial evaluation, we’re specifically interested in the input and target columns. Let’s print out the first entry in full so you can get a taste of a typical problem in this dataset:\n\ncat(are$input[1])\n\nThis bar chart shows the count of different cuts of diamonds,\nand each bar is stacked and filled according to clarity:\n\n```\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity))\n```\n\nCould you change this code so that the proportion of diamonds\nwith a given cut corresponds to the bar height and not the\ncount? Each bar should still be filled according to clarity.\n\n\nHere’s the suggested solution:\n\ncat(are$target[1])\n\nPreferably:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = after_stat(count) /\nsum(after_stat(count)), fill = clarity))\n```\n\nThe dot-dot notation (`..count..`) was deprecated in ggplot2\n3.4.0, but it still works:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill =\nclarity))\n```\n\nSimply setting `position = \"fill\" will result in each bar\nhaving a height of 1 and is not correct.\n\n\nFor now, are was publicly shared after the knowledge cutoff of each of these models, so the answers to these questions (likely) aren’t yet incorporated into the models’ weights."
  },
  {
    "objectID": "blog/2025-04-01-gemini-2-5-pro/index.html#evaluating-gemini",
    "href": "blog/2025-04-01-gemini-2-5-pro/index.html#evaluating-gemini",
    "title": "How Good Is Gemini 2.5 Pro at Writing R Code?",
    "section": "Evaluating Gemini",
    "text": "Evaluating Gemini\nLLM evaluation with rinspect happens in two main steps:\nFirst, use Task$new() to situate a dataset, solver, and scorer in a Task. Tasks are R6 objects that define important methods and data structures for LLM evaluation. Below, I use generate() as a solver, currently the only built-in solver supplied by the package. Think of it like Chat objects’ $chat() method with some bells and whistles—parallel requests, some nice progress functionality, and lots of logging. generate() returns a function that has one argument, solver_chat, which takes an ellmer Chat; you can set a default Chat by supplying it to generate() itself. The scorer, model_graded_qa(), uses model grading (or “LLM-as-a-judge”) to score the solver’s responses. Different models exhibit different behaviors as judges, so I use the same model (Claude Sonnet 3.7) as the judge regardless of which model is being evaluated.\n\nare_task &lt;- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  name = \"An R Eval\"\n)\n\nare_task\n\nAn evaluation task An R Eval.\n\n\nSecond, use Task$eval() to evaluate the solver, evaluate the scorer, and then explore a persistent log of the results in an interactive viewer.\n\nare_task_gemini_2_5 &lt;- are_task$clone()\nare_task_gemini_2_5$eval(\n  solver_chat = gemini_2_5_pro,\n  epochs = 3\n)\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also run $eval() step-by-step, if you want; $eval() just calls $solve(), $score(), $log(), and $view() in sequence.\n\n\nAfter evaluation, the task contains information from the solving and scoring steps. Here’s what the model responded to that first question with:\n\ncat(are_task_gemini_2_5$samples$result[1])\n\nOkay, you can achieve this by changing the `position` argument\nwithin `geom_bar`. The default is `\"stack\"`, which stacks the\nraw counts. You want `\"fill\"`, which stacks the proportions so\nthat each bar sums to 1 (or 100%).\n\nHere's the modified code:\n\n```R\n# Load the ggplot2 library if you haven't already\n# library(ggplot2)\n# The diamonds dataset is built into ggplot2\n\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity), position =\n\"fill\") +\nlabs(y = \"Proportion\", title = \"Proportion of Clarity Levels\nwithin each Diamond Cut\") # Optional: Add informative labels\n```\n\n**Explanation:**\n\n1.  `ggplot(data = diamonds)`: Initializes the plot using the\n`diamonds` dataset.\n2.  `geom_bar(...)`: Specifies that you want a bar chart.\n3.  `mapping = aes(x = cut, fill = clarity)`: Maps the `cut`\nvariable to the x-axis and the `clarity` variable to the fill\ncolor of the bars.\n4.  `position = \"fill\"`: This is the key change. It tells\n`geom_bar` to calculate the count for each `fill` category\n(clarity) within each `x` category (cut), but then normalizes\nthese counts so that the total height of each bar represents 1\n(or 100%). The segments within the bar then show the\n*proportion* of each clarity level for that specific cut.\n5.  `labs(y = \"Proportion\", ...)`: (Optional but recommended)\nChanges the default y-axis label from \"count\" to \"Proportion\"\nto accurately reflect what the axis now represents, and adds a\ntitle.\n\n\nThen, since we’ve supplied model_graded_qa() as the scorer, Claude Sonnet 3.7 will be supplied the input question, the grading guidance, and the solver’s output, and asked to determine whether the solution is incorrect, partially correct, or correct. Here’s Claude’s grading transcript for the solver’s first answer:\n\ncat(are_task_gemini_2_5$samples$scorer_chat[[1]]$last_turn()@text)\n\nI need to assess whether the submitted answer meets the\ncriterion, which specifies that the proportion of diamonds\nwith a given cut should correspond to the bar height.\n\nThe criterion clearly states that the correct approach is to\nuse:\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = after_stat(count) /\nsum(after_stat(count)), fill = clarity))\n```\n\nThe submission instead recommends using:\n```\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity), position =\n\"fill\")\n```\n\nThe criterion explicitly states that \"Simply setting `position\n= \"fill\"` will result in each bar having a height of 1 and is\nnot correct.\" This is because `position = \"fill\"` normalizes\nwithin each cut category, making each bar the same height\n(1.0), with the segments showing the proportions of clarity\nwithin each cut.\n\nIn contrast, the approach specified in the criterion uses\n`after_stat(count) / sum(after_stat(count))` to calculate the\nproportion of each cut relative to the total number of\ndiamonds. This ensures that the bar heights represent the\nproportion of diamonds with each cut, while still maintaining\nthe clarity breakdown within each bar.\n\nSince the submission provides a solution that contradicts what\nthe criterion explicitly identifies as incorrect, the answer\ndoes not meet the requirements.\n\nGRADE: I\n\n\nrinspect ships with the Inspect Log Viewer, a small .js app that allows you to interactively explore evaluation logs. Especially the first few times you run an eval, the tool is super helpful for uncovering unexpected behavior in solving and scoring. I’ve embedded the viewer in this post so you can check out the problems in An R Eval and how effectively Gemini 2.5 Pro Experimental handled them:\n\n\n\n\n\nI’d encourage you to poke around in this app! You’ll certainly see some bugs that I’ve still yet to work out and some surprising behavior from the scorer, but there’s lots to be learned about how these models work from evaluation logs."
  },
  {
    "objectID": "blog/2025-04-01-gemini-2-5-pro/index.html#evaluating-comparison-models",
    "href": "blog/2025-04-01-gemini-2-5-pro/index.html#evaluating-comparison-models",
    "title": "How Good Is Gemini 2.5 Pro at Writing R Code?",
    "section": "Evaluating Comparison Models",
    "text": "Evaluating Comparison Models\nWe can evaluate the remaining models by cloning the original task and running $eval() with a new solver chat. First, to evaluate the previous Gemini generation, Gemini 2.0 Flash:\n\nare_task_gemini_2_0 &lt;- are_task$clone()\nare_task_gemini_2_0$eval(\n  solver_chat = gemini_2_0_flash,\n  epochs = 3\n)\n\nFor o3-mini high:\n\nare_task_o3_mini_high &lt;- are_task$clone()\nare_task_o3_mini_high$eval(\n  solver_chat = o3_mini_high,\n  epochs = 3\n)\n\nFinally, for Claude Sonnet 3.7:\n\nare_task_claude_sonnet_3_7 &lt;- are_task$clone()\nare_task_claude_sonnet_3_7$eval(\n  solver_chat = claude_sonnet_3_7,\n  epochs = 3\n)\n\nI’ve also situated the logs for both of the above evaluations in the above app—just click the three stacked bars in the top right of the app to check out the logs for 2.0 Flash, o3-mini and Sonnet 3.7."
  },
  {
    "objectID": "blog/2025-04-01-gemini-2-5-pro/index.html#analysis",
    "href": "blog/2025-04-01-gemini-2-5-pro/index.html#analysis",
    "title": "How Good Is Gemini 2.5 Pro at Writing R Code?",
    "section": "Analysis",
    "text": "Analysis\nAt evaluation time, rinspect does a naive accuracy calculation that you can see displayed in the app, but in general is quite restrained in its analysis functionality. Instead, the package aims to get analysts to Happy Data Frame Land as quickly as possible using inspect_bind():\n\nare_eval &lt;- \n  inspect_bind(\n    `Gemini 2.5 Pro Experimental` = are_task_gemini_2_5,\n    `Gemini 2.0 Flash` = are_task_gemini_2_0,\n    `o3-Mini High` = are_task_o3_mini_high,\n    `Claude Sonnet 3.7 (No Thinking)` = are_task_claude_sonnet_3_7,\n  ) %&gt;%\n  rename(model = task) %&gt;%\n  mutate(\n    model = factor(model, levels = c(\n      \"Gemini 2.5 Pro Experimental\", \n      \"Gemini 2.0 Flash\", \n      \"o3-Mini High\", \n      \"Claude Sonnet 3.7 (No Thinking)\"\n    ))\n  )\n\nare_eval\n\n# A tibble: 336 × 5\n   model                          id epoch score metadata         \n   &lt;fct&gt;                       &lt;int&gt; &lt;int&gt; &lt;ord&gt; &lt;list&gt;           \n 1 Gemini 2.5 Pro Experimental     1     1 I     &lt;tibble [1 × 11]&gt;\n 2 Gemini 2.5 Pro Experimental     1     2 I     &lt;tibble [1 × 11]&gt;\n 3 Gemini 2.5 Pro Experimental     1     3 I     &lt;tibble [1 × 11]&gt;\n 4 Gemini 2.5 Pro Experimental     2     1 I     &lt;tibble [1 × 11]&gt;\n 5 Gemini 2.5 Pro Experimental     2     2 P     &lt;tibble [1 × 11]&gt;\n 6 Gemini 2.5 Pro Experimental     2     3 I     &lt;tibble [1 × 11]&gt;\n 7 Gemini 2.5 Pro Experimental     3     1 C     &lt;tibble [1 × 11]&gt;\n 8 Gemini 2.5 Pro Experimental     3     2 C     &lt;tibble [1 × 11]&gt;\n 9 Gemini 2.5 Pro Experimental     3     3 P     &lt;tibble [1 × 11]&gt;\n10 Gemini 2.5 Pro Experimental     4     1 I     &lt;tibble [1 × 11]&gt;\n# ℹ 326 more rows\n\n\nIn this dataset, each row represents a single time a solver is invoked to answer a question:\n\nmodel gives the model used to solve a given question\nid gives the question id\nepoch identifies the run/resample of the given question\nscores shows whether the scoring model (Claude Sonnet 3.7) identified the solver’s answer as Incorrect, Partially Correct, or Correct. It’s an ordinal factor with I &lt; P &lt; C.\nmetadata is a list column containing just about all of the information that rinspect collects during the evaluation process.\n\nWe’re interested in which of these three models are right more often. We have 28 unique questions, each resampled across 3 epochs for each of 4 models. For a cursory analysis, we could do the canonical Bar Chart Dodged By Model visualization:\n\nare_eval %&gt;%\n  mutate(\n    score = fct_rev(score),\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = score, fill = model)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_manual(values = c(\n    \"Gemini 2.5 Pro Experimental\" = \"#5CB85C\", \n    \"Gemini 2.0 Flash\" = \"#8BC34A\",\n    \"o3-Mini High\" = \"#69b1cf\", \n    \"Claude Sonnet 3.7 (No Thinking)\" = \"#d6941a\"\n  )) +\n  labs(\n    x = \"Score\", y = \"Count\", \n    title = \"An R Eval\",\n    subtitle = \n      \"The newest Gemini release is a substantial improvement on the previous\\ngeneration and is a contender with Claude and o3-mini on R coding problems.\"\n  ) +\n  theme(plot.subtitle = element_text(face = \"italic\"))\n\n\n\n\n\n\n\nCould the differences we’re seeing be attributed to random noise, though? We can use a hierarchical modeling technique called a mixed model to model the probability of each score (i.e., correct, etc.) as a function of the LLM. In this case, observations are not independent; some questions may be harder than others, and we’re repeating each question multiple times since we’ve set epochs = 3. A random intercept on the question id can help account for this variation. Since score is ordinal, we use a cumulative link mixed model rather than the usual suspect lme4::glmer():\n\nlibrary(ordinal)\n\nare_mod &lt;- clmm(score ~ model + (1|id), data = are_eval)\n\n\nsummary(are_mod)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  336  -257.80 527.59 213(1039) 1.59e-04 4.6e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 7.561    2.75    \nNumber of groups:  id 28 \n\nCoefficients:\n                                     Estimate Std. Error z value\nmodelGemini 2.0 Flash                -1.48455    0.38106  -3.896\nmodelo3-Mini High                    -0.29653    0.37041  -0.801\nmodelClaude Sonnet 3.7 (No Thinking) -0.04314    0.36520  -0.118\n                                     Pr(&gt;|z|)    \nmodelGemini 2.0 Flash                9.79e-05 ***\nmodelo3-Mini High                       0.423    \nmodelClaude Sonnet 3.7 (No Thinking)    0.906    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -1.4919     0.6059  -2.462\nP|C   0.2869     0.5975   0.480\n\n\nGemini 2.0 Flash performs notably worse than the reference Gemini 2.5 Pro Experimental model (-1.485, p &lt; 0.001), with an odds ratio of 0.23, indicating substantially lower odds of achieving higher rating categories. 2.5 Pro’s peers, o3-Mini High and Claude Sonnet 3.7, show largely comparable performance to 2.5 Pro. The substantial random effect variance indicates that question difficulty varies considerably and has a strong influence on performance. With threshold values of -1.49 (I|P) and 0.29 (P|C), the model suggests a larger gap between Incorrect and Partially Correct responses than between Partially Correct and Correct ones.\n\n\n\n\n\n\nNote\n\n\n\nKeep an eye out for a rinspect vignette with a more thorough model-based analysis than this one in the near future.\n\n\nBased on those results, it seems like we can make a few conclusions:\n\nGemini 2.5 Pro is certainly an improvement over Gemini 2.0 Flash.\nGemini 2.5 Pro seems “up there” with Claude Sonnet 3.7 and o3-Mini High when it comes to writing R code.\n\nAn R Eval, and the scoring functionality I associate it with, definitely needs some more work. :)\n\nAgain, Google has yet to announce API pricing for Gemini 2.5 Pro. If Gemini 2.5 Pro’s API is comparable to Gemini 2.0 Flash’s, I might agree that we have a new SoTA on our hands! In that case, I might consider switching my go-to coding assistance model from Claude more seriously.\n\nThank you to Max Kuhn for advising on the model-based analysis here, and to Hadley Wickham for suggesting that I also evaluate against the previous Gemini model. Grateful for your mentorship."
  },
  {
    "objectID": "blog/2025-04-01-gemini-2-5-pro/index.html#footnotes",
    "href": "blog/2025-04-01-gemini-2-5-pro/index.html#footnotes",
    "title": "How Good Is Gemini 2.5 Pro at Writing R Code?",
    "section": "Footnotes",
    "text": "Footnotes\n\nAltogether, the data underlying this blog post took around $3 USD to generate.↩︎"
  },
  {
    "objectID": "blog/2024-07-15-syrup/index.html",
    "href": "blog/2024-07-15-syrup/index.html",
    "title": "A new package for profiling parallel R code",
    "section": "",
    "text": "I’ve found that the following pattern looms large in content about parallel processing with R (including my own):\nTo help myself ask better questions about parallelism and its effect on usage of system resources, I wanted a tool that could help me collect data beyond execution times. There are many good tools for profiling memory usage and execution time for sequential (as in “normal,” single-threaded, not parallelized) R code—such as Rprofmem(), the profmem package, the bench package, and packages in the R-prof GitHub organization—but extending those tools to the parallel context is a hard, hard problem.\nI’m excited to share the initial release of syrup, an R package providing measures of memory and CPU usage for parallel R code. Rather than logging allocations and garbage collections, syrup measures usage of system resources by regularly pinging the system command ps and logging its results for all running R sessions. While the measurements that syrup can take are rather coarse in comparison to available tools for profiling sequential R code, I’ve found that they provide enough data to help me argue hypotheses that I was unable to support with data before.\nYou can install the package with the following code:\nIn this blog post, I’ll give a brief example of what it does and then share briefly about how it works.\nlibrary(syrup)"
  },
  {
    "objectID": "blog/2024-07-15-syrup/index.html#sec-what",
    "href": "blog/2024-07-15-syrup/index.html#sec-what",
    "title": "A new package for profiling parallel R code",
    "section": "What does it do?",
    "text": "What does it do?\nThe syrup package provides one function, syrup(). Supplied an expression, the function will return snapshots of memory and CPU usage captured while the expression evaluates. For example:\n\nsyrup(Sys.sleep(2))\n\n# A tibble: 8 × 8\n     id time                  pid  ppid name           pct_cpu       rss     vms\n  &lt;dbl&gt; &lt;dttm&gt;              &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;bch:byt&gt; &lt;bch:b&gt;\n1     1 2024-07-15 08:47:07 23227 23168 R               NA         146MB   392GB\n2     1 2024-07-15 08:47:07 22648 21210 rsession-arm64  NA         176MB   393GB\n3     2 2024-07-15 08:47:08 23227 23168 R                0         146MB   392GB\n4     2 2024-07-15 08:47:08 22648 21210 rsession-arm64   0.988     176MB   393GB\n5     3 2024-07-15 08:47:08 23227 23168 R                0         146MB   392GB\n6     3 2024-07-15 08:47:08 22648 21210 rsession-arm64   0.864     176MB   393GB\n7     4 2024-07-15 08:47:09 23227 23168 R                0.162     146MB   392GB\n8     4 2024-07-15 08:47:09 22648 21210 rsession-arm64   0.824     176MB   393GB\n\n\nIn this tibble, id defines a specific time point at which process usage was snapshotted, and the remaining columns show output derived from ps::ps(). Notably, pid is the process ID, ppid is the process ID of the parent process, pct_cpu is the percent CPU usage, and rss is the resident set size (a measure of memory usage).\nFor a more interesting demo, we’ll tune a regularized linear model using cross-validation with tidymodels. First, loading needed packages:\n\nlibrary(future)\nlibrary(tidymodels)\nlibrary(rlang)\n\nUsing future to define our parallelism strategy, we’ll set plan(multicore, workers = 5), indicating that we’d like to distribute computations across 5 cores using forking. By default, future disables forking from RStudio; I know that, in the context of building this README, this usage of forking is safe, so I’ll temporarily override that default with parallelly.fork.enable.\n\nlocal_options(parallelly.fork.enable = TRUE)\nplan(multicore, workers = 5)\n\nNow, simulating some data:\n\nset.seed(1)\ndat &lt;- sim_regression(1000000)\n\ndat\n\n# A tibble: 1,000,000 × 21\n   outcome predictor_01 predictor_02 predictor_03 predictor_04 predictor_05\n     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1    3.63       -1.88        0.872       -0.799       -0.0379       2.68  \n 2   41.6         0.551      -2.47         2.37         3.90         5.18  \n 3   -6.99       -2.51       -3.15         2.61         2.13         3.08  \n 4   33.2         4.79        1.86        -2.37         4.27        -3.59  \n 5   34.3         0.989      -0.315        3.08         2.56        -5.91  \n 6   26.7        -2.46       -0.459        1.75        -5.24         5.04  \n 7   21.4         1.46       -0.674       -0.894       -3.91        -3.38  \n 8   21.7         2.21        1.28        -1.05        -0.561        2.99  \n 9   -8.84        1.73        0.0725       0.0976       5.40         4.30  \n10   24.5        -0.916      -0.223       -0.561       -4.12         0.0508\n# ℹ 999,990 more rows\n# ℹ 15 more variables: predictor_06 &lt;dbl&gt;, predictor_07 &lt;dbl&gt;,\n#   predictor_08 &lt;dbl&gt;, predictor_09 &lt;dbl&gt;, predictor_10 &lt;dbl&gt;,\n#   predictor_11 &lt;dbl&gt;, predictor_12 &lt;dbl&gt;, predictor_13 &lt;dbl&gt;,\n#   predictor_14 &lt;dbl&gt;, predictor_15 &lt;dbl&gt;, predictor_16 &lt;dbl&gt;,\n#   predictor_17 &lt;dbl&gt;, predictor_18 &lt;dbl&gt;, predictor_19 &lt;dbl&gt;,\n#   predictor_20 &lt;dbl&gt;\n\n\nWe’ve got a million observations from a simulated regression problem, where 20 predictors are available to predict the value of outcome.\nThe call to tune_grid() does some setup sequentially before sending data off to the five child processes to actually carry out the model fitting. After models are fitted, data is sent back to the parent process to be combined. To better understand system resource usage throughout that process, we wrap the call in syrup():\n\nres_mem &lt;- syrup({\n  res &lt;-\n    # perform a grid search, where we...\n    tune_grid(\n      # vary the amount of regularization in a linear regression, and...\n      linear_reg(engine = \"glmnet\", penalty = tune()),\n      # use all predictors to model the outcome, ...\n      outcome ~ .,\n      # cross-validating the training data with (default) 10 folds.\n      vfold_cv(dat)\n    )\n})\n\nres_mem\n\n# A tibble: 46 × 8\n      id time                  pid  ppid name           pct_cpu       rss    vms\n   &lt;dbl&gt; &lt;dttm&gt;              &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;bch:byt&gt; &lt;bch:&gt;\n 1     1 2024-07-15 08:47:12 23227 23168 R               NA         1.2GB  393GB\n 2     1 2024-07-15 08:47:12 22648 21210 rsession-arm64  NA      175.95MB  393GB\n 3     2 2024-07-15 08:47:13 23252 23227 R               NA      543.59MB  393GB\n 4     2 2024-07-15 08:47:13 23251 23227 R               NA      556.78MB  393GB\n 5     2 2024-07-15 08:47:13 23250 23227 R               NA      563.22MB  393GB\n 6     2 2024-07-15 08:47:13 23249 23227 R               NA      565.36MB  393GB\n 7     2 2024-07-15 08:47:13 23248 23227 R               NA      596.19MB  393GB\n 8     2 2024-07-15 08:47:13 23227 23168 R               62.0      1.25GB  393GB\n 9     2 2024-07-15 08:47:13 22648 21210 rsession-arm64   0.399  175.95MB  393GB\n10     3 2024-07-15 08:47:14 23252 23227 R               98.5    744.69MB  393GB\n# ℹ 36 more rows\n\n\nThese results are a bit more interesting than the sequential results from Sys.sleep(2). Look closely at the ppids for each id; after a snapshot or two, you’ll see five identical ppids for each id, and those ppids match up with the remaining pid in the one remaining R process. This shows us that we’ve indeed distributed computations using forking in that one remaining R process, the “parent,” has spawned off five child processes from itself.\nWe can plot the result to get a better sense of how memory usage of these processes changes over time:\n\n# retrieve the current process ID, which will be the parent\n# ID for the workers\nworker_ppid &lt;- ps::ps_pid()\n\nres_mem %&gt;%\n  # keep only the parent process and its workers\n  filter(ppid == worker_ppid | pid == worker_ppid) %&gt;%\n  ggplot() +\n  aes(x = id, y = rss, group = pid) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:max(res_mem$id))\n\n\n\n\n\n\n\nAt first, only the parent process has non-NA rss, as tidymodels hasn’t sent data off to any workers yet. Then, each of the 5 workers receives data from tidymodels and begins fitting models. Eventually, each of those workers returns their results to the parent process, and their rss is once again NA. The parent process wraps up its computations before completing evaluation of the expression, at which point syrup() returns. (Keep in mind: memory is weird. In the above plot, the total memory allotted to the parent session and its five workers at each ID is not simply the sum of those rss values, as memory is shared among them.) We see another side of the story come together for CPU usage:\n\nres_mem %&gt;%\n  filter(ppid == worker_ppid | pid == worker_ppid) %&gt;%\n  ggplot() +\n  aes(x = id, y = pct_cpu, group = pid) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:max(res_mem$id))\n\n\n\n\n\n\n\nThe percent CPU usage will always be NA the first time a process ID is seen, as the usage calculation is based on change since the previous recorded value. As soon as we’re able to start measuring, we see the workers at 100% usage, while the parent process is largely idle once it has sent data off to workers."
  },
  {
    "objectID": "blog/2024-07-15-syrup/index.html#sec-how",
    "href": "blog/2024-07-15-syrup/index.html#sec-how",
    "title": "A new package for profiling parallel R code",
    "section": "How does it work?",
    "text": "How does it work?\nLoosely, the function works like this:\n\n\nsyrup() captures the supplied expression (and doesn’t yet evaluate it).\nThe function then spins up another R session—call it sesh—that snapshots memory and CPU usage information for all running R processes at a regular interval.\nIn the original R session, syrup() evaluates the supplied expression. While it’s doing so, sesh is running in the background tracking its usage of system resources.\nOnce the expression is finished evaluating, the original R session tells sesh to return all of the information it’s queried and then closes sesh.\nFinally, syrup() returns the memory and CPU usage information.\n\nThose “snapshots” are calls to ps::ps(), whose output looks something like this:\n\nps::ps()\n\n# A data frame: 613 × 11\n     pid  ppid name            username  status    user  system     rss      vms\n * &lt;int&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 23257 21210 license-manager simoncou… &lt;NA&gt;   NA      NA      NA      NA      \n 2 23256    NA &lt;NA&gt;            &lt;NA&gt;      &lt;NA&gt;   NA      NA      NA      NA      \n 3 23227 23168 R               simoncou… runni…  3.47    0.546   1.40e9  4.22e11\n 4 23225     1 mdworker_shared simoncou… runni…  0.0657  0.0266  2.72e7  4.20e11\n 5 23222     1 mdworker_shared simoncou… runni…  0.0636  0.0223  2.74e7  4.20e11\n 6 23220     1 mdworker_shared simoncou… runni…  0.0409  0.0116  1.15e7  4.20e11\n 7 23219     1 mdworker_shared simoncou… runni…  0.0436  0.0131  1.15e7  4.20e11\n 8 23217     1 mdworker_shared simoncou… runni…  0.0370  0.0111  1.16e7  4.20e11\n 9 23210     1 mdworker_shared simoncou… runni…  0.0299  0.0145  2.21e7  4.20e11\n10 23208     1 mdworker_shared simoncou… runni…  0.0421  0.0148  2.19e7  4.20e11\n# ℹ 603 more rows\n# ℹ 2 more variables: created &lt;dttm&gt;, ps_handle &lt;I&lt;list&gt;&gt;\n\n\nps::ps() returns information on all active processes. Under the hood, syrup() does a filter() to extract only active R processes1 and does some mutate()s to calculate the percent CPU usage.\nMy graphic design skills are lame, but here’s an attempt at visualizing that process for the simple Sys.sleep() example:\n\n\n\n\nA diagram showing what happens behind the scenes of syrup() in a simple case. The interval argument controls how often resource usage is snapshotted and defaults to half a second; I set it explicitly here just for demonstration.\n\n\n\nAgain, the function output becomes a bit more interesting when code is executed in parallel. For that tuning process, it might look something like:\n\n\n\n\nSince we’ve configured future to distribute computations across 5 cores using forking, the call to tune_grid() results in 5 workers spinning up to cross-validate models. Each call to ps::ps() will capture details on all running R sessions."
  },
  {
    "objectID": "blog/2024-07-15-syrup/index.html#concluding",
    "href": "blog/2024-07-15-syrup/index.html#concluding",
    "title": "A new package for profiling parallel R code",
    "section": "Concluding",
    "text": "Concluding\nsyrup is a pretty scrappy tool, both in terms of its implementation and the utility of its output. At the same time, I think its existence is a testament to the power of the R package ecosystem–syrup is able to traverse quite a bit of complexity in only a couple hundred lines of source code thanks to a few (relatively heavy) dependencies. The callr and ps packages do a lot of heavy lifting for interfacing with multiple R processes at once2, and I took on tidyverse dependencies liberally to speed up the development process while ensuring safety.\nWhile the kinds of measurements that syrup can provide are pretty coarse in comparison to what’s available for sequential R code, they provide more data points with which to test hypotheses about how parallel R code interfaces with system processes and consumes computational resources. I’ve found the package to be a helpful tool in better understanding tidymodels’ support for parallelism, and I hope others find it useful in their own work."
  },
  {
    "objectID": "blog/2024-07-15-syrup/index.html#footnotes",
    "href": "blog/2024-07-15-syrup/index.html#footnotes",
    "title": "A new package for profiling parallel R code",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis is just a grep() for processes named \"R\" or R processes that look like the ones RStudio and Positron carry around (\"rsession\" and \"ark\", respectively).↩︎\nThanks, Gábor.↩︎"
  },
  {
    "objectID": "blog/2025-03-26-claude-code/index.html",
    "href": "blog/2025-03-26-claude-code/index.html",
    "title": "How I’m Using Claude Code to Develop R Packages",
    "section": "",
    "text": "Since the release of Claude Code a few weeks ago, I’ve been experimenting with using the tool to develop R packages. I’ve come to really appreciate it as part of my LLM toolkit, but it’s definitely taken a bit of getting used to."
  },
  {
    "objectID": "blog/2025-03-26-claude-code/index.html#vibes",
    "href": "blog/2025-03-26-claude-code/index.html#vibes",
    "title": "How I’m Using Claude Code to Develop R Packages",
    "section": "Vibes",
    "text": "Vibes\nClaude Code is a LLM coding-assistance tool that can interface with your project directory. It can read and modify files (without you explicitly asking it to do so), run bash commands, and use git. Claude Code is interfaced with via a cli, though once you initialize it with claude, you can mostly just interact with it via plain-language from there.\nI use Claude Code in my day-to-day working on R packages. Claude Code is one of a few different ways I interface with LLMs:\n\nClaude Code: here we are!\nThe claude.ai web interface\nVia Claude/Deepseek/Gemini/OpenAI APIs\n\nFrom R using ellmer\n\nWhen programming, just via ellmer’s Chat class\nFor a chat-like interface that has access to my R environment, using btw::btw_app()\n\nA few specialized tools that I’ve built on top of ellmer, like gander and [chores]\n\n\nFrom Positron, the IDE I’m usually working in, using Continue\n\n\n\n\nI’ve mostly found that I haven’t converted to using Claude Code for any tasks that I used to use another interface for. For example, I still tend to use Continue to ask questions about specific files in codebases, and I still use gander for little syntactical things that would take me a minute or so to write out myself. That said, using Claude Code has opened up a new mode of interaction with LLMs for me; I’ve started to lean on models for writing and refactoring tasks that used to take me something like 15 minutes. This is a context where I used to not attempt to use models at all, suspecting that I’d probably spend more time debugging slop than it would have taken me to write the code myself. That said, even for these 15ish-minute tasks, there are still situations where I’ve found I’m better off working through changes myself."
  },
  {
    "objectID": "blog/2025-03-26-claude-code/index.html#prompting",
    "href": "blog/2025-03-26-claude-code/index.html#prompting",
    "title": "How I’m Using Claude Code to Develop R Packages",
    "section": "Prompting",
    "text": "Prompting\nOther than vibes, the only thing I really have to contribute in this blog post is a version of the CLAUDE.md I’ve been situating in my R package directories. CLAUDE.md is sort of a README.md specifically for the model. The Claude Code documentation suggests folks use a model to draft a CLAUDE.md using \\init—I’d recommend doing this once just to see the kinds of things they suggest including in the document, but in general, models tend to churn through a good few tokens at this step to generate slightly-off instructions on how to navigate the structure of an R package.\nAt the moment, my CLAUDE.md files are composed of a few pieces:\n\n\nHow R package development works: e.g. where the source files are, where the corresponding test files are, how to run the tests. I had initially assumed that there would be enough “this is how R packages are structured” baked into the weights that I wouldn’t have to do this, but this doesn’t seem to be the case.\n\nMy taste:\n\nI tell the thing to read a source file and a test file I’ve written that I like by pointing it to the relative paths of the files.\nThe thing really wants to add code comments that speak to the “what” rather than the “why.” I have two mentions of “please don’t add new code comments and don’t remove old ones” in my CLAUDE.md files, and I’ve also gotten into the habit of appending “no new code comments” to every first message I send in a Claude Code chat, and yet I’m still consistently asking the model to remove them from output.\n\n\n\nContext: This is one lesson from interacting with Claude that I’ve gotten a ton of value out of. If something I’m working on has an oft-used dependency that isn’t likely to be well-represented in the training data, I supply btw::btw() output about the dependency into CLAUDE.md. For example, I’m working on a package right now that makes heavy use of ellmer Chat objects. Instead of supplying context that I feel is probably relevant whenever it’s needed, I’ve just pasted the output of btw::btw(\"?ellmer::Chat\", \"{ellmer}\") into my CLAUDE.md. That output (see below) has the help page for ?ellmer::Chat and the package-level help page inlined into plain-text. Since adding that output, Claude Code has almost been a more proficient user of ellmer than I am.\n\nHere’s an example CLAUDE.md that I’ve used recently."
  },
  {
    "objectID": "blog/2025-03-26-claude-code/index.html#workflow",
    "href": "blog/2025-03-26-claude-code/index.html#workflow",
    "title": "How I’m Using Claude Code to Develop R Packages",
    "section": "Workflow",
    "text": "Workflow\nOne note on an (anti-)pattern for interfacing with Claude Code… The tool has a weird relationship with unit tests. It really wants them to pass, so much so that it will add problematic mocking or even remove tests entirely that it can’t get to pass rather than asking you for input or revisiting the issues in source files. For that reason, I often opt to pause Claude Code when it notes it’s going to a run a test and instead run it myself, pasting output from the test along with my own interpretation of the issue if there are any failures."
  },
  {
    "objectID": "blog/2025-03-26-claude-code/index.html#section",
    "href": "blog/2025-03-26-claude-code/index.html#section",
    "title": "How I’m Using Claude Code to Develop R Packages",
    "section": "💰💰💰",
    "text": "💰💰💰\nOne of the most common observations I see when people talk about Claude Code is how expensive it is. Their website reads “Typical usage costs range from $5-10 per developer per day, but can exceed $100 per hour during intensive use.” $100 per hour would be wild. That said, I haven’t really felt that Claude Code’s churned through an inordinate number of tokens.\nI’ve found that:\n\n\nThe absolute cost has been in the ballpark of other LLM interfaces. I pay something like 20 bucks a month to Anthropic for the web interface to their models, and I think OpenAI’s base paid plan is similar. Similarly, via BYO-key tools like Continue used in Positron and btw_client() (and also some GitHub Actions), I churn through something like 20 bucks a month in tokens between the Claude/Deepseek/Gemini/OpenAI APIs. Since March 1st (25 days ago as I write this), I’ve spent about $30 on my Claude Code API key. Again, I’m not using Claude Code all day every day, but just in those times where 1) I’m actively working on code and 2) it seems like the best tool for the job.\n\nI’m happy with the value per dollar. What I don’t mean here is that Claude Code is the best value per dollar for every task. Very seldomly will I use Claude Code for something that would only take a couple minutes for me to do; in that case, I’m either doing the thing myself or using some interface to an LLM that is lower-friction to get started up. What I do mean is that Claude Code is capable of accomplishing 15-ish minute tasks, albeit with a couple minutes of prompting, in a way that other LLM interfaces are not, and I’m happy to pay for that.\n\nYour feelings on what is A Lot Of Money to spend on LLMs likely differ from mine. I write software for a living, and I do so in the USA, both of which greatly effect what I think of as “cheap.”"
  },
  {
    "objectID": "blog/2024-10-29-book/index.html",
    "href": "blog/2024-10-29-book/index.html",
    "title": "Announcing a new book",
    "section": "",
    "text": "Over the last couple years, I’ve spent quite a bit of time focused on making tidymodels code run as fast as possible. Throughout, I’ve written about this work a good bit on this blog1 and the tidyverse blog2. Early this year, I had the idea that maybe I ought to compile many of those learnings together in a book, focused on helping tidymodels users reduce the computational time needed to develop machine learning models without sacrificing predictive performance. I wrote portions of a couple chapters over the course of a couple weeks, and then mostly set the book aside for many months.\nAttending posit::conf(2024), though, renewed my excitement about the book. Some folks that had read my blog posts on tidymodels’ performance over the years tracked me down to tell me that they found them really useful. One even told me something along the lines of:\nThis really, really impacted me. Beyond that, others even approached me—knowing that I worked on tidymodels but not knowing that performance was an interest of mine—with questions about how to make their tidymodels code run faster. This renewed my sense that this was a book worth writing, and in the couple months since, I’ve tried to notch out a 4-hour stretch sometime each week to focus on the book. Today, during my R/Pharma 2024 talk, I open-sourced the current draft at emlwr.org!\nThe book focuses on helping you adapt your tidymodels code to run faster while preserving predictive performance. For now, I’m calling it Efficient Machine Learning with R. No chapter is fully fleshed out, but the introduction, parallelism, and submodel trick chapters have a good bit of content in them. If you’d like to be notified when new material is added, follow me on socials @simonpcouch or watch the source repository.\nWhile I’m actively working on the book, I have to balance writing it with the usual hum of development on tidymodels and other R packages; it will be a good while before this thing is finished. My hope is that the book will ultimately be published in print, though its content will always be freely available online. I’m hopeful that folks will find this book useful!\nMany thanks to those who have supported me along the way."
  },
  {
    "objectID": "blog/2024-10-29-book/index.html#footnotes",
    "href": "blog/2024-10-29-book/index.html#footnotes",
    "title": "Announcing a new book",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee tidymodels is getting a whole lot faster, Optimizing model parameters faster with tidymodels, How to best parallelize boosted tree model fits with tidymodels, and Down the submodels rabbit hole with tidymodels.↩︎\nSee tune 1.2.0, Tuning hyperparameters with tidymodels is a delight, and Writing performant code with tidy tools.↩︎"
  },
  {
    "objectID": "blog/2025-05-07-gemini-2-5-pro-new/index.html",
    "href": "blog/2025-05-07-gemini-2-5-pro-new/index.html",
    "title": "Evaluating the new Gemini 2.5 Pro update on R coding",
    "section": "",
    "text": "The title line of Google’s release post on the newest Gemini 2.5 Pro release is “even better coding performance.” Reading this, I was curious whether we’d see a notable increase in performance compared to the last generation on R coding tasks; in an earlier post, I saw that the March release of Gemini 2.5 Pro was a contender with Claude 3.7 Sonnet on An R Eval, a dataset of challenging R coding problems.\nIn this post, I’ll be using the vitals package to compare the new Gemini 2.5 Pro release against its previous generation, as well as the leading “snappy” thinking models on this task from Anthropic and OpenAI: Claude 3.7 Sonnet (Thinking Enabled) and GPT o4-mini, respectively."
  },
  {
    "objectID": "blog/2025-05-07-gemini-2-5-pro-new/index.html#setting-up-the-evaluation",
    "href": "blog/2025-05-07-gemini-2-5-pro-new/index.html#setting-up-the-evaluation",
    "title": "Evaluating the new Gemini 2.5 Pro update on R coding",
    "section": "Setting up the evaluation",
    "text": "Setting up the evaluation\nLet’s start by defining our model connections using ellmer (notably, the dev version of the package):\n\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\n\ngemini_2_5_pro_new &lt;- chat_google_gemini(\n  model = \"gemini-2.5-pro-preview-03-25\"\n)\nclaude_sonnet_3_7 &lt;- sonnet_3_7_thinking &lt;- chat_anthropic(\n  model = \"claude-3-7-sonnet-latest\",\n  api_args = list(\n    thinking = list(type = \"enabled\", budget_tokens = 2000)\n  )\n)\ngpt_o4_mini &lt;- chat_openai(model = \"o4-mini-2025-04-16\")\n\nYou may have noticed that gemini_2_5_pro_new seemingly refers to a model from March \"gemini-2.5-pro-preview-03-25\". Google switched out the model underlying that string, meaning we can no longer access the old model, and code that used to refer to the old model will refer to the new one automatically. That was a choice. Thankfully, I ran this eval against the old Gemini 2.5 Pro version and saved the results a month ago, so I’ll do some trickery under-the-hood to include its results here.\nNote that I needed to configure GOOGLE_API_KEY, ANTHROPIC_API_KEY, and OPENAI_API_KEY environment variables to connect to these services. The pricing for these models is roughly comparable:\n\n\n# A tibble: 3 × 3\n  Name              Input Output\n  &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; \n1 Gemini 2.5 Pro    $1.25 $10.00\n2 Claude 3.7 Sonnet $3.00 $15.00\n3 GPT o4-mini       $1.10 $4.40 \n\n\nA couple bits worth noting for interpreting this table:\n\nThe new Gemini 2.5 Pro update has the same pricing as the old one.\nThe per-token pricing for Gemini 2.5 Pro increases after the first 200,000 tokens in a request. We won’t come close to hitting that threshold in this eval, so the shown pricing is what applies here.\nEach of these are “reasoning” models, but may use varying numbers of tokens to come to an answer, so the actual cost breakdown for each of these may not reflect the ratios shown above.\n\nWe’ll be able to quantify the actual cost and number of tokens used using the new $get_cost() method from vitals; the package can total the cost for running a given eval for any model supported by ellmer."
  },
  {
    "objectID": "blog/2025-05-07-gemini-2-5-pro-new/index.html#a-baseline-task",
    "href": "blog/2025-05-07-gemini-2-5-pro-new/index.html#a-baseline-task",
    "title": "Evaluating the new Gemini 2.5 Pro update on R coding",
    "section": "A baseline task",
    "text": "A baseline task\nAs in my previous evaluations, I’ll use the are dataset from vitals and set up a task that will evaluate each model:\n\nare_task &lt;- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n\nAn evaluation task An-R-Eval.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee my previous post on Gemini 2.5 Pro for a more thorough description of this evaluation.\n\n\nLet’s start by evaluating the new Gemini 2.5 Pro model.\n\nare_gemini_2_5_pro_new &lt;- are_task$clone()\nare_gemini_2_5_pro_new$eval(solver_chat = gemini_2_5_pro_new)\n\nThe new $get_cost() method from vitals gives a go at tallying up the cost of the eval for the solver and the scorer. I’ve uncovered a couple bugs in writing this blog post, but I’ll show the results here anyway:\n\nare_gemini_2_5_pro_new$get_cost()\n\n       provider             model input output price\n1 Google/Gemini    gemini-2.5-pro 34038  98863 $0.00\n2     Anthropic claude-3-7-sonnet    NA     NA $1.35\n\n\nNeedless to say, the Gemini tokens were not free. :) Since the cost information for that model isn’t available from ellmer, that price should read NA rather than 0, but based on the I/O, that’s 5 cents for input tokens and a dollar for output tokens.\nInterestingly, the knowledge cutoff for this model is January 2025. This eval was open sourced shortly after then; I wonder if we’ll start to see big jumps in performance once the knowledge cutoffs for newly released models include the time when this eval was publicly available on GitHub.\nNext, we’ll evaluate Claude 3.7 Sonnet with thinking enabled:\n\nare_claude_3_7 &lt;- are_task$clone()\nare_claude_3_7$eval(solver_chat = claude_sonnet_3_7)\n\n\nare_claude_3_7$get_cost()\n\n   provider             model  input output price\n1 Anthropic claude-3-7-sonnet 126611 141150 $2.50\n\n\nNote, here, that both the solver and the scorer used Claude 3.7 Sonnet here. By default, the method doesn’t differentiate between solver and scorer tokens; I wonder whether it should.\nFinally, let’s evaluate GPT o4-mini:\n\nare_gpt_o4_mini &lt;- are_task$clone()\nare_gpt_o4_mini$eval(solver_chat = gpt_o4_mini)\n\n\nare_gpt_o4_mini$get_cost()\n\n   provider              model input output price\n1    OpenAI o4-mini-2025-04-16 29601 135219 $0.00\n2 Anthropic  claude-3-7-sonnet    NA     NA $1.12\n\n\nIn this case, o4-mini’s costs were something like 3 cents for input and 60 cents for output.\nUnder the hood, I’ve imported the old Gemini 2.5 Pro results and situated them in an updated task object as gemini_2_5_pro_old. It’s shown first in the interactive viewer as gemini-2.5-pro-exp-03-25:"
  },
  {
    "objectID": "blog/2025-05-07-gemini-2-5-pro-new/index.html#analysis",
    "href": "blog/2025-05-07-gemini-2-5-pro-new/index.html#analysis",
    "title": "Evaluating the new Gemini 2.5 Pro update on R coding",
    "section": "Analysis",
    "text": "Analysis\nLet’s combine the results of all evaluations to compare the models:\n\nare_eval &lt;- \n  vitals_bind(\n    `Gemini 2.5 Pro (New)` = are_gemini_2_5_pro_new,\n    `Gemini 2.5 Pro (Old)` = are_gemini_2_5_pro_old,\n    `Claude Sonnet 3.7` = are_claude_3_7,\n    `GPT o4-mini` = are_gpt_o4_mini\n  ) %&gt;%\n  rename(model = task) %&gt;%\n  mutate(\n    model = factor(model, levels = c(\n      \"Gemini 2.5 Pro (New)\",\n      \"Gemini 2.5 Pro (Old)\",\n      \"Claude Sonnet 3.7\",\n      \"GPT o4-mini\"\n    ))\n  )\n\nare_eval\n\n# A tibble: 310 × 5\n   model                id                        epoch score metadata\n   &lt;fct&gt;                &lt;chr&gt;                     &lt;int&gt; &lt;ord&gt; &lt;list&gt;  \n 1 Gemini 2.5 Pro (New) after-stat-bar-heights        1 I     &lt;tibble&gt;\n 2 Gemini 2.5 Pro (New) after-stat-bar-heights        2 C     &lt;tibble&gt;\n 3 Gemini 2.5 Pro (New) after-stat-bar-heights        3 I     &lt;tibble&gt;\n 4 Gemini 2.5 Pro (New) conditional-grouped-summ…     1 P     &lt;tibble&gt;\n 5 Gemini 2.5 Pro (New) conditional-grouped-summ…     2 P     &lt;tibble&gt;\n 6 Gemini 2.5 Pro (New) conditional-grouped-summ…     3 I     &lt;tibble&gt;\n 7 Gemini 2.5 Pro (New) correlated-delays-reason…     1 C     &lt;tibble&gt;\n 8 Gemini 2.5 Pro (New) correlated-delays-reason…     2 P     &lt;tibble&gt;\n 9 Gemini 2.5 Pro (New) correlated-delays-reason…     3 C     &lt;tibble&gt;\n10 Gemini 2.5 Pro (New) curl-http-get                 1 C     &lt;tibble&gt;\n# ℹ 300 more rows\n\n\nLet’s visualize the results with a bar chart:\n\nare_eval %&gt;%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n  ) %&gt;%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"The updated Gemini 2.5 Pro seems roughly comparable to its previous\\ngeneration on R coding tasks.\"\n  ) +\n  theme(\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\nTo determine if the differences we’re seeing are statistically significant, we’ll use a cumulative link mixed model:\n\nlibrary(ordinal)\n\nare_mod &lt;- clmm(score ~ model + (1|id), data = are_eval)\n\n\nsummary(are_mod)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  310  -239.32 490.63 228(1343) 9.83e-06 3.8e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 8.096    2.845   \nNumber of groups:  id 26 \n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)\nmodelGemini 2.5 Pro (Old) -0.08905    0.37518  -0.237    0.812\nmodelClaude Sonnet 3.7     0.35848    0.37519   0.955    0.339\nmodelGPT o4-mini           0.59695    0.38113   1.566    0.117\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -1.7738     0.6526  -2.718\nP|C   0.4034     0.6428   0.627\n\n\nFor the purposes of this post, we’ll just take a look at the Coefficients table. The reference model here is the new Gemini 2.5 Pro. Negative coefficient estimates for a given model indicate that model is less likely to receive higher ratings than the new Gemini 2.5 Pro. Looking at the coefficients:\n\nThe old Gemini 2.5 Pro is roughly the same in its R coding capabilities as the new one.\nWhile neither of the differences from the new Gemini 2.5 Pro are statistically significant, Claude Sonnet 3.7 and GPT o4-mini are neck-and-neck with each other as the SotA on this eval.\n\nIn short, from my perspective, there’s not much to see here with this new Gemini release.\n\nThank you to Max Kuhn for advising on the model-based analysis here."
  },
  {
    "objectID": "blog/2025-05-19-germany/index.html",
    "href": "blog/2025-05-19-germany/index.html",
    "title": "Visiting Germany",
    "section": "",
    "text": "In an effort to not forget what I most want to remember about the places I visit, I usually jot down notes about what I’m thinking about and try to take lots of pictures. I’ve left a few of them here. :)\n**"
  },
  {
    "objectID": "blog/2025-05-19-germany/index.html#days-0-3-munich",
    "href": "blog/2025-05-19-germany/index.html#days-0-3-munich",
    "title": "Visiting Germany",
    "section": "Days 0-3: Munich",
    "text": "Days 0-3: Munich\nThe first days of my visit include lots of biking, coffee, sweet treats, and perusing.\nThe bridge Donnersbergerbrücke overlooks a railyard; the structure of the supports make them a pleasant place to sit and watch the sunset.\nJohanna, making bagels. I learn that the literal translation for “oven mitts” in German is “hand shoes.”\nI walk into a coffee shop and hear a Chicago rapper, Smino, playing. I learn that staring is not considered impolite here.\n **\n **\nBiking around, no car-dodging necessary.\nAfter I order what the waiter describes as a “light mixed plate” and receive nine sausages on a bed of sauerkraut, I resolve to overindex on pretzels for the remainder of the trip."
  },
  {
    "objectID": "blog/2025-05-19-germany/index.html#days-4-6-mittenwald",
    "href": "blog/2025-05-19-germany/index.html#days-4-6-mittenwald",
    "title": "Visiting Germany",
    "section": "Days 4-6: Mittenwald",
    "text": "Days 4-6: Mittenwald\nWoke up feeling anxious about the journey from Munich to Mittenwald. Had an incredible coffee (Roca Junior!) and managed to catch the right trains. From here, I start taking many more pictures.\n **\nThe only depictions of Bavaria I’ve encountered in the U.S. are at theme parks and fairs; as I walked through the town throughout the next few days, I almost expected to see rollercoasters or something.\nThe hiking from Mittenwald was absurd. On the first day, I just started walking West of the city into some mountains. On the second, East. On the third, a Friday, my friend Johanna takes off work and drives out to pick me up and cross the border to Austria for another nearby hike.\n **\n\n\n **\n\n\n\n **\n\n\n\n\n **\n\n\n\nLots of these all around Bavaria. It’s pretty difficult to see at this resolution, but in the background, a group of soldiers was practicing an alpine rescue operation on the shown cliff face.\n\n\n\n\nDammkarhütte.\n\n\n\n **\n\n\n\n\n **\n\n\n\n **\n\n\nThat evening, we went to my friend’s friend’s birthday party, where everyone was speaking German and the birthday boy was slinging really good pizzas off of his balcony. What a trip."
  },
  {
    "objectID": "blog/2025-05-19-germany/index.html#days-7-8-munich",
    "href": "blog/2025-05-19-germany/index.html#days-7-8-munich",
    "title": "Visiting Germany",
    "section": "Days 7-8: Munich",
    "text": "Days 7-8: Munich\nBiking, coffee, sweet treats, and perusing."
  },
  {
    "objectID": "blog/2025-05-21-gemini-2-5-flash/index.html",
    "href": "blog/2025-05-21-gemini-2-5-flash/index.html",
    "title": "Evaluating Gemini 2.5 Flash on R coding tasks",
    "section": "",
    "text": "Google’s preview of their Gemini 2.5 Pro model has really made a splash. The model has become many folks’ daily driver, and I’ve started to see “What about Gemini?” in the comments of each of these blog posts if they don’t explicitly call out the model series in the title. Yesterday, Google announced an update of the preview for Gemini 2.5 Flash, a smaller and cheaper version of 2.5 Pro.\nIn the model card, Google juxtaposes Gemini 2.5 Flash with OpenAI’s o4-mini:\nThis comparison especially caught my eye, given that o4-mini is the current leader in the class of cheap, snappy thinking models on an R coding evaluation I’ve been running the last few months. The proposition seems to be “o4-mini-ish performance at a fraction of the price.”\nIn this post, I’ll use the vitals package to compare Gemini 2.5 Flash against several other models:"
  },
  {
    "objectID": "blog/2025-05-21-gemini-2-5-flash/index.html#setting-up-the-evaluation",
    "href": "blog/2025-05-21-gemini-2-5-flash/index.html#setting-up-the-evaluation",
    "title": "Evaluating Gemini 2.5 Flash on R coding tasks",
    "section": "Setting up the evaluation",
    "text": "Setting up the evaluation\nLet’s start by defining our model connections using ellmer:\n\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\n\n# thinking is enabled by default, and can be disabled by\n# with a magic incantation\n# https://ai.google.dev/gemini-api/docs/thinking\ngemini_2_5_flash_thinking &lt;- chat_google_gemini(\n  model = \"gemini-2.5-flash-preview-05-20\"\n)\n\ngemini_2_5_flash_non_thinking &lt;- chat_google_gemini(\n  model = \"gemini-2.5-flash-preview-05-20\",\n  api_args = list(\n    generationConfig = list(\n      thinkingConfig = list(\n        thinkingBudget = 0\n        )\n      )\n    )\n)\n\ngemini_2_0_flash &lt;- chat_google_gemini(\n  model = \"gemini-2.0-flash\"\n)\ngemini_2_5_pro &lt;- chat_google_gemini(\n  model = \"gemini-2.5-pro-preview-05-06\"\n)\n\ngpt_o4_mini &lt;- chat_openai(model = \"o4-mini-2025-04-16\")\n\n# note that i don't enable thinking here; thinking \n# doesn't seem to have an effect for claude on this \n# eval: https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/\nclaude_sonnet_3_7 &lt;- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\n\nNote that I needed to configure GOOGLE_API_KEY, ANTHROPIC_API_KEY, and OPENAI_API_KEY environment variables to connect to these models. The pricing for these models varies considerably:\n\n\n# A tibble: 6 × 3\n  Name                            Input Output\n  &lt;chr&gt;                           &lt;chr&gt; &lt;chr&gt; \n1 Gemini 2.5 Flash (Thinking)     $0.15 $3.50 \n2 Gemini 2.5 Flash (Non-thinking) $0.15 $0.60 \n3 Gemini 2.0 Flash                $0.10 $0.40 \n4 Gemini 2.5 Pro                  $1.25 $10.00\n5 GPT o4-mini                     $1.10 $4.40 \n6 Claude 3.7 Sonnet               $3.00 $15.00\n\n\nGemini 2.5 Flash has a thinking and non-thinking mode, where thinking tokens are not surfaced to the user but output tokens are charged at a higher rate. With thinking enabled (as shown on the model card), Gemini 2.5 Flash’s output tokens are priced somewhat similarly to o4-mini.\nGemini 2.5 Pro, Gemini 2.5 Flash (Thinking), and GPT o4-mini are reasoning models, and thus will use more tokens than non-reasoning models. While Claude 3.7 Sonnet has a reasoning mode that could be enabled here, I haven’t done so for this eval as it doesn’t seem to make a difference for performance on this eval.\nLet’s set up a task that will evaluate each model using the are dataset from vitals:\n\nare_task &lt;- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n\nAn evaluation task An-R-Eval.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee my first post on Gemini 2.5 Pro for a more thorough description of this evaluation."
  },
  {
    "objectID": "blog/2025-05-21-gemini-2-5-flash/index.html#running-the-evaluations",
    "href": "blog/2025-05-21-gemini-2-5-flash/index.html#running-the-evaluations",
    "title": "Evaluating Gemini 2.5 Flash on R coding tasks",
    "section": "Running the evaluations",
    "text": "Running the evaluations\nFirst, we’ll evaluate our reference model, Gemini 2.5 Flash with thinking enable:\n\nare_gemini_2_5_flash_thinking &lt;- are_task$clone()\nare_gemini_2_5_flash_thinking$eval(\n  solver_chat = gemini_2_5_flash_thinking\n)\n\nFrom here, it’s pretty rote. The same model without thinking enabled:\n\nare_gemini_2_5_flash_non_thinking &lt;- are_task$clone()\nare_gemini_2_5_flash_non_thinking$eval(\n  solver_chat = gemini_2_5_flash_non_thinking\n)\n\nNow for the other Gemini models:\n\nare_gemini_2_0_flash &lt;- are_task$clone()\nare_gemini_2_0_flash$eval(solver_chat = gemini_2_0_flash)\n\n\nare_gemini_2_5_pro &lt;- are_task$clone()\nare_gemini_2_5_pro$eval(solver_chat = gemini_2_5_pro)\n\nNext, we’ll evaluate GPT o4-mini:\n\nare_gpt_o4_mini &lt;- are_task$clone()\nare_gpt_o4_mini$eval(solver_chat = gpt_o4_mini)\n\nFinally, let’s evaluate Claude 3.7 Sonnet:\n\nare_claude_3_7 &lt;- are_task$clone()\nare_claude_3_7$eval(solver_chat = claude_sonnet_3_7)\n\nThe interactive viewer will allow us to inspect the evaluation in detail:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the total durations of the evaluations are correct in the viewer, the timings of specific samples are now off. Given some changes in downstream packages, vitals has to estimate how long a given request takes rather than receiving the exact duration; this will be resolved down the line."
  },
  {
    "objectID": "blog/2025-05-21-gemini-2-5-flash/index.html#analysis",
    "href": "blog/2025-05-21-gemini-2-5-flash/index.html#analysis",
    "title": "Evaluating Gemini 2.5 Flash on R coding tasks",
    "section": "Analysis",
    "text": "Analysis\nLet’s combine the results of all evaluations to compare the models:\n\nare_eval &lt;- \n  vitals_bind(\n    `Gemini 2.5 Flash (Thinking)` = are_gemini_2_5_flash_thinking,\n    `Gemini 2.5 Flash (Non-thinking)` = are_gemini_2_5_flash_non_thinking,\n    `Gemini 2.0 Flash` = are_gemini_2_0_flash,\n    `Gemini 2.5 Pro` = are_gemini_2_5_pro,\n    `GPT o4-mini` = are_gpt_o4_mini,\n    `Claude Sonnet 3.7` = are_claude_3_7\n  ) %&gt;%\n  rename(model = task) %&gt;%\n  mutate(\n    model = factor(model, levels = c(\n      \"Gemini 2.5 Flash (Thinking)\",\n      \"Gemini 2.5 Flash (Non-thinking)\",\n      \"Gemini 2.0 Flash\",\n      \"Gemini 2.5 Pro\",\n      \"GPT o4-mini\",\n      \"Claude Sonnet 3.7\"\n    ))\n  )\n\nare_eval\n\n# A tibble: 468 × 5\n   model                       id                 epoch score metadata\n   &lt;fct&gt;                       &lt;chr&gt;              &lt;int&gt; &lt;ord&gt; &lt;list&gt;  \n 1 Gemini 2.5 Flash (Thinking) after-stat-bar-he…     1 I     &lt;tibble&gt;\n 2 Gemini 2.5 Flash (Thinking) after-stat-bar-he…     2 I     &lt;tibble&gt;\n 3 Gemini 2.5 Flash (Thinking) after-stat-bar-he…     3 I     &lt;tibble&gt;\n 4 Gemini 2.5 Flash (Thinking) conditional-group…     1 P     &lt;tibble&gt;\n 5 Gemini 2.5 Flash (Thinking) conditional-group…     2 C     &lt;tibble&gt;\n 6 Gemini 2.5 Flash (Thinking) conditional-group…     3 C     &lt;tibble&gt;\n 7 Gemini 2.5 Flash (Thinking) correlated-delays…     1 P     &lt;tibble&gt;\n 8 Gemini 2.5 Flash (Thinking) correlated-delays…     2 P     &lt;tibble&gt;\n 9 Gemini 2.5 Flash (Thinking) correlated-delays…     3 P     &lt;tibble&gt;\n10 Gemini 2.5 Flash (Thinking) curl-http-get          1 C     &lt;tibble&gt;\n# ℹ 458 more rows\n\n\nLet’s visualize the results with a bar chart:\n\nare_eval %&gt;%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n  ) %&gt;%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"The Gemini 2.5 Flash models represent a middle-ground between 2.5 Pro and\\no4-mini, both in terms of price and performance.\"\n  ) +\n  theme(\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\nTo determine if the differences we’re seeing are statistically significant, we’ll use a cumulative link mixed model:\n\nlibrary(ordinal)\n\nare_mod &lt;- clmm(score ~ model + (1|id), data = are_eval)\n\n\nsummary(are_mod)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  468  -375.15 766.30 397(1971) 8.31e-05 6.4e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 5.523    2.35    \nNumber of groups:  id 26 \n\nCoefficients:\n                                     Estimate Std. Error z value\nmodelGemini 2.5 Flash (Non-thinking)  -0.8506     0.3604  -2.360\nmodelGemini 2.0 Flash                 -1.3338     0.3698  -3.607\nmodelGemini 2.5 Pro                    0.6600     0.3611   1.828\nmodelGPT o4-mini                       0.7887     0.3620   2.179\nmodelClaude Sonnet 3.7                 0.3740     0.3567   1.048\n                                     Pr(&gt;|z|)    \nmodelGemini 2.5 Flash (Non-thinking)  0.01826 *  \nmodelGemini 2.0 Flash                 0.00031 ***\nmodelGemini 2.5 Pro                   0.06759 .  \nmodelGPT o4-mini                      0.02934 *  \nmodelClaude Sonnet 3.7                0.29441    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -1.3191     0.5404  -2.441\nP|C   0.4820     0.5361   0.899\n\n\nFor the purposes of this post, we’ll just take a look at the Coefficients table. The reference model here is Gemini 2.5 Flash. Negative coefficient estimates for a given model indicate that model is less likely to receive higher ratings than Gemini 2.5 Flash. Looking at the coefficients:\n\nGemini 2.5 Flash is a marked improvement over its previous generation on this eval.\nGemini 2.5 Flash significantly lags behind o4-mini on this eval.\nEnabling Gemini 2.5 Flash’s thinking results in a marked increase in performance over the non-thinking model, though brings the pricing much closer to the more performant o4-mini.\nGemini 2.5 Flash with thinking disabled is nearly indistinguishable from GPT 4.1-nano on this eval. 2.5 Flash reached an accuracy of 43.6% at a cost of $0.15/m input, $0.60/m output, which 4.1-nano scored 44.2% at a cost of $0.10/m input, $0.40/m output. 4.1-nano continues to pack the greatest punch in the budget, non-thinking price point.\n\nOne more note before I wrap up: For the past month or two, development on ellmer and vitals has been quite coupled to support answering common questions about LLM performance. With the release of ellmer 0.2.0 on CRAN last week, I’m starting to gear up for an initial CRAN release of vitals here soon. In the meantime, I’m especially interested in feedback from folks who have given the package a go! Do let me know if you give it a whir and run into any hiccups.\n\nThank you to Max Kuhn for advising on the model-based analysis here.\nIn a first for this blog, I tried using a model to help me write this post. In general, I don’t tend to use models to help with writing at all. Now that I’ve written a good few of these posts to pattern-match from, I wondered if Claude 3.7 Sonnet could draft a reasonable starting place. I used this prompt; as usual, I ended up deleting all of the prose that the model wrote, but it was certainly a boost to have all of the code written for me."
  },
  {
    "objectID": "blog/2025-05-21-gemini-2-5-flash/index.html#conclusion",
    "href": "blog/2025-05-21-gemini-2-5-flash/index.html#conclusion",
    "title": "Evaluating Gemini 2.5 Flash on R coding tasks",
    "section": "Conclusion",
    "text": "Conclusion\nFor the past month or two, development on ellmer and vitals has been quite coupled to support answering common questions about LLM performance. With the release of ellmer 0.2.0 on CRAN, I’m starting to gear up for an initial CRAN release of vitals here soon. In the meantime, I’m especially interested in feedback from folks who have given the package a go! Do let me know if you give it a whir and run into any hiccups.\n\nThank you to Max Kuhn for advising on the model-based analysis here.\nI tried out using a model to help me write this post. In general, I don’t tend to use models to help with writing at all. Now that I’ve written a good few of these posts to pattern-match from, I wondered if Claude 3.7 Sonnet could draft a reasonable starting place. I used this prompt; as usual, I ended up deleting all of the prose that the model wrote, but it was certainly a boost to have all of the code written for me."
  },
  {
    "objectID": "blog/2025-05-27-claude-4/index.html",
    "href": "blog/2025-05-27-claude-4/index.html",
    "title": "Claude 4 and R Coding",
    "section": "",
    "text": "Claude 4 dropped on Thursday! Given that Claude 3.7 Sonnet is my daily driver LLM for R coding, I’ve been excited to poke at it.\nThe last few months, I’ve been writing a series of blog posts where I evaluate new LLM releases on their R coding performance. I do so entirely in R using the ellmer and vitals packages, the latter of which will be headed to CRAN in the coming weeks. In this post, I’ll skip over all of the evaluation code and just make some graphs; if you’re interested in learning more about how to run an eval like this one, check out the post Evaluating o3 and o4-mini on R coding performance.\nHere’s the gist:\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\nlibrary(ggrepel)\nYou can view the raw results of the evaluation in this interactive viewer:"
  },
  {
    "objectID": "blog/2025-05-27-claude-4/index.html#analysis",
    "href": "blog/2025-05-27-claude-4/index.html#analysis",
    "title": "Claude 4 and R Coding",
    "section": "Analysis",
    "text": "Analysis\nAt this point, we have access to are_eval, a data frame containing all of the results collected during the evaluation.\n\nare_eval\n\n# A tibble: 312 × 5\n   model           id                          epoch score metadata\n   &lt;fct&gt;           &lt;chr&gt;                       &lt;int&gt; &lt;ord&gt; &lt;list&gt;  \n 1 Claude 4 Sonnet after-stat-bar-heights          1 I     &lt;tibble&gt;\n 2 Claude 4 Sonnet after-stat-bar-heights          2 I     &lt;tibble&gt;\n 3 Claude 4 Sonnet after-stat-bar-heights          3 I     &lt;tibble&gt;\n 4 Claude 4 Sonnet conditional-grouped-summary     1 C     &lt;tibble&gt;\n 5 Claude 4 Sonnet conditional-grouped-summary     2 P     &lt;tibble&gt;\n 6 Claude 4 Sonnet conditional-grouped-summary     3 C     &lt;tibble&gt;\n 7 Claude 4 Sonnet correlated-delays-reasoning     1 P     &lt;tibble&gt;\n 8 Claude 4 Sonnet correlated-delays-reasoning     2 P     &lt;tibble&gt;\n 9 Claude 4 Sonnet correlated-delays-reasoning     3 I     &lt;tibble&gt;\n10 Claude 4 Sonnet curl-http-get                   1 C     &lt;tibble&gt;\n# ℹ 302 more rows\n\n\nThe evaluation scores each answer as “Correct”, “Partially Correct”, or “Incorrect”. We can use a bar chart to visualize the proportions of responses that fell into each of those categories:\n\nare_eval %&gt;%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n  ) %&gt;%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"Claude 4 models represent a step forward in R coding performance.\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.border = element_blank(),\n    panel.background = element_rect(fill = \"#f8f8f1\", color = NA),\n    plot.background = element_rect(fill = \"#f8f8f1\", color = NA),\n    legend.background = element_rect(fill = \"#F3F3EE\", color = NA),\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\nThe pricing per token of each of these models differs quite a bit, though, and o4-mini might use many more tokens during its “reasoning.” How much did it cost to run each of these evals, and what’s the resulting cost-per-performance?\nThe pricing per million tokens for these models is as follows:\n\n\n# A tibble: 4 × 3\n  Name              Input  Output\n  &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt; \n1 Claude 4 Sonnet   $3.00  $15.00\n2 Claude 4 Opus     $15.00 $75.00\n3 Claude 3.7 Sonnet $3.00  $15.00\n4 o4-mini           $1.10  $4.40 \n\n\nUnder the hood, I’ve calculated the total cost of running the eval for each model using the shown pricing and joined it to the evaluation results:\n\ncosts\n\n               Name     Price    Score\n1   Claude 4 Sonnet 0.7204620 59.61538\n2     Claude 4 Opus 3.3161850 66.02564\n3 Claude 3.7 Sonnet 0.4359000 57.05128\n4           o4-mini 0.6164763 65.38462\n\n\n\nggplot(costs) +\n  aes(x = Price, y = Score, label = Name) +\n  geom_point(size = 3, color = \"#4A7862\") +\n  geom_label_repel() +\n  scale_y_continuous(limits = c(55, 70), labels = function(x) paste0(x, \"%\")) +\n  labs(\n    x = \"Total Price (USD)\",\n    y = \"Score\",\n    title = \"R Coding Performance vs. Cost\",\n    subtitle = \"o4-mini packs the most punch at its price point, though\\nClaude 4 Opus is state-of-the-art.\"\n  ) +\n  lims(x = c(0, 4))\n\n\n\n\n\n\n\nWhile Claude 4 Opus is the new SotA on this eval, o4-mini almost matches its performance at a small fraction of the price.\nTo determine if the differences we’re seeing are statistically significant, we’ll use a cumulative link mixed model:\n\nlibrary(ordinal)\n\nare_mod &lt;- clmm(score ~ model + (1|id), data = are_eval)\n\n\nsummary(are_mod)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  312  -205.43 422.85 234(1729) 7.06e-06 2.0e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 13.56    3.682   \nNumber of groups:  id 26 \n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)\nmodelClaude 4 Opus       0.6190     0.4126   1.500    0.134\nmodelClaude 3.7 Sonnet  -0.3293     0.4115  -0.800    0.424\nmodelGPT o4-mini         0.5753     0.4236   1.358    0.174\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P -2.25743    0.43244  -5.220\nP|C -0.04208    0.48543  -0.087\n\n\nFor the purposes of this post, we’ll just take a look at the Coefficients table. The reference model here is Claude 4 Sonnet. Negative coefficient estimates for a given model indicate that model is less likely to receive higher ratings than Claude 4 Sonnet. Looking at the coefficients, while Claude 4 Sonnet seems like an improvement upon its previous generation, we don’t see evidence of statistically significantly differing performance on this eval from Claude 4 Sonnet and any other model.\n\nThank you to Max Kuhn for advising on the model-based analysis here."
  },
  {
    "objectID": "blog/2025-07-14-kimi-k2/index.html",
    "href": "blog/2025-07-14-kimi-k2/index.html",
    "title": "Kimi K2 and R Coding",
    "section": "",
    "text": "It was a hoot and a half of a weekend in the LLM world. A company I hadn’t heard of called Moonshot AI released a model called Kimi K2. From 30,000 feet:\nAlso, their release post includes a supposedly-one-shotted Javascript Minecraft clone? Can this please be a thing from now on?\nIn this post, we’ll put this model through its paces on some R coding tasks using the newly-on-CRAN vitals R package for LLM evaluation. First, I’ll show how to use ellmer to connect to “unsupported” models. Then, we’ll load in An R Eval, a dataset of challenging R coding problems. We’ll let Kimi K2 take a crack at each of the problems, and then compare how well it does to a few other leading models.\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\nlibrary(ggrepel)"
  },
  {
    "objectID": "blog/2025-07-14-kimi-k2/index.html#connecting-to-kimi-k2-with-ellmer",
    "href": "blog/2025-07-14-kimi-k2/index.html#connecting-to-kimi-k2-with-ellmer",
    "title": "Kimi K2 and R Coding",
    "section": "Connecting to Kimi K2 with ellmer",
    "text": "Connecting to Kimi K2 with ellmer\nSince this Kimi series of models is a relatively new player, the ellmer R package doesn’t yet have “official” support for the model. However, Moonshot’s API uses the OpenAI spec, meaning that we can just make use of ellmer’s support for OpenAI to interact with the model by changing the base_url, api_key, and default model.\n\nchat_moonshot &lt;- function(\n  system_prompt = NULL, \n  base_url = \"https://api.moonshot.ai/v1\", \n  api_key = Sys.getenv(\"MOONSHOT_API_KEY\"), \n  model = \"kimi-k2-0711-preview\", \n  ...\n) {\n  chat_openai(\n    system_prompt = system_prompt,\n    base_url = base_url, \n    api_key = api_key, \n    model = model, \n    ...\n  )\n}\n\n\nch &lt;- chat_moonshot()\nch$chat(\"hey!\")\n#&gt; Hey! What’s up?\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the API is advertised as OpenAI-compatible, it has some small modifications that result in 400 Errors occasionally. I switched out chat_deepseek() for chat_openai() in the above and resolved most of them.\n\n\nI had to sign up for an API key and put a dollar on it to run this eval. This model seems quite cheap compared to the models it’s being likened to, so I’ll also run it against some models that are closer in pricing:\n\n\nModel\nInput Price\nOutput Price\n\n\n\nKimi K2\n$0.60\n$2.50\n\n\nClaude 4 Sonnet\n$3.00\n$15.00\n\n\nClaude 4 Opus\n$15.00\n$75.00\n\n\nGPT-4.1\n$2.00\n$8.00\n\n\nGPT-4.1 mini\n$0.40\n$1.60\n\n\nGemini 2.5 Flash\n$0.30\n$2.50\n\n\n\nBeing able to wrap the OpenAI API this easily is so nice, and their API key flow was easy peasy. Google Gemini, take notes."
  },
  {
    "objectID": "blog/2025-07-14-kimi-k2/index.html#evaluating-the-model",
    "href": "blog/2025-07-14-kimi-k2/index.html#evaluating-the-model",
    "title": "Kimi K2 and R Coding",
    "section": "Evaluating the model",
    "text": "Evaluating the model\nTo evaluate the model, we define a chat instance for the solver (Kimi K2) and scorer (Claude 3.7 Sonnet), set up an evaluation Task object, and then run $eval():\n\nclaude_3_7_sonnet &lt;- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\nkimi_k2 &lt;- chat_moonshot()\n\nare_task &lt;- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_3_7_sonnet, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task$eval(solver_chat = kimi_k2)\n\nI’m writing this on a Sunday, so take this with a grain of salt, but the API was snappy. No issues. We’ll see how that holds up tomorrow, when I’ll send this out.3\nIt costed about 7 cents to run this eval against Kimi K2.\n\nFull evaluation code here\n\nclaude_4_sonnet &lt;- chat_anthropic(model = \"claude-sonnet-4-20250514\")\nclaude_4_opus &lt;- chat_anthropic(model = \"claude-opus-4-20250514\")\nclaude_3_7_sonnet &lt;- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\ngpt_4_1 &lt;- chat_openai(model = \"gpt-4.1\")\ngpt_4_1_mini &lt;- chat_openai(model = \"gpt-4.1-mini\")\ngemini_2_5_flash &lt;- chat_google_gemini(\n  model = \"gemini-2.5-flash\",\n  api_args = list(\n    generationConfig = list(\n      thinkingConfig = list(\n        thinkingBudget = 0\n        )\n      )\n    )\n)\nkimi_k2 &lt;- chat_moonshot()\n\nare_task &lt;- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_3_7_sonnet, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n\nare_claude_4_sonnet &lt;- are_task$clone()\nare_claude_4_sonnet$eval(solver_chat = claude_4_sonnet)\nsave(are_claude_4_sonnet, file = \"blog/2025-07-14-kimi-k2/tasks/are_claude_4_sonnet.rda\")\n\nare_claude_4_opus &lt;- are_task$clone()\nare_claude_4_opus$eval(solver_chat = claude_4_opus)\nsave(are_claude_4_opus, file = \"blog/2025-07-14-kimi-k2/tasks/are_claude_4_opus.rda\")\n\nare_claude_4_opus &lt;- are_task$clone()\nare_claude_4_opus$eval(solver_chat = claude_4_opus)\nsave(are_claude_4_opus, file = \"blog/2025-07-14-kimi-k2/tasks/are_claude_4_opus.rda\")\n\nare_gpt_4_1 &lt;- are_task$clone()\nare_gpt_4_1$eval(solver_chat = gpt_4_1)\nsave(are_gpt_4_1, file = \"blog/2025-07-14-kimi-k2/tasks/are_gpt_4_1.rda\")\n\nare_gpt_4_1_mini &lt;- are_task$clone()\nare_gpt_4_1_mini$eval(solver_chat = gpt_4_1_mini)\nsave(are_gpt_4_1_mini, file = \"blog/2025-07-14-kimi-k2/tasks/are_gpt_4_1_mini.rda\")\n\nare_gemini_2_5_flash &lt;- are_task$clone()\nare_gemini_2_5_flash$eval(solver_chat = gemini_2_5_flash)\nsave(are_gemini_2_5_flash, file = \"blog/2025-07-14-kimi-k2/tasks/are_gemini_2_5_flash.rda\")\n\nare_kimi_k2 &lt;- are_task$clone()\nare_kimi_k2$eval(solver_chat = kimi_k2)\nsave(are_kimi_k2, file = \"blog/2025-07-14-kimi-k2/tasks/are_kimi_k2.rda\")\n\nYou can view the raw results of the evaluation in this interactive viewer:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the total durations of the evaluations are correct in the viewer, the timings of specific samples are now estimated. Given some changes in downstream packages, vitals has to estimate how long a given request takes rather than receiving the exact duration; this will be resolved down the line."
  },
  {
    "objectID": "blog/2025-07-14-kimi-k2/index.html#analysis",
    "href": "blog/2025-07-14-kimi-k2/index.html#analysis",
    "title": "Kimi K2 and R Coding",
    "section": "Analysis",
    "text": "Analysis\nAt this point, we have access to are_eval, a data frame containing all of the results collected during the evaluation.\n\nare_eval\n\n# A tibble: 504 × 5\n   model   id                          epoch score metadata         \n   &lt;fct&gt;   &lt;chr&gt;                       &lt;int&gt; &lt;ord&gt; &lt;list&gt;           \n 1 Kimi K2 after-stat-bar-heights          1 I     &lt;tibble [1 × 10]&gt;\n 2 Kimi K2 after-stat-bar-heights          2 I     &lt;tibble [1 × 10]&gt;\n 3 Kimi K2 after-stat-bar-heights          3 I     &lt;tibble [1 × 10]&gt;\n 4 Kimi K2 conditional-grouped-summary     1 I     &lt;tibble [1 × 10]&gt;\n 5 Kimi K2 conditional-grouped-summary     2 I     &lt;tibble [1 × 10]&gt;\n 6 Kimi K2 conditional-grouped-summary     3 I     &lt;tibble [1 × 10]&gt;\n 7 Kimi K2 correlated-delays-reasoning     1 C     &lt;tibble [1 × 10]&gt;\n 8 Kimi K2 correlated-delays-reasoning     2 I     &lt;tibble [1 × 10]&gt;\n 9 Kimi K2 correlated-delays-reasoning     3 P     &lt;tibble [1 × 10]&gt;\n10 Kimi K2 curl-http-get                   1 P     &lt;tibble [1 × 10]&gt;\n# ℹ 494 more rows\n\n\nThe evaluation scores each answer as “Correct”, “Partially Correct”, or “Incorrect”. We can use a bar chart to visualize the proportions of responses that fell into each of those categories:\n\nare_eval %&gt;%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n    model = fct_rev(model)\n  ) %&gt;%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"Kimi K2 lags behind many of the models it was likened to\\nin Moonshot AI's release post.\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.border = element_blank(),\n    panel.background = element_rect(fill = \"#f8f8f1\", color = NA),\n    plot.background = element_rect(fill = \"#f8f8f1\", color = NA),\n    legend.background = element_rect(fill = \"#F3F3EE\", color = NA),\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\nAt least on this eval, it looks like Kimi K2 is slightly more expensive and slightly worse or comparable to GPT 4.1-mini and Gemini 2.5 Flash, and certainly not a contender with GPT 4.1, Claude 4 Sonnet, or Claude 4 Opus."
  },
  {
    "objectID": "blog/2025-07-14-kimi-k2/index.html#extra-credits",
    "href": "blog/2025-07-14-kimi-k2/index.html#extra-credits",
    "title": "Kimi K2 and R Coding",
    "section": "Extra credit(s)",
    "text": "Extra credit(s)\nI’m not sure if it was just some weird UI artifact or an intended behavior, but it seemed the minimum amount I could put on a Moonshot API key was $10. So, after spending the 7 cents needed to run this eval, I had some extra runway.\nI’ve recently been working on a yet-to-be-open-sourced agent thing that allows for plugging in different models under the hood. At a high level:\n\nThere are about 20,000 tokens in the prompt, 10,000 of which are attached to one tool.\nThe application relies on 1) strong instruction-following, 2) strong tool usage, and 3) some imagination.\nThere’s one tool that is asynchronous in the sense that it returns immediately when called, saying “Ok, got your inputs, running now,” but only later returns the actual results of the tool call. I’ve seen a few models really struggle with this format.\n\nI figured this would be a good context in which to vibe-eval this new model with my leftover credits. The vibes:\n\nTokens streamed a good bit more slowly than I’m used to seeing from Claude Sonnet or Gemini 2.5 Pro. (This was on Monday, so presumably much higher usage on the API than Sunday.)\nThe model seems fine-tuned into the “responses should be unordered lists with bolded labels” local minimum pretty thoroughly.\nTool calling is weak. The model doesn’t follow instructions in tool descriptions well or react to error messages reasonably. If I prompt the model to make changes to the way that it’s calling a tool myself, it seems to react well.\n\nAltogether, I wouldn’t call this an impressive release, and don’t anticipate I’ll spend much more time with this model."
  },
  {
    "objectID": "blog/2025-07-14-kimi-k2/index.html#footnotes",
    "href": "blog/2025-07-14-kimi-k2/index.html#footnotes",
    "title": "Kimi K2 and R Coding",
    "section": "Footnotes",
    "text": "Footnotes\n\nSome asterisks here with the licensing.↩︎\nWell, Llama 4, but yikes.↩︎\nSpoiler: it was slower.↩︎"
  }
]