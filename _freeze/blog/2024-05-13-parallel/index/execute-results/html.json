{
  "hash": "38e9f62a9073a811b70a1f803eb4cca6",
  "result": {
    "markdown": "---\ntitle: \"How to best parallelize boosted tree model fits with tidymodels\"\ndate: '2024-05-13'\ntags:\n  - tidymodels\n  - parsnip\n  - rstats\nsubtitle: \"Should tidymodels users use the parallelism implementations from XGBoost and LightGBM?\"\nimage: featured.png\nsummary: ''\n---\n\n::: {.cell}\n\n:::\n\n\nThe [XGBoost](https://xgboost.readthedocs.io/) and [LightGBM](https://lightgbm.readthedocs.io/en/stable/) modeling engines both enable distributing the computations needed to train a single boosted tree model across several CPU cores. Similarly, the [tidymodels framework](https://tidymodels.org) enables distributing model fits across cores. The natural question, then, is whether tidymodels users ought to make use of the engine's parallelism implementation, tidymodels' implementation, or both at the same time. This blog post is a scrappy attempt at finding which of those approaches will lead to the smallest elapsed time when fitting many models.\n\n## The problem\n\nFor example, imagine the case where we evaluate a single model against 10 resamples. Doing so sequentially might look something like this, where each dotted orange segment indicates a model fit:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/basic_resample.png){fig-alt='A horizontal line segment. The left- and right-most tips are colored in green, and the majority of the line segment, on the inside, is orange. 10 dots are evenly interspersed among the orange portion of the segment.' width=1126}\n:::\n:::\n\n\nThe x-axis here depicts time. The short green segments on either side of the orange segments indicate the portions of the elapsed time allotted to tidymodels \"overhead,\" like checking arguments and combining results. This graphic depicts a *sequential* series of model fits, where each fit takes place one after the other.\n\n::: callout-note\nIf you're feeling lost already, a [previous blog post of mine](https://www.simonpcouch.com/blog/2023-03-24-speedups-2023/) on how we think about optimizing our code in tidymodels might be helpful.\n:::\n\n### 1) Use the engine's parallelism implementation only.\n\nThe XGBoost and LightGBM engines implement their own parallelism frameworks such that a single model fit can be distributed across many cores. If we distribute a single model fit's computations across 5 cores, we could see, best-case, a 5-fold speedup in the time to fit each model. The model fits still happen in order, but each individual (hopefully) happens much quicker, resulting in a shorter overall time:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/engine_resample.png){fig-alt='The same line segment, but \\'squished\\' horizontally.' width=1126}\n:::\n:::\n\n\nThe increased height of each segment representing a model fit represents how the computations for each model fit are distributed across multiple CPU cores. (I don't know. There's probably a better way to depict that.)\n\n### 2) Use tidymodels' parallelism implementation only.\n\nThe tidymodels framework supports distributing model fits across CPU cores in the sense that, when fitting **n** models across **m** CPU cores, tidymodels can allot each core to fit **n/m** of the models. In the case of 10 models across 5 cores, then, each core takes care of fitting two:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/tidymodels_resample.png){fig-alt='The same line segment to the first, except the orange portion of the segment has been split into 5 segments, 2 dots wide each, and they\\'re all stacked vertically on top of each other.' width=1126}\n:::\n:::\n\n\nNote that a given model fit happens on a single core, so the time to fit a single model stays the same.\n\n### 3) Use both the engine's and tidymodels' parallelism implementation.\n\nWhy can't we do both **1)** and **2)**? If both parallelism approaches play nicely with each other, and neither of them was able to *perfectly* distribute its computations across all of the available resources, then we'd see that we could get some of the benefits from both approach and get the maximal computational performance out of our available resources:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/both_resample.png){fig-alt='The same graphic from above, but the five orange segments stacked on top of each other are also \\'squished\\' horizontally: a combination of the two graphics above.' width=1126}\n:::\n:::\n\n\nIn reality, parallelism frameworks come with their fair share of overhead, and often don't play nicely with each other. It'd be nice to know if, in practice, any of these three approaches stand out among the others as the most performant way to resample XGBoost and LightGBM models with tidymodels. We'll simulate some data and run some quick benchmarks to get some intuition about how to best parallelize parameter tuning with tidymodels.\n\nThis post is based on a similar idea to an [Applied Predictive Modeling blog post](https://blog.aml4td.org/posts/while-you-wait-for-that-to-finish-can-i-interest-you-in-parallel-processing/index.html) from Max Kuhn in 2018, but is generally:\n\n-   less refined (Max tries out many different dataset sizes on three different operating systems, while I fix both of those variables here),\n-   uses modern implementations, incl. tidymodels instead of [caret](https://topepo.github.io/caret/), [future](https://www.futureverse.org/) instead of [foreach](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html), and updated XGBoost (and LightGBM) package versions, and\n-   happens to be situated in a modeling context more similar to one that I'm currently benchmarking for another project.\n\nI'm running this experiment on an M1 Pro Macbook Pro with 32GB of RAM and 10 cores, running MacOS Sonoma 14.4.1. We'll create a 10,000-row dataset and partition it into 10 folds for cross-validation, tuning among a set of 10 possible candidate values, resulting in 100 9,000-row model fits per call to [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html).\n\n## Setup\n\nStarting off by loading needed packages and simulating some data using the [`sim_classification()`](https://modeldata.tidymodels.org/reference/sim_classification.html) function from modeldata:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(bonsai)\nlibrary(future)\n\nset.seed(1)\ndat <- sim_classification(1e4)\n```\n:::\n\n\nWe'd like to predict `class` using the rest of the variables in the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,000 × 16\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   <fct>          <dbl>        <dbl>        <dbl>        <dbl>        <dbl>\n 1 class_2       -0.329       -1.28         0.186       0.578         0.732\n 2 class_2        0.861       -0.389        0.106       0.701         0.647\n 3 class_2       -0.461       -1.69         0.193       0.337         0.814\n 4 class_2        2.75         1.35        -0.215       0.119         0.104\n 5 class_2        0.719        0.127       -0.479       0.878         0.334\n 6 class_2       -0.743       -1.36         0.480       0.0517        0.400\n 7 class_2        0.805        0.447       -0.947       0.342         0.382\n 8 class_2        0.669        1.23         0.682       0.461         0.924\n 9 class_2        0.887        0.593        0.701       0.772         0.297\n10 class_1       -1.14         0.353       -0.729       0.819         0.331\n# ℹ 9,990 more rows\n# ℹ 10 more variables: linear_01 <dbl>, linear_02 <dbl>, linear_03 <dbl>,\n#   linear_04 <dbl>, linear_05 <dbl>, linear_06 <dbl>, linear_07 <dbl>,\n#   linear_08 <dbl>, linear_09 <dbl>, linear_10 <dbl>\n```\n:::\n\n```{.r .cell-code}\nform <- class ~ .\n```\n:::\n\n\nSplitting the data into training and testing sets before making a 10-fold cross-validation object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ndat_split <- initial_split(dat)\ndat_train <- training(dat_split)\ndat_test <- testing(dat_split)\ndat_folds <- vfold_cv(dat_train)\n\ndat_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [6750/750]> Fold01\n 2 <split [6750/750]> Fold02\n 3 <split [6750/750]> Fold03\n 4 <split [6750/750]> Fold04\n 5 <split [6750/750]> Fold05\n 6 <split [6750/750]> Fold06\n 7 <split [6750/750]> Fold07\n 8 <split [6750/750]> Fold08\n 9 <split [6750/750]> Fold09\n10 <split [6750/750]> Fold10\n```\n:::\n:::\n\n\nFor both XGBoost and LightGBM, we'll only tune the learning rate and number of trees.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspec_bt <-\n  boost_tree(learn_rate = tune(), trees = tune()) %>%\n  set_mode(\"classification\")\n```\n:::\n\n\nThe `trees` parameter greatly affects the time to fit a boosted tree model. Just to be *super* sure that analogous fits are happening in each of the following `tune_grid()` calls, we'll create the grid of possible parameter values beforehand and pass it to each `tune_grid()` call.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\ngrid_bt <-\n  spec_bt %>%\n  extract_parameter_set_dials() %>%\n  grid_latin_hypercube(size = 10)\n\ngrid_bt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   trees learn_rate\n   <int>      <dbl>\n 1  1175    0.173  \n 2   684    0.0817 \n 3   558    0.00456\n 4  1555    0.0392 \n 5   861    0.00172\n 6  1767    0.00842\n 7  1354    0.0237 \n 8    42    0.0146 \n 9   241    0.00268\n10  1998    0.222  \n```\n:::\n:::\n\n\nFor both LightGBM and XGBoost, we'll test each of those three approaches. I'll write out the explicit code I use to time each of these computations; note that the only thing that changes in each of those chunks is the parallelism setup code and the arguments to `set_engine()`.\n\n::: {.callout-note title=\"Homework📄\"}\nWrite a function that takes in a parallelism setup and engine and returns a `timing` like those below!😉 Make sure to \"tear down\" the parallelism setup after.\n:::\n\nFor a summary of those timings, see @sec-putting-it-all-together.\n\n## XGBoost\n\nFirst, testing **1) engine implementation only**, we use `plan(sequential)` to tell tidymodels' parallelism framework *not* to kick in, and set `nthread = 10` in `set_engine()` to tell XGBoost to distribute its computations across 10 cores:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(sequential)\n\ntiming_xgb_1 <- system.time({\n  res <-\n    tune_grid(\n      spec_bt %>% set_engine(\"xgboost\", nthread = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n```\n:::\n\n\nNow, for **2) tidymodels implementation only**, we use `plan(multisession, workers = 10)` to tell tidymodels to distribute its computations across cores and set `nthread = 1` to disable XGBoost's parallelization:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(multisession, workers = 10)\n\ntiming_xgb_2 <- system.time({\n  res <-\n    tune_grid(\n      spec_bt %>% set_engine(\"xgboost\", nthread = 1),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n```\n:::\n\n\nFinally, for **3) both parallelism implementations**, we enable parallelism for both framework:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(multisession, workers = 10)\n\ntiming_xgb_3 <- system.time({\n  res <-\n    tune_grid(\n      spec_bt %>% set_engine(\"xgboost\", nthread = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n```\n:::\n\n\nWe'll now do the same thing for LightGBM.\n\n## LightGBM\n\nFirst, testing **1) engine implementation only**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(sequential)\n\ntiming_lgb_1 <- system.time({\n  res <-\n    tune_grid(\n      spec_bt %>% set_engine(\"lightgbm\", num_threads = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n```\n:::\n\n\nNow, **2) tidymodels implementation only**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(multisession, workers = 10)\n\ntiming_lgb_2 <- system.time({\n  res <-\n    tune_grid(\n      spec_bt %>% set_engine(\"lightgbm\", num_threads = 1),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n```\n:::\n\n\nFinally, **3) both parallelism implementations**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplan(multisession, workers = 10)\n\ntiming_lgb_3 <- system.time({\n  res <-\n    tune_grid(\n      spec_bt %>% set_engine(\"lightgbm\", num_threads = 10),\n      form,\n      dat_folds,\n      grid = grid_bt\n    )\n})[[\"elapsed\"]]\n```\n:::\n\n\n## Putting it all together {#sec-putting-it-all-together}\n\nAt a glance, those timings are (in seconds):\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  approach = c(\"engine only\", \"tidymodels only\", \"both\"),\n  xgboost = c(timing_xgb_1, timing_xgb_2, timing_xgb_3),\n  lightgbm = c(timing_lgb_1, timing_lgb_2, timing_lgb_3)\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  approach        xgboost lightgbm\n  <chr>             <dbl>    <dbl>\n1 engine only      988.46  103.13 \n2 tidymodels only  133.23   23.328\n3 both             132.59   23.279\n```\n:::\n:::\n\n\nAt least in this context, we see:\n\n-   Using only the engine's parallelization results in a substantial slowdown for both engines.\n-   For both XGBoost and LightGBM, just using the tidymodels parallelization vs. combining the tidymodels and engine parallelization seem comparable in terms of timing. (This is nice to see in the sense that users don't need to adjust their tidymodels parallelism configuration just to fit this particular kind of model; if they have a parallelism configuration set up already, it won't hurt to keep it around.)\n-   LightGBM models train quite a bit faster than XGBoost models, though we can't meaningfully compare those fit times without knowing whether performance metrics are comparable.\n\nThese are similar conclusions to what Max observes in the linked APM blog post. A few considerations that, in this context, may have made tidymodels' parallelization seem extra advantageous:\n\n-   We're resampling across 10 folds here and, conveniently, distributing those computations across 10 cores. That is, each core is (likely) responsible for the fits on just one fold, and there are no cores \"sitting idle\" unless one model fit finishes well before than another.\n-   We're resampling models rather than just fitting one model. If we had just fitted one model, tidymodels wouldn't offer any support for distributing computations across cores, but this is exactly what XGBoost and LightGBM support.\n-   When using tidymodels' parallelism implementation, it's not *just* the model fits that are distributed across cores. Preprocessing, prediction, and metric calculation is also distributed across cores when using tidymodels' parallelism implementation. (Framed in the context of the diagrams above, there are little green segments dispersed throughout the orange ones that _can_ be parallelized.)\n\n## Session Info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(\n  c(tidymodels_packages(), \"xgboost\", \"lightgbm\"),\n  dependencies = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29)\n os       macOS Sonoma 14.4.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Chicago\n date     2024-05-13\n pandoc   3.1.12.3 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version     date (UTC) lib source\n broom        * 1.0.5.9000  2024-05-09 [1] Github (tidymodels/broom@b984deb)\n cli            3.6.2       2023-12-11 [1] CRAN (R 4.3.1)\n conflicted     1.2.0       2023-02-01 [1] CRAN (R 4.3.0)\n dials        * 1.2.1       2024-02-22 [1] CRAN (R 4.3.1)\n dplyr        * 1.1.4       2023-11-17 [1] CRAN (R 4.3.1)\n ggplot2      * 3.5.1       2024-04-23 [1] CRAN (R 4.3.1)\n hardhat        1.3.1.9000  2024-04-26 [1] Github (tidymodels/hardhat@ae8fba7)\n infer        * 1.0.6.9000  2024-03-25 [1] local\n lightgbm       4.3.0       2024-01-18 [1] CRAN (R 4.3.1)\n modeldata    * 1.3.0       2024-01-21 [1] CRAN (R 4.3.1)\n parsnip      * 1.2.1.9001  2024-05-09 [1] Github (tidymodels/parsnip@320affd)\n purrr        * 1.0.2       2023-08-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.10.9000 2024-04-08 [1] Github (tidymodels/recipes@63ced27)\n rlang          1.1.3       2024-01-10 [1] CRAN (R 4.3.1)\n rsample      * 1.2.1       2024-03-25 [1] CRAN (R 4.3.1)\n rstudioapi     0.16.0      2024-03-24 [1] CRAN (R 4.3.1)\n tibble       * 3.2.1       2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.2.0       2024-03-25 [1] CRAN (R 4.3.1)\n tidyr        * 1.3.1       2024-01-24 [1] CRAN (R 4.3.1)\n tune         * 1.2.1       2024-04-18 [1] CRAN (R 4.3.1)\n workflows    * 1.1.4.9000  2024-05-01 [1] local\n workflowsets * 1.1.0       2024-03-21 [1] CRAN (R 4.3.1)\n xgboost        1.7.7.1     2024-01-25 [1] CRAN (R 4.3.1)\n yardstick    * 1.3.1       2024-03-21 [1] CRAN (R 4.3.1)\n\n [1] /Users/simoncouch/Library/R/arm64/4.3/library\n [2] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}