{
  "hash": "5af5f08663d5bd5412063593416d0cdb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Evaluating the new Gemini 2.5 Pro update on R coding\"\ndate: '2025-05-07'\ntags:\n  - ai\n  - vitals\n  - rstats\nsubtitle: \"The initial Gemini 2.5 Pro release a month ago was surprisingly strong, so I was excited to benchmark the update announced yesterday on R coding problems.\"\nimage: featured.png\nsummary: ''\n---\n\n\n\n\n\nThe title line of [Google's release post](https://developers.googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/) on the newest Gemini 2.5 Pro release is \"even better coding performance.\" Reading this, I was curious whether we'd see a notable increase in performance compared to the last generation on R coding tasks; in [an earlier post](https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/), I saw that the March release of Gemini 2.5 Pro was a contender with Claude 3.7 Sonnet on _An R Eval_, a dataset of challenging R coding problems. \n\n<a href=\"https://simonpcouch.github.io/vtials/\"><img src=\"vitals.png\" alt=\"The hex sticker for the vitals package: a teddy bear in blue scrubs happily holding a stethoscope.\" align=\"right\" height=\"240\"/></a>\n\nIn this post, I'll be using the [vitals package](https://simonpcouch.github.io/vitals/) to compare the new Gemini 2.5 Pro release against its previous generation, as well as the leading \"snappy\" thinking models on this task from Anthropic and OpenAI: Claude 3.7 Sonnet (Thinking Enabled) and [GPT o4-mini](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/#analysis), respectively.\n\n> tl;dr\n>\n> * The old and new Gemini 2.5 Pro releases seem roughly the same in their R coding capabilities.\n> * Claude Sonnet 3.7 with thinking enabled and GPT o4-mini are neck-and-neck as the leaders in this class of models.\n\n## Setting up the evaluation\n\nLet's start by defining our model connections using ellmer (notably, the dev version of the package):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\n\ngemini_2_5_pro_new <- chat_google_gemini(\n  model = \"gemini-2.5-pro-preview-03-25\"\n)\nclaude_sonnet_3_7 <- sonnet_3_7_thinking <- chat_anthropic(\n  model = \"claude-3-7-sonnet-latest\",\n  api_args = list(\n    thinking = list(type = \"enabled\", budget_tokens = 2000)\n  )\n)\ngpt_o4_mini <- chat_openai(model = \"o4-mini-2025-04-16\")\n```\n:::\n\n\n\nYou may have noticed that `gemini_2_5_pro_new` seemingly refers to a model from March `\"gemini-2.5-pro-preview-03-25\"`. Google switched out the model underlying that string, meaning we can no longer access the old model, and code that used to refer to the old model will refer to the new one automatically. That was a choice. Thankfully, I ran this eval against the old Gemini 2.5 Pro version and saved the results a month ago, so I'll do some trickery under-the-hood to include its results here.\n\nNote that I needed to configure `GOOGLE_API_KEY`, `ANTHROPIC_API_KEY`, and `OPENAI_API_KEY` environment variables to connect to these services. The pricing for these models is roughly comparable:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  Name              Input Output\n  <chr>             <chr> <chr> \n1 Gemini 2.5 Pro    $1.25 $10.00\n2 Claude 3.7 Sonnet $3.00 $15.00\n3 GPT o4-mini       $1.10 $4.40 \n```\n\n\n:::\n:::\n\n\n\nA couple bits worth noting for interpreting this table:\n\n* The new Gemini 2.5 Pro update has the same pricing as the old one.\n* The per-token pricing for Gemini 2.5 Pro increases after the first 200,000 tokens in a request. We won't come close to hitting that threshold in this eval, so the shown pricing is what applies here.\n* Each of these are \"reasoning\" models, but may use varying numbers of tokens to come to an answer, so the actual cost breakdown for each of these may not reflect the ratios shown above.\n\nWe'll be able to quantify the actual cost and number of tokens used using the new `$get_cost()` method from vitals; the package can total the cost for running a given eval for any model supported by ellmer.\n\n## A baseline task\n\nAs in my previous evaluations, I'll use the `are` dataset from vitals and set up a task that will evaluate each model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task <- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn evaluation task An-R-Eval.\n```\n\n\n:::\n:::\n\n\n\n:::callout-note\nSee my [previous post on Gemini 2.5 Pro](https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/#an-r-eval-dataset) for a more thorough description of this evaluation.\n:::\n\nLet's start by evaluating the new Gemini 2.5 Pro model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gemini_2_5_pro_new <- are_task$clone()\nare_gemini_2_5_pro_new$eval(solver_chat = gemini_2_5_pro_new)\n```\n:::\n\n\n\n\n\nThe new `$get_cost()` method from vitals gives a go at tallying up the cost of the eval for the solver and the scorer. I've uncovered a couple bugs in writing this blog post, but I'll show the results here anyway:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gemini_2_5_pro_new$get_cost()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       provider             model input output price\n1 Google/Gemini    gemini-2.5-pro 34038  98863 $0.00\n2     Anthropic claude-3-7-sonnet    NA     NA $1.35\n```\n\n\n:::\n:::\n\n\n\nNeedless to say, the Gemini tokens were not free. :) Since the cost information for that model isn't available from ellmer, that price should read `NA` rather than 0, but based on the I/O, that's 5 cents for input tokens and a dollar for output tokens.\n\nInterestingly, the knowledge cutoff for this model is January 2025. This eval was open sourced shortly after then; I wonder if we'll start to see big jumps in performance once the knowledge cutoffs for newly released models include the time when this eval was publicly available on GitHub.\n\nNext, we'll evaluate Claude 3.7 Sonnet with thinking enabled:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_claude_3_7 <- are_task$clone()\nare_claude_3_7$eval(solver_chat = claude_sonnet_3_7)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_claude_3_7$get_cost()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   provider             model  input output price\n1 Anthropic claude-3-7-sonnet 126611 141150 $2.50\n```\n\n\n:::\n:::\n\n\n\nNote, here, that both the solver and the scorer used Claude 3.7 Sonnet here. By default, the method doesn't differentiate between solver and scorer tokens; I wonder whether it should.\n\nFinally, let's evaluate GPT o4-mini:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gpt_o4_mini <- are_task$clone()\nare_gpt_o4_mini$eval(solver_chat = gpt_o4_mini)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gpt_o4_mini$get_cost()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   provider              model input output price\n1    OpenAI o4-mini-2025-04-16 29601 135219 $0.00\n2 Anthropic  claude-3-7-sonnet    NA     NA $1.12\n```\n\n\n:::\n:::\n\n\n\nIn this case, o4-mini's costs were something like 3 cents for input and 60 cents for output.\n\nUnder the hood, I've imported the old Gemini 2.5 Pro results and situated them in an updated task object as `gemini_2_5_pro_old`. It's shown first in the interactive viewer as `gemini-2.5-pro-exp-03-25`:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<iframe src=\"/assets/2025-05-07-gemini-2-5-pro-new/viewer/index.html\" width=\"100%\" height=\"600px\" style=\"border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);\"></iframe>\n```\n\n:::\n:::\n\n\n\n## Analysis\n\nLet's combine the results of all evaluations to compare the models:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval <- \n  vitals_bind(\n    `Gemini 2.5 Pro (New)` = are_gemini_2_5_pro_new,\n    `Gemini 2.5 Pro (Old)` = are_gemini_2_5_pro_old,\n    `Claude Sonnet 3.7` = are_claude_3_7,\n    `GPT o4-mini` = are_gpt_o4_mini\n  ) %>%\n  rename(model = task) %>%\n  mutate(\n    model = factor(model, levels = c(\n      \"Gemini 2.5 Pro (New)\",\n      \"Gemini 2.5 Pro (Old)\",\n      \"Claude Sonnet 3.7\",\n      \"GPT o4-mini\"\n    ))\n  )\n\nare_eval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 310 × 5\n   model                id                        epoch score metadata\n   <fct>                <chr>                     <int> <ord> <list>  \n 1 Gemini 2.5 Pro (New) after-stat-bar-heights        1 I     <tibble>\n 2 Gemini 2.5 Pro (New) after-stat-bar-heights        2 C     <tibble>\n 3 Gemini 2.5 Pro (New) after-stat-bar-heights        3 I     <tibble>\n 4 Gemini 2.5 Pro (New) conditional-grouped-summ…     1 P     <tibble>\n 5 Gemini 2.5 Pro (New) conditional-grouped-summ…     2 P     <tibble>\n 6 Gemini 2.5 Pro (New) conditional-grouped-summ…     3 I     <tibble>\n 7 Gemini 2.5 Pro (New) correlated-delays-reason…     1 C     <tibble>\n 8 Gemini 2.5 Pro (New) correlated-delays-reason…     2 P     <tibble>\n 9 Gemini 2.5 Pro (New) correlated-delays-reason…     3 C     <tibble>\n10 Gemini 2.5 Pro (New) curl-http-get                 1 C     <tibble>\n# ℹ 300 more rows\n```\n\n\n:::\n:::\n\n\n\nLet's visualize the results with a bar chart:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval %>%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n  ) %>%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"The updated Gemini 2.5 Pro seems roughly comparable to its previous\\ngeneration on R coding tasks.\"\n  ) +\n  theme(\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-are-eval-1.png){fig-alt='A horizontal bar chart comparing various AI models\\' performance on R coding tasks. The chart shows percentages of correct (blue), partially correct (beige), and incorrect (orange) answers. The new Gemini 2.5 Pro shows improved performance over the older version, and is roughly on par with Claude 3.7 Sonnet. Both slightly outperform GPT 4.1.' width=100%}\n:::\n:::\n\n\n\nTo determine if the differences we're seeing are statistically significant, we'll use a cumulative link mixed model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ordinal)\n\nare_mod <- clmm(score ~ model + (1|id), data = are_eval)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(are_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  310  -239.32 490.63 228(1343) 9.83e-06 3.8e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 8.096    2.845   \nNumber of groups:  id 26 \n\nCoefficients:\n                          Estimate Std. Error z value Pr(>|z|)\nmodelGemini 2.5 Pro (Old) -0.08905    0.37518  -0.237    0.812\nmodelClaude Sonnet 3.7     0.35848    0.37519   0.955    0.339\nmodelGPT o4-mini           0.59695    0.38113   1.566    0.117\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -1.7738     0.6526  -2.718\nP|C   0.4034     0.6428   0.627\n```\n\n\n:::\n:::\n\n\n\nFor the purposes of this post, we'll just take a look at the `Coefficients` table. The reference model here is the new Gemini 2.5 Pro. Negative coefficient estimates for a given model indicate that model is less likely to receive higher ratings than the new Gemini 2.5 Pro. Looking at the coefficients:\n\n* The old Gemini 2.5 Pro is roughly the same in its R coding capabilities as the new one.\n* While neither of the differences from the new Gemini 2.5 Pro are statistically significant, Claude Sonnet 3.7 and GPT o4-mini are neck-and-neck with each other as the SotA on this eval.\n\nIn short, from my perspective, there's not much to see here with this new Gemini release.\n\n---\n\n_Thank you to Max Kuhn for advising on the model-based analysis here._\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}