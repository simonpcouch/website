{
  "hash": "1a49455a7fb96e082c8072813bf4a00f",
  "result": {
    "markdown": "---\ntitle: \"Measuring elapsed time to fit with tidymodels\"\ndate: '2024-04-08'\ntags:\n  - tidymodels\n  - parsnip\n  - rstats\nsubtitle: \"The development versions of tidymodels packages now include tools to benchmark training time.\"\nimage: featured.png\nsummary: ''\n---\n\n\n> **tl;dr**: The development versions of tidymodels packages include methods for a [new extract function](https://workflows.tidymodels.org/dev/reference/extract-workflow.html), `extract_fit_time()`, that returns the time required to train a workflow. Pass `extract_fit_time()` as a control option while tuning and run `collect_extracts()` to see training times for resampled workflows. In this example, we can identify a modeling workflow that trains more than 10x faster than the most performant model with very little decrease in predictive performance.\n\n\nA year ago, [Emil](https://emilhvitfeldt.com/) put together [a](https://github.com/tidymodels/workflows/pull/191) [proof](https://github.com/tidymodels/recipes/pull/1071) [of](https://github.com/tidymodels/parsnip/pull/853) [concept](https://github.com/tidymodels/hardhat/pull/218) for a function that would return measurements of how long it took for different tidymodels objects to fit. This had been a longstanding feature request across many of our repositories. Then, we got pretty busy implementing [survival analysis](https://tidyverse.org/blog/2024/04/tidymodels-survival-analysis/) and [fairness metrics](https://www.tidyverse.org/blog/2024/03/tidymodels-fairness/) and only recently picked these changes back up to polish off the rough edges. We just merged them into the main developmental versions of the packages and are interested in hearing what folks think before they head off to CRAN!\n\nTo install the packages with these changes, use the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npak::pak(\n  paste0(\n    \"tidymodels/\", \n    c(\"workflows\", \"recipes\", \"parsnip\", \"hardhat\")\n  )\n)\n```\n:::\n\n\nNow, loading the tidymodels packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n## Getting started\n\nFor a simpler example, let's start off with just fitting a parsnip model. We'll use the `taxi` data from modeldata as an example, predicting whether a trip will result in a `tip` or not:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,000 × 7\n   tip   distance company                      local dow   month  hour\n   <fct>    <dbl> <fct>                        <fct> <fct> <fct> <int>\n 1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n 2 yes       0.88 City Service                 yes   Thu   Mar       8\n 3 yes      18.1  other                        no    Mon   Feb      18\n 4 yes      20.7  Chicago Independents         no    Mon   Apr       8\n 5 yes      12.2  Chicago Independents         no    Sun   Mar      21\n 6 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n 7 yes      17.5  Flash Cab                    no    Fri   Mar      12\n 8 yes      17.7  other                        no    Sun   Jan       6\n 9 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n10 yes       1.47 City Service                 no    Tue   Mar      14\n# ℹ 9,990 more rows\n```\n:::\n:::\n\n\nThe following code fits an XGBoost boosted tree with parsnip:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_fit <- fit(boost_tree(mode = \"classification\"), tip ~ ., taxi)\n```\n:::\n\n\nWith these new package versions, we now have access to a function called `extract_fit_time()` that will return the elapsed time to fit the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_fit_time(taxi_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  stage_id   elapsed\n  <chr>        <dbl>\n1 boost_tree   0.120\n```\n:::\n:::\n\n\nCouldn't we just wrap that whole `fit()` expression in `system.time()` and get the same thing, though?\n\nActually, no! `extract_fit_time()` returns the elapsed time to evaluate the *engine* fit, without tidymodels' overhead on top. The differences between `system.time()` and the value of `extract_fit_time()` are exactly where this new function will come in handy.\n\ntidymodels doesn't actually implement it's own training algorithms for boosted trees. Instead, the framework takes in code with a common input interface, translates that code to pass off to modeling *engines* which take care of the actual training, and then translates the output back to a common output interface. That process can be visualized something like this:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/translate_diagram.png){fig-alt='A graphic representing the parsnip interface. In order, step 1 \\'translate\\', step 2 \\'call\\', and step 3 \\'translate\\', outline the process of translating from the standardized tidymodels interface to an engine\\'s specific interface, calling the modeling engine, and translating back to the standardized tidymodels interface. Step 1 and step 3 are in green, while step 2 is in orange.' width=986}\n:::\n:::\n\n\nWhen viewed through the lens of elapsed time, the portion of the diagram in orange, labeled **Step 2)**, is what is measured by `extract_fit_time()`. The portions of the elapsed time shown in green, labeled **Steps 1)** and **3)**, are tidymodels' \"overhead.\" That could be measured as the difference between `system.time()` and `extract_fit_time()`. Let's calculate that difference exactly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_elapsed <- \n  system.time({\n    taxi_fit <- fit(boost_tree(mode = \"classification\"), tip ~ ., taxi)\n  })[[\"elapsed\"]]\n```\n:::\n\n\nJuxtaposing those two values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_elapsed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.119\n```\n:::\n\n```{.r .cell-code}\nextract_fit_time(taxi_fit)$elapsed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.113\n```\n:::\n:::\n\n\nThe total elapsed time of the fit is 0.119 seconds, and the portion of that elapsed time spent inside of XGBoost is 0.113 seconds. Said another way, the XGBoost fit itself accounts for 95% of the total time of this model fit.\n\n::: callout-note\nThe story is a bit more complicated for recipes, which also now have `extract_fit_time()` methods. We *do* implement the training routines for many of those ourselves, so the concept of \"overhead\" isn't so straightforward there. For those methods, fit times refer to the elapsed times of `prep()` and `bake()` while each step is trained.\n:::\n\nMeasuring the relative overhead of tidymodels is of interest to us as the developers, of course. More importantly, though, these tools can also be used to get a sense for how long different parts of a larger ML pipeline take compared to each other and choose models that fit and predict more quickly (as long as predictions are similarly performant).\n\n## Resampling a model\n\nAs an example, let's tune this XGBoost model using cross-validation. First, we'll split the taxi data up into training and testing sets, resampling the training set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntaxi_split <- initial_split(taxi, prop = 0.8, strata = tip)\ntaxi_train <- training(taxi_split)\ntaxi_test <- testing(taxi_split)\ntaxi_folds <- vfold_cv(taxi_train)\n```\n:::\n\n\nThen, defining a boosted tree model that tunes `learn_rate` and `trees`; boosted tree ensembles with higher `learn_rate`s might not perform as effectively, but they require fewer `trees` in the ensemble and thus train faster. We'd like to find the \"sweet spot\" of combinations of these parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt <- \n  boost_tree(\n    mode = \"classification\", \n    learn_rate = tune(), \n    trees = tune()\n  )\n```\n:::\n\n\nNow, we'll conduct a grid search for the best values of `learn_rate` and `trees`, passing the new `extract_fit_time()` function to the [control function `control_grid()`](https://tune.tidymodels.org/reference/control_grid.html). Every time we fit a model when resampling (`nrow(taxi_folds) * grid = 10 * 10 = 100` times!), we'll extract how long it took the engine to fit.\n\n\n::: {.cell hash='index_cache/html/bt_res_6a1a2ffcc08f16634532e53fb7e53eca'}\n\n```{.r .cell-code}\nbt_res <- \n  tune_grid(\n    bt, \n    tip ~ ., \n    taxi_folds, \n    control = control_grid(extract = extract_fit_time)\n  )\n```\n:::\n\n\nThis is what the `bt_res` object looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics          .notes           .extracts\n   <list>             <chr>  <list>            <list>           <list>   \n 1 <split [7200/800]> Fold01 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 2 <split [7200/800]> Fold02 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 3 <split [7200/800]> Fold03 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 4 <split [7200/800]> Fold04 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 5 <split [7200/800]> Fold05 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 6 <split [7200/800]> Fold06 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 7 <split [7200/800]> Fold07 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 8 <split [7200/800]> Fold08 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n 9 <split [7200/800]> Fold09 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n10 <split [7200/800]> Fold10 <tibble [30 × 6]> <tibble [0 × 4]> <tibble> \n```\n:::\n:::\n\n\n## Understanding fit time and performance\n\nEvery column in a tuning result that's prefixed with a `.` has a `collect_*()` function associated with it that helps to summarize that column. We'll use `collect_extracts()` to collect information on the extracts which are, in this case, elapsed fit times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_extracts <- collect_extracts(bt_res)\n\nbt_extracts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 5\n   id     trees learn_rate .extracts        .config              \n   <chr>  <int>      <dbl> <list>           <chr>                \n 1 Fold01   922    0.00161 <tibble [1 × 3]> Preprocessor1_Model01\n 2 Fold01  1804    0.00293 <tibble [1 × 3]> Preprocessor1_Model02\n 3 Fold01    81    0.00394 <tibble [1 × 3]> Preprocessor1_Model03\n 4 Fold01   445    0.00886 <tibble [1 × 3]> Preprocessor1_Model04\n 5 Fold01  1074    0.0171  <tibble [1 × 3]> Preprocessor1_Model05\n 6 Fold01   347    0.0218  <tibble [1 × 3]> Preprocessor1_Model06\n 7 Fold01  1486    0.0393  <tibble [1 × 3]> Preprocessor1_Model07\n 8 Fold01  1391    0.0604  <tibble [1 × 3]> Preprocessor1_Model08\n 9 Fold01   788    0.111   <tibble [1 × 3]> Preprocessor1_Model09\n10 Fold01  1609    0.203   <tibble [1 × 3]> Preprocessor1_Model10\n# ℹ 90 more rows\n```\n:::\n:::\n\n\nUnnesting on `.extracts` so we can see some of those timings:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_extracts_unnested <- bt_extracts %>% unnest(cols = \".extracts\")\n\nbt_extracts_unnested\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 7\n   id     trees learn_rate stage    process_id elapsed .config              \n   <chr>  <int>      <dbl> <chr>    <chr>        <dbl> <chr>                \n 1 Fold01   922    0.00161 workflow workflow     4.94  Preprocessor1_Model01\n 2 Fold01  1804    0.00293 workflow workflow     9.54  Preprocessor1_Model02\n 3 Fold01    81    0.00394 workflow workflow     0.444 Preprocessor1_Model03\n 4 Fold01   445    0.00886 workflow workflow     2.36  Preprocessor1_Model04\n 5 Fold01  1074    0.0171  workflow workflow     5.40  Preprocessor1_Model05\n 6 Fold01   347    0.0218  workflow workflow     1.8   Preprocessor1_Model06\n 7 Fold01  1486    0.0393  workflow workflow     7.34  Preprocessor1_Model07\n 8 Fold01  1391    0.0604  workflow workflow     6.99  Preprocessor1_Model08\n 9 Fold01   788    0.111   workflow workflow     3.87  Preprocessor1_Model09\n10 Fold01  1609    0.203   workflow workflow     7.99  Preprocessor1_Model10\n# ℹ 90 more rows\n```\n:::\n:::\n\n\nThe times to fit vary quite a bit between workflows, to say the least:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(bt_extracts_unnested) +\n  aes(x = elapsed) +\n  geom_histogram(binwidth = 1, boundary = 0) +\n  scale_x_continuous(\n    breaks = seq(0, max(bt_extracts_unnested$elapsed) + 1, 1L)\n  ) +\n  labs(x = \"Elapsed fit time\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-un-1.png){fig-align='center' fig-alt='A ggplot histogram showing elapsed fit times varying from almost 0 to around 10. The distribution is uniform; each bin contains nearly the same count of values.' width=672}\n:::\n:::\n\n\nAlmost all of this variation in elapsed time is explained by the value of `trees`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(bt_extracts_unnested) +\n  aes(x = elapsed, y = trees) +\n  geom_jitter() +\n  labs(x = \"Elapsed fit time\", y = \"# of trees\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-bi-1.png){fig-align='center' fig-alt='A ggplot dotplot showing a very strong, linear, positive correlation between elapsed fit time and number of trees.' width=672}\n:::\n:::\n\n\nThe \"clumps\" of points are the 10 different unique combinations of `trees` and `learn_rate`s that are fitted on 10 different resamples; identical hyperparameters lead to very similar fit times, in this case.\n\nIn use cases where we will go on to resample/train this model many times (such as in iterative search or continuous training) it may be advantageous for us to pick a model that fits as quickly as possible as long as it has comparable performance with the overall most performant model. We can integrate the information from `collect_metrics()` with `collect_extracts()` to see if there any quick-fitting models with comparable performance. First, collecting metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_metrics <- collect_metrics(bt_res)\n\nbt_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 × 8\n   trees learn_rate .metric     .estimator   mean     n  std_err .config        \n   <int>      <dbl> <chr>       <chr>       <dbl> <int>    <dbl> <chr>          \n 1   922    0.00161 accuracy    binary     0.924     10 0.00214  Preprocessor1_…\n 2   922    0.00161 brier_class binary     0.0784    10 0.00128  Preprocessor1_…\n 3   922    0.00161 roc_auc     binary     0.639     10 0.0112   Preprocessor1_…\n 4  1804    0.00293 accuracy    binary     0.924     10 0.00218  Preprocessor1_…\n 5  1804    0.00293 brier_class binary     0.0693    10 0.00160  Preprocessor1_…\n 6  1804    0.00293 roc_auc     binary     0.628     10 0.0126   Preprocessor1_…\n 7    81    0.00394 accuracy    binary     0.924     10 0.00212  Preprocessor1_…\n 8    81    0.00394 brier_class binary     0.165     10 0.000467 Preprocessor1_…\n 9    81    0.00394 roc_auc     binary     0.636     10 0.0110   Preprocessor1_…\n10   445    0.00886 accuracy    binary     0.923     10 0.00211  Preprocessor1_…\n# ℹ 20 more rows\n```\n:::\n:::\n\n\nJoining this output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_metrics_extracts <-\n  left_join(\n    # metrics, summarized across resamples\n    bt_metrics,\n    # summarize fit times across resamples\n    summarize(\n      bt_extracts_unnested, \n      elapsed = mean(elapsed),\n      .by = c(trees, learn_rate, .config)\n    ),\n    by = c(\"trees\", \"learn_rate\", \".config\")\n  ) %>%\n  relocate(elapsed, .before = everything())\n\nbt_metrics_extracts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 × 9\n   elapsed trees learn_rate .metric     .estimator   mean     n  std_err .config\n     <dbl> <int>      <dbl> <chr>       <chr>       <dbl> <int>    <dbl> <chr>  \n 1   4.95    922    0.00161 accuracy    binary     0.924     10 0.00214  Prepro…\n 2   4.95    922    0.00161 brier_class binary     0.0784    10 0.00128  Prepro…\n 3   4.95    922    0.00161 roc_auc     binary     0.639     10 0.0112   Prepro…\n 4   9.55   1804    0.00293 accuracy    binary     0.924     10 0.00218  Prepro…\n 5   9.55   1804    0.00293 brier_class binary     0.0693    10 0.00160  Prepro…\n 6   9.55   1804    0.00293 roc_auc     binary     0.628     10 0.0126   Prepro…\n 7   0.441    81    0.00394 accuracy    binary     0.924     10 0.00212  Prepro…\n 8   0.441    81    0.00394 brier_class binary     0.165     10 0.000467 Prepro…\n 9   0.441    81    0.00394 roc_auc     binary     0.636     10 0.0110   Prepro…\n10   2.37    445    0.00886 accuracy    binary     0.923     10 0.00211  Prepro…\n# ℹ 20 more rows\n```\n:::\n:::\n\n\nNow, plotting:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(bt_metrics_extracts) +\n  aes(x = elapsed, y = mean) +\n  geom_point() +\n  facet_wrap(~.metric, scales = \"free\") +\n  labs(x = \"Elapsed fit time\", y = \"Metric value\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-perf-vs-time-1.png){fig-align='center' fig-alt='Three faceted ggplot dotplots, with different performance metrics in each plot, show that metric values don\\'t correlate well with elapsed time to fit in this case.' width=672}\n:::\n:::\n\n\nIn this context, we see that w.r.t `accuracy()` and `brier_class()`, there's not a clear relationship between the elapsed fit time and model performance. w.r.t. `roc_auc()`, longer-fitting models seem *less* performant.\n\n::: callout-note\nBy most measures, none of these models are particularly performant. Alas.🙂\n:::\n\n## Model selection\n\nWe can use the `select_best()` function from tune to pick the most performant model, without regard for elapsed fit time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_most_performant <- select_best(bt_res, metric = \"roc_auc\")\n\nbt_most_performant\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  trees learn_rate .config              \n  <int>      <dbl> <chr>                \n1   922    0.00161 Preprocessor1_Model01\n```\n:::\n:::\n\n\nUsing `roc_auc()`, ignoring fit time, we'd choose the model configuration Preprocessor1_Model01, corresponding to an `roc_auc()` of 0.639.\n\nInstead, we could choose the quickest-fitting model with a performance value within one standard error of the most performant model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_fit <-\n  bt_metrics_extracts %>%\n  filter(.metric == \"roc_auc\") %>%\n  arrange(desc(mean)) %>%\n  select(mean, std_err, elapsed, .config)\n\nbest_speedy_fit <- \n  bt_metrics_extracts %>%\n  filter(.metric == \"roc_auc\") %>%\n  filter(mean >= best_fit$mean - best_fit$std_err) %>%\n  arrange(elapsed) %>%\n  slice(1)\n\nbest_speedy_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n  elapsed trees learn_rate .metric .estimator  mean     n std_err .config       \n    <dbl> <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>         \n1   0.441    81    0.00394 roc_auc binary     0.636    10  0.0110 Preprocessor1…\n```\n:::\n:::\n\n\n::: callout-note\nExisting tidymodels users may recognize this selection workflow as an ad-hoc version of `select_by_one_std_err()`.\n:::\n\nNow, integrating our knowledge of fit time, we'd choose the model configuration Preprocessor1_Model03, corresponding to an `roc_auc()` of 0.636. The mean elapsed fit time for this model is 0.441 seconds, compared a mean elapsed fit time for the most performant model of 4.951 seconds, **a speedup of 11.2x!** Of course, this isn't as much of an issue when model fits are as quick as they are here, but for more complex fits on larger datasets, an order of magnitude is a gamechanger.\n\n## What do you think?\n\nThese changes haven't yet made it to the CRAN versions of tidymodels packages (and likely won't for a good bit). Before they hit CRAN, we'd love to hear your thoughts on the interface and take any suggestions on how we might improve the interface.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}