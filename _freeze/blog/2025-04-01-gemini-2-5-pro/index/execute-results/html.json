{
  "hash": "2434254568a80f38c4107677be64b7a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How Good Is Gemini 2.5 Pro at Writing R Code?\"\ndate: '2025-04-02'\ntags:\n  - ai\n  - gemini\n  - rinspect\n  - rstats\nsubtitle: \"Since Gemini 2.5 Pro's release last week, I've been seeing a lot of hype claiming that the model is the new state of the art. How well does it know R?\"\nimage: featured.png\nsummary: ''\n---\n\n\n\n\n\nSince Gemini 2.5 Pro Experimental's [release](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) last week, I've been seeing a [lot](https://thezvi.substack.com/p/gemini-25-is-the-new-sota?utm_source=post-email-title&publication_id=573100&post_id=160014258&utm_campaign=email-post-title&isFreemail=true&r=21np6y&triedRedirect=true&utm_medium=email) [of](https://simonwillison.net/2025/Mar/25/gemini/) [hype](https://www.youtube.com/watch?v=A0V4km88tFc&t=700s) claiming that the model is the new state of the art. I've been wondering—how good is this model at writing R code?\n\nAs it happens, I've been working on [a new tool](https://simonpcouch.github.io/rinspect/) for large language model evaluation in R. The package is still pretty early on in it's development, but I figured this might be a good opportunity to kick the tires publicly.\n\n> _tl;dr_:\n> \n> * Gemini 2.5 Pro is definitely an improvement over Gemini 2.0 Flash.\n> * Gemini 2.5 Pro seems \"up there\" with Claude Sonnet 3.7 and o3-Mini High.\n> * If Gemini 2.5 Pro ends up being as cheap as Gemini 2.0 Flash over the API, I would seriously consider transitioning to Gemini for most of what I currently use Claude for.\n\n\n## Introducing rinspect\n\nrinspect is an R port of the widely adopted Python framework [Inspect](https://inspect.ai-safety-institute.org.uk/). While the package doesn't integrate with Inspect directly, it allows users to interface with the [Inspect log viewer](https://inspect.ai-safety-institute.org.uk/log-viewer.html) and shares much of its grammar and philosophy.\n\nrinspect describes LLM evals in three core components:\n\n1)  **Datasets** contain a set of labelled samples. Datasets are just a tibble with columns `input` and `target`, where `input` is a prompt and `target` is either literal value(s) or grading guidance.\n2)  **Solvers** evaluate the `input` in the dataset and produce a final result (hopefully) approximating `target`. In rinspect, the simplest solver is just an ellmer chat (e.g. [`ellmer::chat_claude()`](https://ellmer.tidyverse.org/reference/chat_claude.html)) wrapped in `generate()`, i.e. `generate(ellmer::chat_claude())`), which will call the [Chat object's `$chat()` method](https://ellmer.tidyverse.org/reference/Chat.html#method-Chat-chat) and return whatever it returns.\n3)  **Scorers** evaluate the final output of solvers. They may use text comparisons, model grading, or other custom schemes to determine how well the solver approximated the `target` based on the `input`.\n\nIn this blog post, we'll apply a solver powered by four different models to a dataset of R coding problems. Google's Gemini 2.5 Pro Experimental will be the first. We'll compare that model to its previous generation, Gemini 2.0 Flash. Otherwise, it's a bit difficult to determine reasonable \"peers\" to the model without knowing what the model's API cost will be, but we could use [OpenAI's o3-mini-high](https://openai.com/index/openai-o3-mini/) and [Anthropic's Claude Sonnet 3.7 Extended Thinking](https://www.anthropic.com/news/claude-3-7-sonnet) as peer models since Google used them in the linked release post. We'll then use the same scorer to evaluate how well each model performed.\n\n::: callout-important\nI was unable to get Claude Sonnet 3.7's \"thinking\" enabled properly with ellmer; an apples-to-apples comparison would use that setting.\n:::\n\nIn ellmer, here's how we define those model connections:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\n\ngemini_2_5_pro <- chat_gemini(model = \"gemini-2.5-pro-exp-03-25\")\ngemini_2_0_flash <- chat_gemini(model = \"gemini-2.0-flash\")\no3_mini_high <- chat_openai(\n  model = \"o3-mini\", \n  api_args = list(reasoning_effort = \"high\")\n)\nclaude_sonnet_3_7 <- chat_claude(model = \"claude-3-7-sonnet-latest\")\n```\n:::\n\n\n\nNote that I needed to configure a `GOOGLE_API_KEY`, `OPENAI_API_KEY`, and `ANTHROPIC_API_KEY` to connect to these models, respectively. Gemini 2.5 Pro Experimental is currently free but heavily rate-limited. Here are the prices per million tokens for the other models^[Altogether, the data underlying this blog post took around $3 USD to generate.]:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  Name              Input Output\n  <chr>             <chr> <chr> \n1 Gemini 2.0 Flash  $0.10 $0.40 \n2 ChatGPT o3-mini   $1.10 $4.40 \n3 Claude 3.7 Sonnet $3.00 $15.00\n```\n\n\n:::\n:::\n\n\n\n\n## An R Eval dataset\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rinspect)\nlibrary(tidyverse)\n```\n:::\n\n\n\nWe'll use a dataset that ships with rinspect called `are`, or \"An R Eval.\" From the `are` docs:\n\n> An R Eval is a dataset of challenging R coding problems. Each `input` is a question about R code which could be solved on first-read only by human experts and, with a chance to read documentation and run some code, by fluent data scientists. Solutions are in `target` and enable a fluent data scientist to evaluate whether the solution deserves full, partial, or no credit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(are)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nRows: 28\nColumns: 7\n$ title     <chr> \"after-stat-bar-heights\", \"conditional-grouped-sum…\n$ input     <chr> \"This bar chart shows the count of different cuts …\n$ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n…\n$ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\",…\n$ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"…\n$ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/…\n$ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"…\n````\n\n\n:::\n:::\n\n\n\nAt a high level:\n\n-   `title`: A unique identifier for the problem.\n-   `input`: The question to be answered.\n-   `target`: The solution, often with a description of notable features of a correct solution.\n-   `domain`, `task`, and `knowledge` are pieces of metadata describing the kind of R coding challenge.\n-   `source`: Where the problem came from, as a URL. Many of these coding problems are adapted \"from the wild\" and include the kinds of context usually available to those answering questions.\n\nFor the purposes of actually carrying out the initial evaluation, we're specifically interested in the `input` and `target` columns. Let's print out the first entry in full so you can get a taste of a typical problem in this dataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are$input[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nThis bar chart shows the count of different cuts of diamonds,\nand each bar is stacked and filled according to clarity:\n\n```\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity))\n```\n\nCould you change this code so that the proportion of diamonds\nwith a given cut corresponds to the bar height and not the\ncount? Each bar should still be filled according to clarity.\n````\n\n\n:::\n:::\n\n\n\nHere's the suggested solution:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are$target[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nPreferably:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = after_stat(count) /\nsum(after_stat(count)), fill = clarity))\n```\n\nThe dot-dot notation (`..count..`) was deprecated in ggplot2\n3.4.0, but it still works:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill =\nclarity))\n```\n\nSimply setting `position = \"fill\" will result in each bar\nhaving a height of 1 and is not correct.\n````\n\n\n:::\n:::\n\n\n\nFor now, `are` was publicly shared after the knowledge cutoff of each of these models, so the answers to these questions (likely) aren't yet incorporated into the models' weights.\n\n## Evaluating Gemini\n\nLLM evaluation with rinspect happens in two main steps:\n\n**First**, use `Task$new()` to situate a dataset, solver, and scorer in a `Task`. [Tasks](https://simonpcouch.github.io/rinspect/reference/Task.html) are R6 objects that define important methods and data structures for LLM evaluation. Below, I use `generate()` as a solver, currently the only built-in solver supplied by the package. Think of it like Chat objects' `$chat()` method with some bells and whistles—parallel requests, some nice progress functionality, and lots of logging. `generate()` returns a function that has one argument, `solver_chat`, which takes an ellmer Chat; you can set a default Chat by supplying it to `generate()` itself. The scorer, `model_graded_qa()`, uses model grading (or \"LLM-as-a-judge\") to score the solver's responses. Different models exhibit different behaviors as judges, so I use the same model (Claude Sonnet 3.7) as the judge regardless of which model is being evaluated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task <- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  name = \"An R Eval\"\n)\n\nare_task\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn evaluation task An R Eval.\n```\n\n\n:::\n:::\n\n\n\n**Second**, use `Task$eval()` to evaluate the solver, evaluate the scorer, and then explore a persistent log of the results in an interactive viewer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task_gemini_2_5 <- are_task$clone()\nare_task_gemini_2_5$eval(\n  solver_chat = gemini_2_5_pro,\n  epochs = 3\n)\n```\n:::\n\n\n\n:::callout-note\nYou can also run `$eval()` step-by-step, if you want; `$eval()` just calls `$solve()`, `$score()`, `$log()`, and `$view()` in sequence.\n:::\n\n\n\n\n\n\n\n\n\nAfter evaluation, the task contains information from the solving and scoring steps. Here's what the model responded to that first question with:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are_task_gemini_2_5$samples$result[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nOkay, you can achieve this by changing the `position` argument\nwithin `geom_bar`. The default is `\"stack\"`, which stacks the\nraw counts. You want `\"fill\"`, which stacks the proportions so\nthat each bar sums to 1 (or 100%).\n\nHere's the modified code:\n\n```R\n# Load the ggplot2 library if you haven't already\n# library(ggplot2)\n# The diamonds dataset is built into ggplot2\n\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity), position =\n\"fill\") +\nlabs(y = \"Proportion\", title = \"Proportion of Clarity Levels\nwithin each Diamond Cut\") # Optional: Add informative labels\n```\n\n**Explanation:**\n\n1.  `ggplot(data = diamonds)`: Initializes the plot using the\n`diamonds` dataset.\n2.  `geom_bar(...)`: Specifies that you want a bar chart.\n3.  `mapping = aes(x = cut, fill = clarity)`: Maps the `cut`\nvariable to the x-axis and the `clarity` variable to the fill\ncolor of the bars.\n4.  `position = \"fill\"`: This is the key change. It tells\n`geom_bar` to calculate the count for each `fill` category\n(clarity) within each `x` category (cut), but then normalizes\nthese counts so that the total height of each bar represents 1\n(or 100%). The segments within the bar then show the\n*proportion* of each clarity level for that specific cut.\n5.  `labs(y = \"Proportion\", ...)`: (Optional but recommended)\nChanges the default y-axis label from \"count\" to \"Proportion\"\nto accurately reflect what the axis now represents, and adds a\ntitle.\n````\n\n\n:::\n:::\n\n\n\nThen, since we've supplied `model_graded_qa()` as the scorer, Claude Sonnet 3.7 will be supplied the input question, the grading guidance, and the solver's output, and asked to determine whether the solution is incorrect, partially correct, or correct. Here's Claude's grading transcript for the solver's first answer:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are_task_gemini_2_5$samples$scorer_chat[[1]]$last_turn()@text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nI need to assess whether the submitted answer meets the\ncriterion, which specifies that the proportion of diamonds\nwith a given cut should correspond to the bar height.\n\nThe criterion clearly states that the correct approach is to\nuse:\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = after_stat(count) /\nsum(after_stat(count)), fill = clarity))\n```\n\nThe submission instead recommends using:\n```\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity), position =\n\"fill\")\n```\n\nThe criterion explicitly states that \"Simply setting `position\n= \"fill\"` will result in each bar having a height of 1 and is\nnot correct.\" This is because `position = \"fill\"` normalizes\nwithin each cut category, making each bar the same height\n(1.0), with the segments showing the proportions of clarity\nwithin each cut.\n\nIn contrast, the approach specified in the criterion uses\n`after_stat(count) / sum(after_stat(count))` to calculate the\nproportion of each cut relative to the total number of\ndiamonds. This ensures that the bar heights represent the\nproportion of diamonds with each cut, while still maintaining\nthe clarity breakdown within each bar.\n\nSince the submission provides a solution that contradicts what\nthe criterion explicitly identifies as incorrect, the answer\ndoes not meet the requirements.\n\nGRADE: I\n````\n\n\n:::\n:::\n\n\n\nrinspect ships with the Inspect Log Viewer, a small .js app that allows you to interactively explore evaluation logs. Especially the first few times you run an eval, the tool is super helpful for uncovering unexpected behavior in solving and scoring. I've embedded the viewer in this post so you can check out the problems in *An R Eval* and how effectively Gemini 2.5 Pro Experimental handled them:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<iframe src=\"/assets/2025-04-01-gemini-2-5-pro/viewer/index.html\" width=\"100%\" height=\"600px\" style=\"border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);\"></iframe>\n```\n\n:::\n:::\n\n\n\nI'd encourage you to poke around in this app! You'll certainly see some bugs that I've still yet to work out and some surprising behavior from the scorer, but there's lots to be learned about how these models work from evaluation logs.\n\n## Evaluating Comparison Models\n\nWe can evaluate the remaining models by cloning the original task and running `$eval()` with a new solver chat. First, to evaluate the previous Gemini generation, Gemini 2.0 Flash:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task_gemini_2_0 <- are_task$clone()\nare_task_gemini_2_0$eval(\n  solver_chat = gemini_2_0_flash,\n  epochs = 3\n)\n```\n:::\n\n\n\n\n\nFor o3-mini high:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task_o3_mini_high <- are_task$clone()\nare_task_o3_mini_high$eval(\n  solver_chat = o3_mini_high,\n  epochs = 3\n)\n```\n:::\n\n\n\n\n\nFinally, for Claude Sonnet 3.7:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task_claude_sonnet_3_7 <- are_task$clone()\nare_task_claude_sonnet_3_7$eval(\n  solver_chat = claude_sonnet_3_7,\n  epochs = 3\n)\n```\n:::\n\n\n\n\n\n\n\nI've also situated the logs for both of the above evaluations in the above app—just click the three stacked bars in the top right of the app to check out the logs for 2.0 Flash, o3-mini and Sonnet 3.7.\n\n## Analysis\n\nAt evaluation time, rinspect does a naive accuracy calculation that you can see displayed in the app, but in general is quite restrained in its analysis functionality. Instead, the package aims to get analysts to Happy Data Frame Land as quickly as possible using `inspect_bind()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval <- \n  inspect_bind(\n    `Gemini 2.5 Pro Experimental` = are_task_gemini_2_5,\n    `Gemini 2.0 Flash` = are_task_gemini_2_0,\n    `o3-Mini High` = are_task_o3_mini_high,\n    `Claude Sonnet 3.7 (No Thinking)` = are_task_claude_sonnet_3_7,\n  ) %>%\n  rename(model = task) %>%\n  mutate(\n    model = factor(model, levels = c(\n      \"Gemini 2.5 Pro Experimental\", \n      \"Gemini 2.0 Flash\", \n      \"o3-Mini High\", \n      \"Claude Sonnet 3.7 (No Thinking)\"\n    ))\n  )\n\nare_eval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 336 × 5\n   model                          id epoch score metadata         \n   <fct>                       <int> <int> <ord> <list>           \n 1 Gemini 2.5 Pro Experimental     1     1 I     <tibble [1 × 11]>\n 2 Gemini 2.5 Pro Experimental     1     2 I     <tibble [1 × 11]>\n 3 Gemini 2.5 Pro Experimental     1     3 I     <tibble [1 × 11]>\n 4 Gemini 2.5 Pro Experimental     2     1 I     <tibble [1 × 11]>\n 5 Gemini 2.5 Pro Experimental     2     2 P     <tibble [1 × 11]>\n 6 Gemini 2.5 Pro Experimental     2     3 I     <tibble [1 × 11]>\n 7 Gemini 2.5 Pro Experimental     3     1 C     <tibble [1 × 11]>\n 8 Gemini 2.5 Pro Experimental     3     2 C     <tibble [1 × 11]>\n 9 Gemini 2.5 Pro Experimental     3     3 P     <tibble [1 × 11]>\n10 Gemini 2.5 Pro Experimental     4     1 I     <tibble [1 × 11]>\n# ℹ 326 more rows\n```\n\n\n:::\n:::\n\n\n\nIn this dataset, each row represents a single time a solver is invoked to answer a question:\n\n-   `model` gives the model used to solve a given question\n\n-   `id` gives the question id\n\n-   `epoch` identifies the run/resample of the given question\n\n-   `scores` shows whether the scoring model (Claude Sonnet 3.7) identified the solver's answer as Incorrect, Partially Correct, or Correct. It's an ordinal factor with `I < P < C`.\n\n-   `metadata` is a list column containing just about all of the information that rinspect collects during the evaluation process.\n\nWe're interested in which of these three models are right more often. We have 28 unique questions, each resampled across 3 epochs for each of 4 models. For a cursory analysis, we could do the canonical Bar Chart Dodged By Model visualization:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval %>%\n  mutate(\n    score = fct_rev(score),\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    )\n  ) %>%\n  ggplot(aes(x = score, fill = model)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_manual(values = c(\n    \"Gemini 2.5 Pro Experimental\" = \"#5CB85C\", \n    \"Gemini 2.0 Flash\" = \"#8BC34A\",\n    \"o3-Mini High\" = \"#69b1cf\", \n    \"Claude Sonnet 3.7 (No Thinking)\" = \"#d6941a\"\n  )) +\n  labs(\n    x = \"Score\", y = \"Count\", \n    title = \"An R Eval\",\n    subtitle = \n      \"The newest Gemini release is a substantial improvement on the previous\\ngeneration and is a contender with Claude and o3-mini on R coding problems.\"\n  ) +\n  theme(plot.subtitle = element_text(face = \"italic\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-are-eval-1.png){fig-alt='A ggplot2 bar plot showing the counts of correct, partially correct, and incorrect answers from various LLMs on R coding problems. Generally, Gemini Pro 2.5, o3-mini High, and Claude Sonnet 3.7 get around half of the problems correct. Gemini 2.0 Flash is more like a third.' width=100%}\n:::\n:::\n\n\n\nCould the differences we're seeing be attributed to random noise, though? We can use a hierarchical modeling technique called a mixed model to model the probability of each score (i.e., correct, etc.) as a function of the LLM. In this case, observations are not independent; some questions may be harder than others, and we're repeating each question multiple times since we've set `epochs = 3`. A random intercept on the question `id` can help account for this variation. Since `score` is ordinal, we use a cumulative link mixed model rather than the usual suspect `lme4::glmer()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ordinal)\n\nare_mod <- clmm(score ~ model + (1|id), data = are_eval)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(are_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  336  -257.80 527.59 213(1039) 1.59e-04 4.6e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 7.561    2.75    \nNumber of groups:  id 28 \n\nCoefficients:\n                                     Estimate Std. Error z value\nmodelGemini 2.0 Flash                -1.48455    0.38106  -3.896\nmodelo3-Mini High                    -0.29653    0.37041  -0.801\nmodelClaude Sonnet 3.7 (No Thinking) -0.04314    0.36520  -0.118\n                                     Pr(>|z|)    \nmodelGemini 2.0 Flash                9.79e-05 ***\nmodelo3-Mini High                       0.423    \nmodelClaude Sonnet 3.7 (No Thinking)    0.906    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -1.4919     0.6059  -2.462\nP|C   0.2869     0.5975   0.480\n```\n\n\n:::\n:::\n\n\n\nGemini 2.0 Flash performs notably worse than the reference Gemini 2.5 Pro Experimental model (-1.485, p < 0.001), with an odds ratio of 0.23, indicating substantially lower odds of achieving higher rating categories. 2.5 Pro's peers, o3-Mini High and Claude Sonnet 3.7, show largely comparable performance to 2.5 Pro. The substantial random effect variance indicates that question difficulty varies considerably and has a strong influence on performance. With threshold values of -1.49 (I|P) and 0.29 (P|C), the model suggests a larger gap between Incorrect and Partially Correct responses than between Partially Correct and Correct ones.\n\n:::callout-note\nKeep an eye out for a rinspect vignette with a more thorough model-based analysis than this one in the near future.\n:::\n\nBased on those results, it seems like we can make a few conclusions:\n\n* Gemini 2.5 Pro is certainly an improvement over Gemini 2.0 Flash.\n* Gemini 2.5 Pro seems \"up there\" with Claude Sonnet 3.7 and o3-Mini High when it comes to writing R code.\n* _An R Eval_, and the scoring functionality I associate it with, definitely needs some more work. :)\n\nAgain, Google has yet to announce API pricing for Gemini 2.5 Pro. If Gemini 2.5 Pro's API is comparable to Gemini 2.0 Flash's, I might agree that we have a new SoTA on our hands! In that case, I might consider switching my go-to coding assistance model from Claude more seriously.\n\n----\n\n_Thank you to Max Kuhn for advising on the model-based analysis here, and to Hadley Wickham for suggesting that I also evaluate against the previous Gemini model. Grateful for your mentorship._\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}