{
  "hash": "da5c19f52280234d43e597b3233f6883",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Evaluating Gemini 2.5 Flash on R coding tasks\"\ndate: '2025-05-21'\ntags:\n  - ai\n  - vitals\n  - rstats\nsubtitle: \"Google's Gemini 2.5 Pro release really made a splash last month. They've just announced an update to 2.5 Flash, a faster and cheaper model.\"\nimage: featured.png\nsummary: ''\n---\n\n\n\n\n\nGoogle's preview of their Gemini 2.5 Pro model has [really made a splash](https://www.simonpcouch.com/blog/2025-05-07-gemini-2-5-pro-new/). The model has become many folks' daily driver, and I've started to see \"What about Gemini?\" in the comments of each of these blog posts if they don't explicitly call out the model series in the title. Yesterday, Google announced an update of the preview for Gemini 2.5 Flash, a smaller and cheaper version of 2.5 Pro. \n\nIn the [model card](https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025), Google juxtaposes Gemini 2.5 Flash with OpenAI's o4-mini:\n\n<div style=\"text-align: center;\">\n<img src=\"juxtapose.png\" style=\"border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2); width: 600px; height: auto;\" />\n</div>\n\nThis comparison especially caught my eye, given that [o4-mini is the current leader](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/) in the class of cheap, snappy thinking models on an R coding evaluation I've been running the last few months. The proposition seems to be \"o4-mini-ish performance at a fraction of the price.\"\n\n<a href=\"https://vitals.tidyverse.org/\"><img src=\"vitals.png\" alt=\"The hex sticker for the vitals package: a teddy bear in blue scrubs happily holding a stethoscope.\" align=\"right\" height=\"240\"/></a>\n\nIn this post, I'll use the [vitals package](https://vitals.tidyverse.org/) to compare Gemini 2.5 Flash against several other models: \n\n* Gemini 2.0 Flash, the previous generation of this series\n* Gemini 2.5 Pro, the more performant and expensive version of the model\n* GPT o4-mini, supposedly a peer in performance and a [leader](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/) on this eval in the class of cheap and snappy reasoning models\n* Claude 3.7 Sonnet, my daily driver for coding assistance\n\n> tl;dr\n>\n> * 2.5 Flash's performance is really impressive for its price point and is a marked improvement over its previous generation.\n> * o4-mini does show significantly stronger performance on this eval (though is notably more expensive).\n> * Unlike Claude 3.7 Sonnet, enabling thinking with Gemini 2.5 Flash resulted in a marked increase in performance.\n> * Gemini 2.5 Flash with thinking disabled is [nearly indistinguishable](https://www.simonpcouch.com/blog/2025-04-15-gpt-4-1/) from GPT 4.1-nano on this eval. 2.5 Flash reached an accuracy of 43.6% at a cost of $0.15/m input, $0.60/m output, which 4.1-nano scored 44.2% at a cost of $0.10/m input, $0.40/m output. 4.1-nano continues to pack the greatest punch in the budget, non-thinking price point.\n> * I'm starting to think about an initial CRAN release of vitals. Please do [give it a whir](https://vitals.tidyverse.org/) and let me know if you have any feedback.\n\n## Setting up the evaluation\n\nLet's start by defining our model connections using ellmer:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\n\n# thinking is enabled by default, and can be disabled by\n# with a magic incantation\n# https://ai.google.dev/gemini-api/docs/thinking\ngemini_2_5_flash_thinking <- chat_google_gemini(\n  model = \"gemini-2.5-flash-preview-05-20\"\n)\n\ngemini_2_5_flash_non_thinking <- chat_google_gemini(\n  model = \"gemini-2.5-flash-preview-05-20\",\n  api_args = list(\n    generationConfig = list(\n      thinkingConfig = list(\n        thinkingBudget = 0\n        )\n      )\n    )\n)\n\ngemini_2_0_flash <- chat_google_gemini(\n  model = \"gemini-2.0-flash\"\n)\ngemini_2_5_pro <- chat_google_gemini(\n  model = \"gemini-2.5-pro-preview-05-06\"\n)\n\ngpt_o4_mini <- chat_openai(model = \"o4-mini-2025-04-16\")\n\n# note that i don't enable thinking here; thinking \n# doesn't seem to have an effect for claude on this \n# eval: https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/\nclaude_sonnet_3_7 <- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\n```\n:::\n\n\n\nNote that I needed to configure `GOOGLE_API_KEY`, `ANTHROPIC_API_KEY`, and `OPENAI_API_KEY` environment variables to connect to these models. The pricing for these models varies considerably:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  Name                            Input Output\n  <chr>                           <chr> <chr> \n1 Gemini 2.5 Flash (Thinking)     $0.15 $3.50 \n2 Gemini 2.5 Flash (Non-thinking) $0.15 $0.60 \n3 Gemini 2.0 Flash                $0.10 $0.40 \n4 Gemini 2.5 Pro                  $1.25 $10.00\n5 GPT o4-mini                     $1.10 $4.40 \n6 Claude 3.7 Sonnet               $3.00 $15.00\n```\n\n\n:::\n:::\n\n\n\nGemini 2.5 Flash has a thinking and non-thinking mode, where thinking tokens are not surfaced to the user but output tokens are charged at a higher rate. With thinking enabled (as shown on the model card), Gemini 2.5 Flash's output tokens are priced somewhat similarly to o4-mini.\n\nGemini 2.5 Pro, Gemini 2.5 Flash (Thinking), and GPT o4-mini are reasoning models, and thus will use more tokens than non-reasoning models. While Claude 3.7 Sonnet has a reasoning mode that could be enabled here, I haven't done so for this eval as it [doesn't seem to make a difference](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/) for performance on this eval.\n\nLet's set up a task that will evaluate each model using the `are` dataset from vitals:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task <- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn evaluation task An-R-Eval.\n```\n\n\n:::\n:::\n\n\n\n:::callout-note\nSee my [first post on Gemini 2.5 Pro](https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/#an-r-eval-dataset) for a more thorough description of this evaluation.\n:::\n\n## Running the evaluations\n\nFirst, we'll evaluate our reference model, Gemini 2.5 Flash with thinking enable:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gemini_2_5_flash_thinking <- are_task$clone()\nare_gemini_2_5_flash_thinking$eval(\n  solver_chat = gemini_2_5_flash_thinking\n)\n```\n:::\n\n\n\n\n\n\nFrom here, it's pretty rote. The same model without thinking enabled:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gemini_2_5_flash_non_thinking <- are_task$clone()\nare_gemini_2_5_flash_non_thinking$eval(\n  solver_chat = gemini_2_5_flash_non_thinking\n)\n```\n:::\n\n\n\n\n\nNow for the other Gemini models:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gemini_2_0_flash <- are_task$clone()\nare_gemini_2_0_flash$eval(solver_chat = gemini_2_0_flash)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gemini_2_5_pro <- are_task$clone()\nare_gemini_2_5_pro$eval(solver_chat = gemini_2_5_pro)\n```\n:::\n\n\n\n\n\nNext, we'll evaluate GPT o4-mini:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gpt_o4_mini <- are_task$clone()\nare_gpt_o4_mini$eval(solver_chat = gpt_o4_mini)\n```\n:::\n\n\n\n\n\nFinally, let's evaluate Claude 3.7 Sonnet:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_claude_3_7 <- are_task$clone()\nare_claude_3_7$eval(solver_chat = claude_sonnet_3_7)\n```\n:::\n\n\n\n\n\n\n\nThe interactive viewer will allow us to inspect the evaluation in detail:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<iframe src=\"/assets/2025-05-21-gemini-2-5-flash/viewer/index.html\" width=\"100%\" height=\"600px\" style=\"border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);\"></iframe>\n```\n\n:::\n:::\n\n\n\n:::callout-note\nWhile the total durations of the evaluations are correct in the viewer, the timings of specific samples are now off. Given some changes in downstream packages, vitals has to estimate how long a given request takes rather than receiving the exact duration; this will be resolved down the line.\n:::\n\n## Analysis\n\nLet's combine the results of all evaluations to compare the models:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval <- \n  vitals_bind(\n    `Gemini 2.5 Flash (Thinking)` = are_gemini_2_5_flash_thinking,\n    `Gemini 2.5 Flash (Non-thinking)` = are_gemini_2_5_flash_non_thinking,\n    `Gemini 2.0 Flash` = are_gemini_2_0_flash,\n    `Gemini 2.5 Pro` = are_gemini_2_5_pro,\n    `GPT o4-mini` = are_gpt_o4_mini,\n    `Claude Sonnet 3.7` = are_claude_3_7\n  ) %>%\n  rename(model = task) %>%\n  mutate(\n    model = factor(model, levels = c(\n      \"Gemini 2.5 Flash (Thinking)\",\n      \"Gemini 2.5 Flash (Non-thinking)\",\n      \"Gemini 2.0 Flash\",\n      \"Gemini 2.5 Pro\",\n      \"GPT o4-mini\",\n      \"Claude Sonnet 3.7\"\n    ))\n  )\n\nare_eval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 468 × 5\n   model                       id                 epoch score metadata\n   <fct>                       <chr>              <int> <ord> <list>  \n 1 Gemini 2.5 Flash (Thinking) after-stat-bar-he…     1 I     <tibble>\n 2 Gemini 2.5 Flash (Thinking) after-stat-bar-he…     2 I     <tibble>\n 3 Gemini 2.5 Flash (Thinking) after-stat-bar-he…     3 I     <tibble>\n 4 Gemini 2.5 Flash (Thinking) conditional-group…     1 P     <tibble>\n 5 Gemini 2.5 Flash (Thinking) conditional-group…     2 C     <tibble>\n 6 Gemini 2.5 Flash (Thinking) conditional-group…     3 C     <tibble>\n 7 Gemini 2.5 Flash (Thinking) correlated-delays…     1 P     <tibble>\n 8 Gemini 2.5 Flash (Thinking) correlated-delays…     2 P     <tibble>\n 9 Gemini 2.5 Flash (Thinking) correlated-delays…     3 P     <tibble>\n10 Gemini 2.5 Flash (Thinking) curl-http-get          1 C     <tibble>\n# ℹ 458 more rows\n```\n\n\n:::\n:::\n\n\n\nLet's visualize the results with a bar chart:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval %>%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n  ) %>%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"The Gemini 2.5 Flash models represent a middle-ground between 2.5 Pro and\\no4-mini, both in terms of price and performance.\"\n  ) +\n  theme(\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-are-eval-1.png){fig-alt='A horizontal bar chart comparing various AI models\\' performance on R coding tasks. The chart shows percentages of correct (blue), partially correct (beige), and incorrect (orange) answers. Gemini 2.5 Flash shows comparable performance to o4-mini, both outperforming Gemini 2.0 Flash and GPT 4.1-nano. Claude 3.7 Sonnet and Gemini 2.5 Pro remain the top performers.' width=100%}\n:::\n:::\n\n\n\nTo determine if the differences we're seeing are statistically significant, we'll use a cumulative link mixed model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ordinal)\n\nare_mod <- clmm(score ~ model + (1|id), data = are_eval)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(are_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  468  -375.15 766.30 397(1971) 8.31e-05 6.4e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 5.523    2.35    \nNumber of groups:  id 26 \n\nCoefficients:\n                                     Estimate Std. Error z value\nmodelGemini 2.5 Flash (Non-thinking)  -0.8506     0.3604  -2.360\nmodelGemini 2.0 Flash                 -1.3338     0.3698  -3.607\nmodelGemini 2.5 Pro                    0.6600     0.3611   1.828\nmodelGPT o4-mini                       0.7887     0.3620   2.179\nmodelClaude Sonnet 3.7                 0.3740     0.3567   1.048\n                                     Pr(>|z|)    \nmodelGemini 2.5 Flash (Non-thinking)  0.01826 *  \nmodelGemini 2.0 Flash                 0.00031 ***\nmodelGemini 2.5 Pro                   0.06759 .  \nmodelGPT o4-mini                      0.02934 *  \nmodelClaude Sonnet 3.7                0.29441    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -1.3191     0.5404  -2.441\nP|C   0.4820     0.5361   0.899\n```\n\n\n:::\n:::\n\n\n\nFor the purposes of this post, we'll just take a look at the `Coefficients` table. The reference model here is Gemini 2.5 Flash. Negative coefficient estimates for a given model indicate that model is less likely to receive higher ratings than Gemini 2.5 Flash. Looking at the coefficients:\n\n* Gemini 2.5 Flash is a marked improvement over its previous generation on this eval.\n* Gemini 2.5 Flash significantly lags behind o4-mini on this eval.\n* Enabling Gemini 2.5 Flash's thinking results in a marked increase in performance over the non-thinking model, though brings the pricing much closer to the more performant o4-mini.\n* Gemini 2.5 Flash with thinking disabled is [nearly indistinguishable](https://www.simonpcouch.com/blog/2025-04-15-gpt-4-1/) from GPT 4.1-nano on this eval. 2.5 Flash reached an accuracy of 43.6% at a cost of $0.15/m input, $0.60/m output, which 4.1-nano scored 44.2% at a cost of $0.10/m input, $0.40/m output. 4.1-nano continues to pack the greatest punch in the budget, non-thinking price point.\n\nOne more note before I wrap up: For the past month or two, development on ellmer and vitals has been quite coupled to support answering common questions about LLM performance. With the [release of ellmer 0.2.0 on CRAN](https://ellmer.tidyverse.org/news/index.html#ellmer-020) last week, I'm starting to gear up for an initial CRAN release of vitals here soon. In the meantime, I'm especially interested in feedback from folks who have given the package a go! Do let me know if you [give it a whir](https://vitals.tidyverse.org/) and run into any hiccups.\n\n---\n\n_Thank you to Max Kuhn for advising on the model-based analysis here._\n\n_In a first for this blog, I tried using a model to help me write this post. In general, I don't tend to use models to help with writing at all. Now that I've written a good few of these posts to pattern-match from, I wondered if Claude 3.7 Sonnet could draft a reasonable starting place. I used [this prompt](https://gist.github.com/simonpcouch/d756c133631474f0e5cf1eae244f65c0); as usual, I ended up deleting all  of the prose that the model wrote, but it was certainly a boost to have all of the code written for me._\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}