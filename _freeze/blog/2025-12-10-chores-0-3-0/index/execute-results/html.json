{
  "hash": "1dfbc6ec16982f7ef1d014bc734e8883",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"chores 0.3.0 and local LLMs\"\ndate: '2025-12-10'\ntags:\n  - ai\n  - rstats\nsubtitle: \"There are now small models capable of powering chores helpers.\"\nimage: featured.png\nsummary: ''\n---\n\nThe tl;dr:\n\n* The [chores](https://github.com/simonpcouch/chores) package, up to this point, needed a frontier-ish model to be useful; local models were more trouble than they were worth.\n* [Qwen3 4B Instruct 2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507) is good enough, and it's small enough to comfortably run on high-end laptops.\n\n## The chores package\n\nThe chores package provides a library of ergonomic LLM assistants designed to help you complete repetitive, hard-to-automate tasks quickly. After selecting some code, you can press a keyboard shortcut, select a helper that corresponds to a system prompt, and watch your code be rewritten.\n\n{{< video claude_3_7_chores.mov >}}\n\nWhen you select some code and a chore helper, what's happening under the hood is that the package first retrieves the prompt corresponding the system prompt you chose. For example, the prompt for templating out roxygen2 function documentation looks like this:\n\n<style>\n.chores-prompt pre {\n  background-color: #f6f6f6 !important;\n  font-size: 1.15em;\n}\n.chores-prompt code {\n  background-color: #e9ecef !important;\n  font-size: 0.9em;\n  padding: 2px 4px;\n  border-radius: 3px;\n}\n.chores-prompt pre {\n  font-size: 1.15em;\n}\n.chores-prompt pre code {\n  background-color: transparent !important;\n  font-size: 1em;\n}\n.chores-prompt h1 {\n  font-size: 2.5em;\n  font-weight: bold !important;\n}\n.chores-prompt h2 {\n  font-size: 2.2em;\n  font-weight: bold !important;\n}\n.chores-prompt li {\n  font-size: 1.8em;\n}\n</style>\n\n<div class=\"chores-prompt\" style=\"height: 600px; overflow-y: auto; border: 1px solid #ddd; border-radius: 10px; padding: 20px; background-color: #f8f9fa; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); font-size: 0.5em;\">\n\n**Templating function documentation**\n\nYou are a terse assistant designed to help R package developers quickly template out their function documentation using roxygen2. Given some highlighted function code, return minimal documentation on the function's parameters and return type. Beyond those two elements, be sparing so as not to describe things you don't have context for. Respond with *only* R `#'` roxygen2 comments---no backticks or newlines around the response, no further commentary.\n\nFor function parameters in `@params`, describe each according to their type (e.g. \"A numeric vector\" or \"A single string\") and note if the parameter isn't required by writing \"Optional\" if it has a default value. If the parameters have a default enum (e.g. `arg = c(\"a\", \"b\", \"c\")`), write them out as 'one of `\"a\"`, `\"b\"`, or `\"c\"`.' If there are ellipses in the function signature, note what happens to them. If they're checked with `rlang::check_dots_empty()` or otherwise, document them as \"Currently unused; must be empty.\" If the ellipses are passed along to another function, note which function they're passed to.\n\nFor the return type in `@returns`, note any important errors or warnings that might occur and under what conditions. If the `output` is returned with `invisible(output)`, note that it's returned \"invisibly.\"\n\n**Here are some examples:**\n\nGiven:\n```r\nkey_get <- function(name, error_call = caller_env()) {\n  val <- Sys.getenv(name)\n  if (!identical(val, \"\")) {\n    val\n  } else {\n    if (is_testing()) {\n      testthat::skip(sprintf(\"%s env var is not configured\", name))\n    } else {\n      cli::cli_abort(\"Can't find env var {.code {name}}.\", call = error_call)\n    }\n  }\n}\n```\n\nReply with:\n```r\n#' Get key\n#'\n#' @description\n#' A short description...\n#' \n#' @param name A single string.\n#' @param error_call A call to mention in error messages. Optional.\n#'\n#' @returns \n#' If found, the value corresponding to the provided `name`. Otherwise,\n#' the function will error.\n#'\n#' @export\n```\n\nGiven:\n```r\nchat_perform <- function(provider,\n                         mode = c(\"value\", \"stream\", \"async-stream\", \"async-value\"),\n                         turns,\n                         tools = list(),\n                         extra_args = list()) {\n\n  mode <- arg_match(mode)\n  stream <- mode %in% c(\"stream\", \"async-stream\")\n\n  req <- chat_request(\n    provider = provider,\n    turns = turns,\n    tools = tools,\n    stream = stream,\n    extra_args = extra_args\n  )\n\n  switch(mode,\n    \"value\" = chat_perform_value(provider, req),\n    \"stream\" = chat_perform_stream(provider, req),\n    \"async-value\" = chat_perform_async_value(provider, req),\n    \"async-stream\" = chat_perform_async_stream(provider, req)\n  )\n}\n```\n\nReply with:\n```r\n#' Perform chat\n#'\n#' @description\n#' A short description...\n#' \n#' @param provider A provider.\n#' @param mode One of `\"value\"`, `\"stream\"`, `\"async-stream\"`, or `\"async-value\"`.\n#' @param turns Turns.\n#' @param tools Optional. A list of tools.\n#' @param extra_args Optional. A list of extra arguments.\n#'\n#' @returns \n#' A result.\n#'\n#' @export\n```\n\nGiven:\n```r\ncheck_args <- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names <- names(formals(fn))\n  if (length(arg_names) < 2) {\n    cli::cli_abort(\"Function must have at least two arguments.\", .internal = TRUE)\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\n```\n\nReply with:\n```r\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\n```\n\nWhen two functions are supplied, only provide documentation for the first function, only making use of later functions as additional context. For example:\n\nGiven:\n```r\ncheck_args <- function(fn, ...) {\n  rlang::check_dots_empty()\n  arg_names <- names(formals(fn))\n  if (length(arg_names) < 2) {\n    error_less_than_two_args()\n  } else if (arg_names[[1]] != \"self\") {\n    cli::cli_abort(\"First argument must be {.arg self}.\", .internal = TRUE)\n  } else if (arg_names[[2]] != \"private\") {\n    cli::cli_abort(\"Second argument must be {.arg private}.\", .internal = TRUE)\n  }\n  invisible(fn)\n}\n\nerror_less_than_two_args <- function(call = caller_env()) {\n  cli::cli_abort(\"Function must have at least two arguments.\", call = call, .internal = TRUE)\n}\n```\n \nReply with:\n```r\n#' Check a function's arguments\n#'\n#' @description\n#' A short description...\n#' \n#' @param fn A function.\n#' @param ... Currently unused; must be empty.\n#'\n#' @returns \n#' `fn`, invisibly. The function will instead raise an error if the function\n#' doesn't take first argument `self` and second argument `private`.\n#'\n#' @export\n```\n\n</div>\n<br>\n\nThen, the selected helper prompt is set as the system prompt and the code you selected is set as the user prompt in a call to `ellmer::Chat()`. It looks something like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\n\nch <- chat_anthropic(system_prompt = the_prompt_from_above)\n \nch$chat(\"<the code you selected>\")\n#> #' The documentation for the selected code.\n#> #' \n#> #' Yada yada yada.\n```\n:::\n\n\n## Choosing a model\n\nThe chores package allows you to use any model you can connect to with [ellmer](https://ellmer.tidyverse.org). So, how do you choose which one to use?\n\nThe model powering chores needs the following characteristics:\n\n* Strict instruction-following: Looking back at that roxygen prompt, those instructions make two asks of LLMs that are pretty difficult for models that have been trained so strictly into the \"helpful assistant\" role: no exposition or explanatory text before or after the roxygen comments, and no (triple) backticks around the response. The chores package writes the LLMs' output directly to the source file, so it's really frustrating when models provide any text other than what's requested.\n* Minimally- or non-thinking: Thinking adds latency and shouldn't be necessary to complete these tasks. There are many interfaces where thinking is nice and/or necessary, but this isn't one of them.\n\nNotably, the model does _not_ need the ability to call tools, carry out long-horizon tasks, or be a pleasant conversationalist. It's fine if the model used with chores is bad at pretty much everything besides writing syntactically valid code in compliance with the instructions in the provided prompt.\n\nIn the package documentation, I recommend Claude 3.7 Sonnet and GPT 4.1 (optionally, -mini).^[Notably, I do not currently recommend Claude 4 Sonnet, Claude 4.5 Sonnet, or Claude 4.5. Haiku. The newer Claude models tend to include triple backticks in their responses even when prompted not to.] Up to this point, though, I'd thought you really _had_ needed to use a frontier-ish model to get any value out of chores. It's seemed to me that many of the models that I can currently run on my laptop (up to late 2025) had been trained into the \"I'm a helpful assistant\" persona so strictly--even those that are advertised as instruction-tuned--that they'll ramble on and on before and after providing the requested code, even if the provided code is reasonable. \n\nIn some ways, some time just needed to pass, but I also had overlooked a critical issue (and seemingly everyone else that's tried to use chores with local models). In working on [another problem](https://www.simonpcouch.com/blog/2025-12-04-local-agents/), I learned that:\n\n1) ollama's and LM Studio's default context length is 4,096 tokens, even for models that support much longer context windows, and \n2) If you provide a prompt that's greater than the size of the context length, it will be truncated to fit inside the length rather than erroring.\n\nI think this probably contributed to my misconception that models small enough to run on my laptop weren't capable of powering chores. Once I [trimmed](https://github.com/simonpcouch/chores/commit/8ca18453763dceb8fad5274d880c3eb068e213bc) the cli helper prompt to fit inside the context window (and/or increased the size of the context length in LM Studio) with some wiggle room, I saw much more promising results on these tasks than I had seen from local models before.\n\n## A new kid on the block\n\nSo, even with the models that I had `ollama pull`ed a few months ago, I realized we were already closer to local models powering chores than I had thought. At that point, I wondered what the newest releases were that might show stronger performance. In particular, I've been pretty amazed by several of the [Qwen3](https://huggingface.co/collections/Qwen/qwen3) models, so I started there.\n\nAfter some pokings-around, I think that [Qwen3 4B Instruct 2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507) is a great model for local use with chores. Here's a real-time (as in, not sped up) demo of that model in action on my M4 Macbook:\n\n{{< video qwen3_4b_chores.mov >}}\n\nThe cli refactor is a little bit wonky; the model chose to use backticks around the env var markup rather than curly braces, which won't render correctly. The templating for the roxygen2 documentation is totally reasonable. Performance isn't quite Claude 3.7 Sonnet, but I'm pretty blown away by how good it is.\n\nIf you're interested in trying this model out, you can either use LM Studio or ollama.\n\nOn Apple Silicon (Mac M-series), I recommend [LM Studio](https://lmstudio.ai/); LM Studio supports MLX, an array framework for Apple Silicon, which helps the model run much more quickly than with ollama. Click \"Discover\", search \"Qwen3 4B Instruct 2507\", and click \"Download.\" Once downloaded, click the \"Developer\" tab and change the Status from Stopped to Running. Then, in R, configure chores with:\n\n```r\nqwen3_4b <- ellmer::chat_openai_compatible(\n  base_url = \"http://127.0.0.1:1234/v1\",\n  model = \"qwen/qwen3-4b-2507\"\n)\n\noptions(chores.chat = qwen3_4b)\n```\n\nNote the `/v1` in the base URL; this will hit LM Studio's OpenAI API completions v1 endpoint.\n\nOn other systems, you could also use [Ollama](https://ollama.com). Run `ollama pull qwen3:4b` at the terminal, then set `options(chores.chat = ellmer::chat_ollama(model = \"qwen3:4b\"))`.\n\nAt least with MLX on LM Studio, the model takes up 2.5GB of disk space and requires 2.5GB of RAM to run.\n\n## Oh, and chores 0.3.0\n\nFor a good experience \"by default\" (i.e. without the need to change the context length in LM Studio to use the default helpers), install the [new release](https://github.com/simonpcouch/chores/releases) of chores with `install.packages(\"chores\")`!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}