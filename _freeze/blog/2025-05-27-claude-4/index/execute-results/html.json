{
  "hash": "24768cd99afce07b5272a46777ab4db1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Claude 4 and R Coding\"\ndate: '2025-05-27'\ntags:\n  - ai\n  - vitals\n  - rstats\nsubtitle: \"Evaluating the most recent releases of Claude Sonnet and Opus on challenging R coding problems.\"\nimage: featured.png\nsummary: ''\n---\n\n\n\n\n\n[Claude 4](https://www.anthropic.com/news/claude-4) dropped on Thursday! Given that Claude 3.7 Sonnet is my daily driver LLM for R coding, I've been excited to poke at it.\n\n<a href=\"https://vitals.tidyverse.org/\"><img src=\"vitals.png\" alt=\"The hex sticker for the vitals package: a teddy bear in blue scrubs happily holding a stethoscope.\" align=\"right\" height=\"240\"/></a>\n\nThe last few months, I've been writing a [series](https://www.simonpcouch.com/blog/2025-05-21-gemini-2-5-flash/) [of](https://www.simonpcouch.com/blog/2025-05-07-gemini-2-5-pro-new/) [blog](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/) [posts](https://www.simonpcouch.com/blog/2025-04-15-gpt-4-1/) where I evaluate new LLM releases on their R coding performance. I do so entirely in R using the [ellmer](https://ellmer.tidyverse.org/) and [vitals](https://vitals.tidyverse.org/) packages, the latter of which will be headed to CRAN in the coming weeks. In this post, I'll skip over all of the evaluation code and just make some graphs; if you're interested in learning more about how to run an eval like this one, check out the post [Evaluating o3 and o4-mini on R coding performance](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/).\n\nHere's the gist:\n\n* [An R Eval](https://vitals.tidyverse.org/reference/are.html) is a dataset of challenging R coding problems.\n* We'll run an evaluation on that dataset on Claude 4 Sonnet, Claude 4 Opus (which Anthropic made a point to note had impressive coding performance), Claude 3.7 Sonnet (my previous coding daily driver), and o4-mini (up to this point, the [most performant model](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/) in this eval).\n* Using the results, we can measure how well different models solved the R coding problems, how many tokens (and thus dollars) they used to get to those answers, and how long it took them to finish their answer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nlibrary(vitals)\nlibrary(tidyverse)\nlibrary(ggrepel)\n```\n:::\n\n\n\n<details>\n  <summary>Full evaluation code here</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclaude_4_sonnet <- chat_anthropic(model = \"claude-sonnet-4-20250514\")\nclaude_4_opus <- chat_anthropic(model = \"claude-opus-4-20250514\")\nclaude_3_7_sonnet <- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\ngpt_o4_mini <- chat_openai(model = \"o4-mini-2025-04-16\")\n\nare_task <- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = claude_3_7_sonnet, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n\nare_claude_4_sonnet <- are_task$clone()\nare_claude_4_sonnet$eval(solver_chat = claude_4_sonnet)\nare_claude_4_sonnet <- vitals:::scrub_providers(are_claude_4_sonnet)\nsave(are_claude_4_sonnet, file = \"blog/2025-05-27-claude-4/tasks/are_claude_4_sonnet.rda\")\n\nare_claude_4_opus <- are_task$clone()\nare_claude_4_opus$eval(solver_chat = claude_4_opus)\nare_claude_4_opus <- vitals:::scrub_providers(are_claude_4_opus)\nsave(are_claude_4_opus, file = \"blog/2025-05-27-claude-4/tasks/are_claude_4_opus.rda\")\n\nare_claude_3_7_sonnet <- are_task$clone()\nare_claude_3_7_sonnet$eval(solver_chat = claude_3_7_sonnet)\nare_claude_3_7_sonnet <- vitals:::scrub_providers(are_claude_3_7_sonnet)\nsave(are_claude_3_7_sonnet, file = \"blog/2025-05-27-claude-4/tasks/are_claude_3_7_sonnet.rda\")\n\nare_gpt_o4_mini <- are_task$clone()\nare_gpt_o4_mini$eval(solver_chat = gpt_o4_mini)\nare_gpt_o4_mini <- vitals:::scrub_providers(are_gpt_o4_mini)\nsave(are_gpt_o4_mini, file = \"blog/2025-05-27-claude-4/tasks/are_gpt_o4_mini.rda\")\n```\n:::\n\n\n\n</details>\n\nYou can view the raw results of the evaluation in this interactive viewer:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<iframe src=\"/assets/2025-05-27-claude-4/viewer/index.html\" width=\"100%\" height=\"600px\" style=\"border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);\"></iframe>\n```\n\n:::\n:::\n\n\n\n:::callout-note\nWhile the total durations of the evaluations are correct in the viewer, the timings of specific samples are now estimated. Given some changes in downstream packages, vitals has to estimate how long a given request takes rather than receiving the exact duration; this will be resolved down the line.\n:::\n\n## Analysis\n\nAt this point, we have access to `are_eval`, a data frame containing all of the results collected during the evaluation.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 312 × 5\n   model           id                          epoch score metadata\n   <fct>           <chr>                       <int> <ord> <list>  \n 1 Claude 4 Sonnet after-stat-bar-heights          1 I     <tibble>\n 2 Claude 4 Sonnet after-stat-bar-heights          2 I     <tibble>\n 3 Claude 4 Sonnet after-stat-bar-heights          3 I     <tibble>\n 4 Claude 4 Sonnet conditional-grouped-summary     1 C     <tibble>\n 5 Claude 4 Sonnet conditional-grouped-summary     2 P     <tibble>\n 6 Claude 4 Sonnet conditional-grouped-summary     3 C     <tibble>\n 7 Claude 4 Sonnet correlated-delays-reasoning     1 P     <tibble>\n 8 Claude 4 Sonnet correlated-delays-reasoning     2 P     <tibble>\n 9 Claude 4 Sonnet correlated-delays-reasoning     3 I     <tibble>\n10 Claude 4 Sonnet curl-http-get                   1 C     <tibble>\n# ℹ 302 more rows\n```\n\n\n:::\n:::\n\n\n\nThe evaluation scores each answer as \"Correct\", \"Partially Correct\", or \"Incorrect\". We can use a bar chart to visualize the proportions of responses that fell into each of those categories:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval %>%\n  mutate(\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    ),\n  ) %>%\n  ggplot(aes(y = model, fill = score)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(\n    breaks = rev,\n    values = c(\"Correct\" = \"#67a9cf\", \n               \"Partially Correct\" = \"#f6e8c3\", \n               \"Incorrect\" = \"#ef8a62\")\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Percent\", y = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"Claude 4 models represent a step forward in R coding performance.\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.border = element_blank(),\n    panel.background = element_rect(fill = \"#f8f8f1\", color = NA),\n    plot.background = element_rect(fill = \"#f8f8f1\", color = NA),\n    legend.background = element_rect(fill = \"#F3F3EE\", color = NA),\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.position = \"bottom\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-are-eval-1.png){fig-alt='A horizontal bar chart comparing various AI models\\' performance on R coding tasks. The chart shows percentages of correct (blue), partially correct (beige), and incorrect (orange) answers. Claude 4 Opus has the highest proportion of correct answers, followed by o4-mini, Claude 4 Sonnet, and Claude 3.7 Sonnet.' width=100%}\n:::\n:::\n\n\n\nThe pricing per token of each of these models differs quite a bit, though, and o4-mini might use many more tokens during its \"reasoning.\" How much did it cost to run each of these evals, and what's the resulting cost-per-performance?\n\nThe pricing per million tokens for these models is as follows:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  Name              Input  Output\n  <chr>             <chr>  <chr> \n1 Claude 4 Sonnet   $3.00  $15.00\n2 Claude 4 Opus     $15.00 $75.00\n3 Claude 3.7 Sonnet $3.00  $15.00\n4 o4-mini           $1.10  $4.40 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\nUnder the hood, I've calculated the total cost of running the eval for each model using the shown pricing and joined it to the evaluation results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Name     Price    Score\n1   Claude 4 Sonnet 0.7204620 59.61538\n2     Claude 4 Opus 3.3161850 66.02564\n3 Claude 3.7 Sonnet 0.4359000 57.05128\n4           o4-mini 0.6164763 65.38462\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(costs) +\n  aes(x = Price, y = Score, label = Name) +\n  geom_point(size = 3, color = \"#4A7862\") +\n  geom_label_repel() +\n  scale_y_continuous(limits = c(55, 70), labels = function(x) paste0(x, \"%\")) +\n  labs(\n    x = \"Total Price (USD)\",\n    y = \"Score\",\n    title = \"R Coding Performance vs. Cost\",\n    subtitle = \"o4-mini packs the most punch at its price point, though\\nClaude 4 Opus is state-of-the-art.\"\n  ) +\n  lims(x = c(0, 4))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){fig-alt='A scatterplot comparing the price and performance of four LLMs. While the Claude Sonnet models and o4-mini are clustered around $1, o4-mini shows much stronger performance. Claude 4 Opus delivers slightly stronger performance than o4-mini, though is much more expensive at $4 total.' width=100%}\n:::\n:::\n\n\n\nWhile Claude 4 Opus is the new SotA on this eval, o4-mini almost matches its performance at a small fraction of the price.\n\nTo determine if the differences we're seeing are statistically significant, we'll use a cumulative link mixed model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ordinal)\n\nare_mod <- clmm(score ~ model + (1|id), data = are_eval)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(are_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  312  -205.43 422.85 234(1729) 7.06e-06 2.0e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 13.56    3.682   \nNumber of groups:  id 26 \n\nCoefficients:\n                       Estimate Std. Error z value Pr(>|z|)\nmodelClaude 4 Opus       0.6190     0.4126   1.500    0.134\nmodelClaude 3.7 Sonnet  -0.3293     0.4115  -0.800    0.424\nmodelGPT o4-mini         0.5753     0.4236   1.358    0.174\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P -2.25743    0.43244  -5.220\nP|C -0.04208    0.48543  -0.087\n```\n\n\n:::\n:::\n\n\n\nFor the purposes of this post, we'll just take a look at the `Coefficients` table. The reference model here is Claude 4 Sonnet. Negative coefficient estimates for a given model indicate that model is less likely to receive higher ratings than Claude 4 Sonnet. Looking at the coefficients, while Claude 4 Sonnet seems like an improvement upon its previous generation, we don't see evidence of statistically significantly differing performance on this eval from Claude 4 Sonnet and any other model.\n\n---\n\n_Thank you to Max Kuhn for advising on the model-based analysis here._\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}