{
  "hash": "5e7fbc1452d4c4fafa10ac4362dcc11c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How good are the GPT 4.1 models at writing R code?\"\ndate: '2025-04-15'\ntags:\n  - ai\n  - vitals\n  - rstats\nsubtitle: \"OpenAI dropped a trio of models yesterday that they claim result from \\\"focusing closely on real-world developer needs.\\\" I was curious whether this might mean stronger R coding performance.\"\nimage: featured.png\nsummary: ''\n---\n\n\n\n\n\nYesterday, OpenAI dropped [a new series of models](https://openai.com/index/gpt-4-1/) called GPT 4.1, 4.1 mini, and GPT 4.1 nano. This line from their release post, specifically, caught my eye:\n\n> GPT‑4.1 is a significant step forward in the practical application of AI. By focusing closely on real-world developer needs—ranging from coding to instruction-following and long context understanding—these models unlock new possibilities for building intelligent systems and sophisticated agentic applications.\n\nIt's no surprise to me that OpenAI's newest drop tops benchmark after benchmark. That said, when I see news of new models beating out Claude Sonnet by various measures, I usually wait a week before coming to any conclusions; many developers seem to feel that the Claude series of models have some secret sauce, and I'm among them. Seeing this explicit focus on real-world coding and instruction-following piqued my curiosity, so I'm bypassing my usual \"wait a week\" policy to see what's up.\n\n<a href=\"https://simonpcouch.github.io/vtials/\"><img src=\"vitals.png\" alt=\"The hex sticker for the vitals package: a teddy bear in blue scrubs happily holding a stethoscope.\" align=\"right\" height=\"240\"/></a>\n\nAs it happens, I've been working on [a new tool called vitals](https://simonpcouch.github.io/vitals/) for large language model evaluation in R. The package is still pretty early on in it's development and is changing rapidly--so much so that its name has changed in the two weeks since I last [wrote about it on this blog](https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/)--but I'll use it here to evaluate these models on an R coding benchmark.\n\n> _tl;dr_:\n> \n> * This eval is a good measure for R coding problems, but doesn't aim to measure instruction-following or long context understanding.\n> * The GPT 4.1 series of models does seem to improve on GPT-4o for solving R coding problems.\n> * Claude Sonnet 3.7 still outperforms GPT-4o and the GPT 4.1 series of models on R coding.\n> * The GPT 4.1 nano model seems to pack quite the punch for its price point; I'm curious whether it might be a good fit for a budget [chores](https://simonpcouch.github.io/chores/) and [gander](https://simonpcouch.github.io/gander/) engine.\n\n## Introducing vitals\n\nvitals is an R port of the widely adopted Python framework [Inspect](https://inspect.ai-safety-institute.org.uk/). While the package doesn't integrate with Inspect directly, it allows users to interface with the [Inspect log viewer](https://inspect.ai-safety-institute.org.uk/log-viewer.html) and shares much of its grammar and philosophy.\n\nvitals describes LLM evals in three core components:\n\n1)  **Datasets** contain a set of labelled samples. Datasets are just a tibble with columns `input` and `target`, where `input` is a prompt and `target` is either literal value(s) or grading guidance.\n2)  **Solvers** evaluate the `input` in the dataset and produce a final result (hopefully) approximating `target`. In vitals, the simplest solver is just an ellmer chat (e.g. [`ellmer::chat_claude()`](https://ellmer.tidyverse.org/reference/chat_claude.html)) wrapped in `generate()`, i.e. `generate(ellmer::chat_claude())`), which will call the [Chat object's `$chat()` method](https://ellmer.tidyverse.org/reference/Chat.html#method-Chat-chat) and return whatever it returns.\n3)  **Scorers** evaluate the final output of solvers. They may use text comparisons, model grading, or other custom schemes to determine how well the solver approximated the `target` based on the `input`.\n\nIn this blog post, we'll apply a solver powered by four different models to a dataset of R coding problems. Our baseline will be Claude 3.7 Sonnet, as this is my daily driver for coding assistance and a peer to GPT 4.1 in pricing. Then, I'll also add GPT-4o, as I know this has been the model of choice for many other folks. Finally, I'll include the three new models for the GPT series: 4.1, 4.1 mini, and 4.1 nano.\n\nIn ellmer, here's how we define those model connections:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\n\nsonnet_3_7 <- chat_anthropic(model = \"claude-3-7-sonnet-latest\")\n\ngpt_4o <- chat_openai(model = \"gpt-4o\")\n\ngpt_4_1 <- chat_openai(model = \"gpt-4.1\")\ngpt_4_1_mini <- chat_openai(model = \"gpt-4.1-mini\")\ngpt_4_1_nano <- chat_openai(model = \"gpt-4.1-nano\")\n```\n:::\n\n\n\n:::callout-note\nIf you're interested in how Gemini's newest 2.5 Pro release stacks up on this eval, check out [this post](https://www.simonpcouch.com/blog/2025-04-01-gemini-2-5-pro/) from two weeks ago.\n:::\n\nNote that I needed to configure a `ANTHROPIC_API_KEY` and `OPENAI_API_KEY` to connect to these models, respectively. These new models are quite cheap compared to Claude 3.7 Sonnet and GPT-4o! Their pricing per million tokens is as follows\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  Name              Input Output\n  <chr>             <chr> <chr> \n1 Claude 3.7 Sonnet $3.00 $15.00\n2 GPT-4o            $3.75 $15.00\n3 GPT-4.1           $2.00 $8.00 \n4 GPT-4.1 mini      $0.40 $1.60 \n5 GPT-4.1 nano      $0.10 $0.40 \n```\n\n\n:::\n:::\n\n\n\nAltogether, the data underlying this blog post took around $3 USD to generate.\n\n## An R Eval dataset\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vitals)\nlibrary(tidyverse)\n```\n:::\n\n\n\nWe'll use a dataset that ships with vitals called `are`, or \"An R Eval.\" From the `are` docs:\n\n> An R Eval is a dataset of challenging R coding problems. Each `input` is a question about R code which could be solved on first-read only by human experts and, with a chance to read documentation and run some code, by fluent data scientists. Solutions are in `target` and enable a fluent data scientist to evaluate whether the solution deserves full, partial, or no credit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(are)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nRows: 26\nColumns: 7\n$ id        <chr> \"after-stat-bar-heights\", \"conditional-grouped-sum…\n$ input     <chr> \"This bar chart shows the count of different cuts …\n$ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n…\n$ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\",…\n$ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"…\n$ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/…\n$ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"…\n````\n\n\n:::\n:::\n\n\n\nAt a high level:\n\n-   `title`: A unique identifier for the problem.\n-   `input`: The question to be answered.\n-   `target`: The solution, often with a description of notable features of a correct solution.\n-   `domain`, `task`, and `knowledge` are pieces of metadata describing the kind of R coding challenge.\n-   `source`: Where the problem came from, as a URL. Many of these coding problems are adapted \"from the wild\" and include the kinds of context usually available to those answering questions.\n\nNotably, these coding problems look like a typical chat, so the eval doesn't measure instruction-following / structured output specifically.\n\nFor the purposes of actually carrying out the initial evaluation, we're specifically interested in the `input` and `target` columns. Let's print out the first entry in full so you can get a taste of a typical problem in this dataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are$input[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nThis bar chart shows the count of different cuts of diamonds,\nand each bar is stacked and filled according to clarity:\n\n```\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity))\n```\n\nCould you change this code so that the proportion of diamonds\nwith a given cut corresponds to the bar height and not the\ncount? Each bar should still be filled according to clarity.\n````\n\n\n:::\n:::\n\n\n\nHere's the suggested solution:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are$target[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nPreferably:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = after_stat(count) /\nsum(after_stat(count)), fill = clarity))\n```\n\nThe dot-dot notation (`..count..`) was deprecated in ggplot2\n3.4.0, but it still works:\n\n```\nggplot(data = diamonds) +\ngeom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill =\nclarity))\n```\n\nSimply setting `position = \"fill\" will result in each bar\nhaving a height of 1 and is not correct.\n````\n\n\n:::\n:::\n\n\n\nFor now, `are` was publicly shared after the knowledge cutoff of each of these models, so the answers to these questions (likely) aren't yet incorporated into the models' weights.\n\n## A baseline model\n\nLLM evaluation with vitals happens in two main steps:\n\n**First**, use `Task$new()` to situate a dataset, solver, and scorer in a `Task`. [Tasks](https://simonpcouch.github.io/vitals/reference/Task.html) are R6 objects that define important methods and data structures for LLM evaluation. Below, I use `generate()` as a solver, currently the only built-in solver supplied by the package. Think of it like Chat objects' `$chat()` method with some bells and whistles—parallel requests, some nice progress functionality, and lots of logging. `generate()` returns a function that has one argument, `solver_chat`, which takes an ellmer Chat; you can set a default Chat by supplying it to `generate()` itself. The scorer, `model_graded_qa()`, uses model grading (or \"LLM-as-a-judge\") to score the solver's responses. Different models exhibit different behaviors as judges, so I use the same model (Claude Sonnet 3.7) as the judge regardless of which model is being evaluated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_task <- Task$new(\n  dataset = are,\n  solver = generate(),\n  scorer = model_graded_qa(\n    scorer_chat = sonnet_3_7, \n    partial_credit = TRUE\n  ),\n  epochs = 3,\n  name = \"An R Eval\"\n)\n\nare_task\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn evaluation task AnREval.\n```\n\n\n:::\n:::\n\n\n\n**Second**, use `Task$eval()` to evaluate the solver, evaluate the scorer, and then explore a persistent log of the results in an interactive viewer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_sonnet_3_7 <- are_task$clone()\nare_sonnet_3_7$eval(solver_chat = sonnet_3_7)\n```\n:::\n\n\n\n:::callout-note\nYou can also run `$eval()` step-by-step, if you want; `$eval()` just calls `$solve()`, `$score()`, `$log()`, and `$view()` in sequence.\n:::\n\n\n\n\n\n\n\n\nAfter evaluation, the task contains information from the solving and scoring steps. Here's what the model responded to that first question with:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are_sonnet_3_7$samples$result[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\nTo change the bar chart from showing counts to showing\nproportions of diamonds with each cut, I'll modify the code to\nuse proportions instead of raw counts while keeping the\nclarity breakdown within each bar.\n\nHere's the modified code:\n\n```r\nggplot(data = diamonds) +\ngeom_bar(mapping = aes(x = cut, fill = clarity),\nposition = \"fill\") +\nlabs(y = \"Proportion\")\n```\n\nThis change:\n\n1. Adds `position = \"fill\"` to the `geom_bar()` function,\nwhich normalizes each bar to have the same height (1.0 or\n100%) and shows the proportional breakdown of clarity\ncategories within each cut.\n\n2. Adds `labs(y = \"Proportion\")` to relabel the y-axis, since\nit now represents proportions rather than counts.\n\nEach bar will now have the same height, representing 100% of\ndiamonds with that cut, and the colored segments within each\nbar will show the proportional distribution of clarity\ncategories for that particular cut.\n````\n\n\n:::\n:::\n\n\n\nThen, since we've supplied `model_graded_qa()` as the scorer, Claude Sonnet 3.7 will be supplied the input question, the grading guidance, and the solver's output, and asked to determine whether the solution is incorrect, partially correct, or correct. Here's Claude's grading transcript for the solver's first answer:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(are_sonnet_3_7$samples$scorer_chat[[1]]$last_turn()@text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nI need to analyze whether the submitted answer correctly\naddresses the task according to the criterion.\n\nThe submission suggests using `position = \"fill\"` in\n`geom_bar()`, which normalizes each bar to have the same\nheight (1.0) and shows the proportional distribution of\nclarity categories within each cut.\n\nHowever, according to the criterion, this approach is\nexplicitly stated as incorrect. The criterion specifies that\nwe need to use either:\n1. `after_stat(count) / sum(after_stat(count))` in newer\nversions of ggplot2, or\n2. `..count.. / sum(..count..)` in older versions of ggplot2\n\nThe reason this is different from `position = \"fill\"` is that\n`position = \"fill\"` normalizes each cut category individually,\nshowing the proportion of different clarity values within each\ncut. In contrast, the requested solution shows the proportion\nof each cut relative to the total number of diamonds, while\nstill maintaining the clarity breakdown within each bar.\n\nThe submitted solution does not match what was specifically\nrequested. The criterion explicitly states that using\n`position = \"fill\"` is not correct for this task.\n\nGRADE: I\n```\n\n\n:::\n:::\n\n\n\nvitals ships with the Inspect Log Viewer, a small .js app that allows you to interactively explore evaluation logs. Especially the first few times you run an eval, the tool is super helpful for uncovering unexpected behavior in solving and scoring. I've embedded the viewer in this post so you can check out the problems in *An R Eval* and how effectively Claude Sonnet 3.7 handled them:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<iframe src=\"/assets/2025-04-15-gpt-4-1/viewer/index.html\" width=\"100%\" height=\"600px\" style=\"border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);\"></iframe>\n```\n\n:::\n:::\n\n\n\nI'd encourage you to poke around in this app! You'll certainly see some bugs that I've still yet to work out and some surprising behavior from the scorer, but there's lots to be learned about how these models work from evaluation logs.\n\n## Evaluating the rest\n\nWe can evaluate the remaining models by cloning the original task and running `$eval()` with a new solver chat. First, to evaluate the previous GPT (non-thinking) generation, GPT-4o:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gpt_4o <- are_task$clone()\nare_gpt_4o$eval(solver_chat = gpt_4o)\nsave(are_gpt_4o, file = \"are_gpt_4o.rda\")\n```\n:::\n\n\n\n\n\nFrom here, it's pretty rote. Evaluating each of GPT 4.1, 4.1 mini, and 4.1 nano on this dataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gpt_4_1 <- are_task$clone()\nare_gpt_4_1$eval(solver_chat = gpt_4_1)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gpt_4_1_mini <- are_task$clone()\nare_gpt_4_1_mini$eval(solver_chat = gpt_4_1_mini)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_gpt_4_1_nano <- are_task$clone()\nare_gpt_4_1_nano$eval(solver_chat = gpt_4_1_nano)\n```\n:::\n\n\n\n\n\n\n\nI've also situated the logs for the above evaluations in the above app—just click the three stacked bars in the top right of the app to check out the logs for the remaining models.\n\n## Analysis\n\nAt evaluation time, vitals does a naive accuracy calculation that you can see displayed in the app, but in general is quite restrained in its analysis functionality. Instead, the package aims to get analysts to Happy Data Frame Land as quickly as possible using `vitals_bind()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval <- \n  vitals_bind(\n    `Claude Sonnet 3.7` = are_sonnet_3_7,\n    `GPT-4o` = are_gpt_4o,\n    `GPT-4.1` = are_gpt_4_1,\n    `GPT-4.1 mini` = are_gpt_4_1_mini,\n    `GPT-4.1 nano` = are_gpt_4_1_nano,\n  ) %>%\n  rename(model = task) %>%\n  mutate(\n    model = factor(model, levels = c(\n      \"Claude Sonnet 3.7\", \n      \"GPT-4o\",\n      \"GPT-4.1\", \n      \"GPT-4.1 mini\",\n      \"GPT-4.1 nano\"\n    ))\n  )\n\nare_eval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 390 × 5\n   model             id                          epoch score metadata\n   <fct>             <chr>                       <int> <ord> <list>  \n 1 Claude Sonnet 3.7 after-stat-bar-heights          1 I     <tibble>\n 2 Claude Sonnet 3.7 after-stat-bar-heights          2 I     <tibble>\n 3 Claude Sonnet 3.7 after-stat-bar-heights          3 I     <tibble>\n 4 Claude Sonnet 3.7 conditional-grouped-summary     1 P     <tibble>\n 5 Claude Sonnet 3.7 conditional-grouped-summary     2 P     <tibble>\n 6 Claude Sonnet 3.7 conditional-grouped-summary     3 C     <tibble>\n 7 Claude Sonnet 3.7 correlated-delays-reasoning     1 P     <tibble>\n 8 Claude Sonnet 3.7 correlated-delays-reasoning     2 C     <tibble>\n 9 Claude Sonnet 3.7 correlated-delays-reasoning     3 P     <tibble>\n10 Claude Sonnet 3.7 curl-http-get                   1 I     <tibble>\n# ℹ 380 more rows\n```\n\n\n:::\n:::\n\n\n\nIn this dataset, each row represents a single time a solver is invoked to answer a question:\n\n-   `model` gives the model used to solve a given question\n\n-   `id` gives the question id\n\n-   `epoch` identifies the run/resample of the given question\n\n-   `scores` shows whether the scoring model (Claude Sonnet 3.7) identified the solver's answer as Incorrect, Partially Correct, or Correct. It's an ordinal factor with `I < P < C`.\n\n-   `metadata` is a list column containing just about all of the information that vitals collects during the evaluation process.\n\nWe're interested in which of these three models are right more often. We have 26 unique questions, each resampled across 3 epochs for each of 5 models. For a cursory analysis, we could do the canonical Bar Chart Dodged By Model visualization:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nare_eval %>%\n  mutate(\n    score = fct_rev(score),\n    score = fct_recode(\n      score, \n      \"Correct\" = \"C\", \"Partially Correct\" = \"P\", \"Incorrect\" = \"I\"\n    )\n  ) %>%\n  ggplot(aes(x = score, fill = model)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_manual(values = c(\n    \"Claude Sonnet 3.7\" = \"#d6941a\", \n    \"GPT-4o\" = \"#0f4c81\", \n    \"GPT-4.1\" = \"#4f86c6\", \n    \"GPT-4.1 mini\" = \"#6a9ed4\",  \n    \"GPT-4.1 nano\" = \"#89b9e2\" \n  )) +\n  labs(\n    x = \"Score\", y = \"Count\", fill = \"Model\",\n    title = \"An R Eval\",\n    subtitle = \"While the newest GPT 4.1 series models tend to solve R coding problems\\nmore effectively than GPT-4o, they still seem to lag behind Claude Sonnet 3.7.\"\n  ) +\n  theme(plot.subtitle = element_text(face = \"italic\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-are-eval-1.png){fig-alt='A ggplot2 bar plot showing the counts of correct, partially correct, and incorrect answers from various LLMs on R coding problems. Claude Sonnet 3.7, shown in orange, answer questions correctly much more often than the GPT series models, shown in shades of blue. GPT-4o answers questions correctly less often than any of the GPT 4.1 series models.' width=100%}\n:::\n:::\n\n\n\nCould the differences we're seeing be attributed to random noise, though? We can use a hierarchical modeling technique called a mixed model to model the probability of each score (i.e., correct, etc.) as a function of the LLM. In this case, observations are not independent; some questions may be harder than others, and we're repeating each question multiple times since we've set `epochs = 3`. A random intercept on the question `id` can help account for this variation. Since `score` is ordinal, we use a cumulative link mixed model rather than the usual suspect `lme4::glmer()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ordinal)\n\nare_mod <- clmm(score ~ model + (1|id), data = are_eval)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(are_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: score ~ model + (1 | id)\ndata:    are_eval\n\n link  threshold nobs logLik  AIC    niter     max.grad cond.H \n logit flexible  390  -268.62 551.24 272(1363) 1.69e-05 5.8e+01\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 13       3.605   \nNumber of groups:  id 26 \n\nCoefficients:\n                  Estimate Std. Error z value Pr(>|z|)    \nmodelGPT-4o        -2.3531     0.4359  -5.398 6.72e-08 ***\nmodelGPT-4.1       -1.2544     0.4116  -3.048 0.002307 ** \nmodelGPT-4.1 mini  -1.6051     0.4135  -3.882 0.000104 ***\nmodelGPT-4.1 nano  -1.6902     0.4195  -4.029 5.60e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\nI|P  -2.3073     0.7963  -2.897\nP|C   0.1779     0.7782   0.229\n```\n\n\n:::\n:::\n\n\n\nFirst, let's take a look at the `Coefficients` table. We have coefficients for each model other than Claude Sonnet 3.7, which is our \"reference\" model. Negative `Estimates` indicate lower odds of achieving higher rating categories, and the `Pr(>|z|)` values to their right show the p-values associated with those coefficients. We have evidence here that Claude Sonnet 3.7 is the strongest contender on this eval. While these estimates show that GPT 4.1 is closest to Claude Sonnet 3.7's performance, followed by GPT 4.1 mini, 4.1 nano, and then 4o, we haven't tested whether those pairwise differences could be attributed to random noise.\n\nThe `Threshold coefficients` describe whether ratings of Incorrect vs. Partially Correct and Partially Correct vs. Correct are meaningfully different from each other. The thresholds establish the baseline \"difficulty\" of achieving each category on the grading scale; more negative values for a pair of grades indicate that moving between those grades is relatively easy. If we saw that both coefficients here were quite negative, we could conclude that the rating system has a strong tendency toward higher ratings overall. In our case, those ratings seems relatively balanced.\n\nFinally, the substantial `Random effects` value here shows that there's substantial heterogeneity in question difficulty that's being captured by the model. We can visualize these question-level effects:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){fig-alt='A ggplot2 plot with 26 unique questions on the y axis and the random intercept estimate associated with each on the x. The estimates seem roughly normally distributed, and the estimates range from around -5 to around 5; most are clustered near 0.' width=100%}\n:::\n:::\n\n\n\nEach of the rows here is a given question, where smaller random intercept estimates indicate that a question is more difficult. The most challenging sample was \"after-stat-bar-heights\" where, across all LLMs and epochs, 0 of 15 scores were categorized as correct. As this eval's author, this is an indication to me that I should audit these questions and determine if they're answerable at all; it's fine if these are just hard questions, but if there's not enough information in the question to actually answer it, or if the grading guidance is incorrect, this is a bug in the eval dataset rather than a measure of these models' coding ability.\n\n:::callout-note\nKeep an eye out for a vitals vignette with a more thorough model-based analysis than this one in the near future.\n:::\n\nAltogether:\n\n* The GPT 4.1 series of models does seem to improve on GPT-4o for solving R coding problems.\n* Claude Sonnet 3.7 still outperforms GPT-4o and the GPT 4.1 series of models on R coding.\n* At least for this sort of problem, the GPT 4.1 nano model seems to pack quite the punch for its price point.\n\nGiven this set of releases' apparent focus on instruction-following and the relatively strong performance of the nano model here, I'm now curious if GPT 4.1 nano (or even mini) would make for a good model to underlie the [chores](https://simonpcouch.github.io/chores/) and [gander](https://simonpcouch.github.io/gander/) packages, which require a model that's very good at pattern-matching and instruction-following and don't necessarily rely on extensive coding prowess otherwise. \n\n-----\n\n\n_Thank you to Max Kuhn for advising on the model-based analysis here._\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}