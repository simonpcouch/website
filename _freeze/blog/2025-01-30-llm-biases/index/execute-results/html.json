{
  "hash": "ac9450ce3b832637047bfebbf49ca3f8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploring Biases in GPT-4o, Claude, and Qwen2.5 Judgements\"\ndate: '2025-01-30'\ntags:\n  - ellmer\n  - ai\nsubtitle: \"What happens when you ask language models to evaluate their own output?\"\nimage: featured.png\nsummary: ''\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n\n\n\nI've been spending some time recently learning about LLMs evaluating output from LLMs, or as its referred to in the literature, \"LLM-as-a-judge.\" That is, after asking a question to an LLM and receiving an answer, both the question and answer are provided to another language model and that model is asked to somehow judge whether the provided response was satisfactory.\n\nThere are all sorts of considerations that come into play when designing LLM-as-a-judge systems; LLMs can exhibit many of the same cognitive biases that humans do when judging paired comparisons.^[If you're interested in learning about this, I put together [this primer](https://simonpcouch.github.io/evalthat/articles/model-grading.html).] For example, when asked to choose between two responses, many models will prefer the one presented first, prefer the longer of the two, or prefer the one it generated itself; we'll look into each of these effects in this blog post. In this post, I'll examine some data I generated while prototyping out some code for evaluating [ellmer](https://ellmer.tidyverse.org/) outputs. That code isn't quite usable generally yet, but the kinds of data one can pull out of it already is super interesting.^[This post is based on output generated using the code in the commit message [here](https://github.com/simonpcouch/evalthat/commit/74f1c0475276d7f30bcd5fe82baf77b698da6702).]\n\n## The data\n\nEach row in the dataset is the result of a paired comparison of two model responses. For example, consider the following question asked of a language model:\n\n> Make the bars side-by-side rather than stacked on top of each other:\n> ```r\n> ggplot(mtcars) + \n>   aes(x = cyl, fill = factor(vs)) + \n>   geom_bar()\n> ```\n\nThat model's system prompt has been set to the following:\n\n> When asked a question about R code, reply with only the code needed to answer the question. No exposition, no backticks.\n\nIdeally, I'd like the model to respond with changes that implement the requested change and _only_ the requested change. No exposition, no additional formatting, so something like this:\n\n```r\nggplot(mtcars) + \n  aes(x = cyl, fill = factor(vs)) +\n  geom_bar(position = \"dodge\")\n```\n\nNow, I ask a few different models—OpenAI's GPT-4o, Anthropic's Claude Sonnet 3.5, and a locally hosted Qwen 2.5 model—to respond to this query. (This is an admittedly arbitrary bunch. Just what I had configured at the moment.) Then, I take a bunch of paired comparisons, where e.g. a response from GPT-4o is compared to one from Claude. Those comparisons are evaluated by letting each of GPT-4o, Claude, and Qwen take their turn as a \"judge,\" where they're prompted to choose one or the other based on a reference \"target\" and some criteria. In this example, I blended the prompts from two recent research papers [@ye2024justiceprejudicequantifyingbiases; @schroeder2024trustllmjudgmentsreliability]:\n\n> Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user’s instructions and best resembles the provided target response.\n> \n> Your evaluation should consider factors such as the helpfulness, relevance, \n> accuracy, conciseness, and level of detail of their responses, though be terse\n> and don't provide your reasoning, just your judgement.\n> \n> * Ensure that the order in which the responses were presented does not influence\n> your decision. \n> * Do not allow the length of the responses to influence your evaluation. \n> * Do not favor certain names of the assistants. \n> * Be as objective as possible. \n> \n> Output your final verdict by strictly following this format: \"Best Response: [[letter]]\".  Include the brackets, so \"Best Response: [[A]]\" if assistant A is better, \"Best Response: [[B]]\" if assistant B is better.\n> \n> \\<userInput\\>\n> \n> Make the bars side-by-side rather than stacked on top of each other:\n> ```r\n> ggplot(mtcars) + aes(x = cyl, fill = factor(vs)) + geom_bar()\n> ```\n> \n> \\</userInput\\>\n> \n>\n> \\<targetResponse\\>\n>\n> ```r\n> ggplot(mtcars) + \n>    aes(x = cyl, fill = factor(vs)) + \n>    geom_bar(position = \"dodge\")\n> ```\n> \n> \\</targetResponse\\>\n> \n> \\<assistantResponseA\\>\n> \n> ```r\n> ggplot(mtcars) + \n>   aes(x = cyl, fill = factor(vs)) + \n>   geom_bar(position = position_dodge())\n> ```\n> \n> \\</assistantResponseA\\>\n> \n> \\<assistantResponseB\\>\n> \n> ```r\n> ggplot(mtcars) + \n>   aes(x = cyl, fill = factor(vs)) + \n>   geom_bar(position = \"dodge\") +\n>   scale_fill_brewer(palette = \"Set2\") +\n>   labs(\n>     x = \"Number of Cylinders\",\n>     y = \"Count\",\n>     fill = \"Engine Type\"\n>   ) +\n>   theme_minimal()\n> ```\n> \n> \\</assistantResponseB\\>\n\n:::callout-warning\nI've added some code fences inside the XML tags in this example prompt just for readability, but they wouldn't actually appear in the prompt (so that the models could penalize models that include code fences despite being asked not to).\n:::\n\nNote that, in this prompt, I don't note which models each response comes from, instead referring to them as \"Response A\" and \"Response B.\" This is intended to guard against the most blatant \"That result is labeled Claude and I am Claude\" kind of [self-enhancement bias](#sec-self-enhancement-bias).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/ggplot2_evals.rda\")\n\nglimpse(ggplot2_evals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 882\nColumns: 9\n$ config_a       <chr> \"OpenAI gpt-4o\", \"OpenAI gpt-4o\", \"OpenAI gpt…\n$ config_b       <chr> \"Qwen2.5 14b\", \"Qwen2.5 14b\", \"Qwen2.5 14b\", …\n$ judge          <chr> \"Claude Sonnet 3.5\", \"OpenAI gpt-4o\", \"Qwen2.…\n$ choice         <chr> \"OpenAI gpt-4o\", \"OpenAI gpt-4o\", \"OpenAI gpt…\n$ input          <chr> \"Add a square in each box to show the mean:\\n…\n$ target         <chr> \"ggplot(mtcars, aes(x = factor(cyl), y = mpg)…\n$ response_a     <chr> \"ggplot(mtcars, aes(x = factor(cyl), y = mpg)…\n$ response_b     <chr> \"ggplot(mtcars, aes(x = factor(cyl), y = mpg)…\n$ response_judge <chr> \"Best Response: [[A]]\", \"Best Response: [[A]]…\n```\n\n\n:::\n:::\n\n\n\nEach row in this data represents one paired comparison on an LLM eval task centered on writing, revising, and debugging ggplot2 code. Column-wise:\n\n* `config_a` and `config_b` are descriptions of the models used to generate a response\n* `judge` is the model used to judge the responses from `config_a` and `config_b`\n* `choice` is the value of `config_a` or `config_b` chosen by the judge, or `NA` if it didn't make a decision.\n* `input` is the user request, `target` is the \"ideal\" output, and `response_*` variables represent the raw output from each model.\n\nI let three models take a go at the eval and gave the same three each a turn at being the judge:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(unique(ggplot2_evals$config_a))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Claude Sonnet 3.5\" \"OpenAI gpt-4o\"     \"Qwen2.5 14b\"      \n```\n\n\n:::\n\n```{.r .cell-code}\nsort(unique(ggplot2_evals$judge))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Claude Sonnet 3.5\" \"OpenAI gpt-4o\"     \"Qwen2.5 14b\"      \n```\n\n\n:::\n:::\n\n\n\n## Position bias\n\nLLMs tend to exhibit position bias when making paired comparisons, where they tend to prefer the first response presented to them over the second (or vice versa) despite response quality [@wang2023largelanguagemodelsfair]. For the purposes of this experiment, I just assign one model to be `config_a` and the other `config_b` randomly, though there's some research out there proposing better ways to address this issue. We can see though that each of these three models seem to demonstrate substantial position bias:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2_evals %>%\n  mutate(\n    choice_position = if_else(choice == config_a, \"First\", \"Second\")\n  ) %>%\n  ggplot() +\n  aes(x = judge, fill = choice_position) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Judge\", y = \"Proportion\", fill = \"Position\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-position-bias-1.png){width=100%}\n:::\n:::\n\n\n\n:::callout-note\nA more rigorous way to quantify this would be position consistency, where we run the comparison with both orders and see how often the model prefers the same response either way [@gu2025surveyllmasajudge].\n:::\n\n## Verbosity bias\n\nAnother bias that many LLMs seem to demonstrate is verbosity bias, where longer responses are preferred over shorter ones [@ye2024justiceprejudicequantifyingbiases]. One way we could measure this is how often in this data models choose the response that has more characters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2_evals %>%\n  mutate(\n    chose_a = choice == config_a,\n    a_longer_than_b = nchar(response_a) > nchar(response_b),\n    choice_length = \n      if_else(\n        (a_longer_than_b & chose_a) | (!a_longer_than_b & !chose_a),\n        \"Longer\",\n        \"Shorter\"\n      )\n  ) %>%\n  ggplot() +\n  geom_bar(position = \"fill\") +\n  aes(x = judge, fill = choice_length) +\n  labs(x = \"Judge\", y = \"Proportion\", fill = \"Preferred Length\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-verbosity-bias-1.png){width=100%}\n:::\n:::\n\n\n\nAt least by this simplistic metric, verbosity bias in the \"longer is better\" direction doesn't seem to be the case.^[@ye2024justiceprejudicequantifyingbiases use a much more nuanced analysis method here.] At the same time, the models are definitely preferring the shorter responses over the longer ones in this data, though this may just be the (desired) effect of models choosing the most minimal solution to the problem.\n\n## Self-enhancement bias {#sec-self-enhancement-bias}\n\nAnother interesting effect is self-enhancement bias, where models are likely to prefer their own answer over one supplied by another model, even when they don’t know where a given response arises from [@ye2024justiceprejudicequantifyingbiases]. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2_evals %>%\n  rowwise() %>%\n  mutate(judged_itself = judge %in% c(config_a, config_b)) %>%\n  filter(judged_itself) %>%\n  mutate(\n    choice_self = if_else(judge == choice, \"Itself\", \"Other Model\")\n  ) %>%\n  ggplot() +\n  aes(x = judge, fill = choice_self) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Judge\", y = \"Proportion\", fill = \"Choice\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-self-enhancement-bias-1.png){width=100%}\n:::\n:::\n\n\n\nIn this case, we really don't see that at all. It may be the case in this example that, given there are generally a limited set of ways to get to the \"right\" answer in this eval, models don't have much of an opportunity to display their \"taste,\" which is presumably the thing they're exhibiting when they display this bias.\n\n## Okay, but who won?\n\nThis data really isn't good enough to state we've done anything quite resembling an \"eval,\" but we can at least check out which model gave the preferred response most often:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2_evals %>%\n  ggplot() +\n  aes(x = choice) +\n  geom_bar() +\n  labs(x = \"Judge\", y = \"# Times Response Was Preferred\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-winner-1.png){width=100%}\n:::\n:::\n\n\n\nOkay, Qwen!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}