{
  "hash": "21e77387e609cccf3f6d429b2d04e35f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"When plotting, LLMs see what they expect to see\"\ndate: '2025-11-26'\ntags:\n  - ai\n  - rstats\nsubtitle: \"Data science agents need to accurately read plots even when the content contradicts their expectations.\"\nsummary: ''\nimage: ../../assets/blank.png\n---\n\n> _This post is a cross-post of a [post](https://posit.co/blog/introducing-bluffbench/) on the Posit Blog, co-written with Sara Altman._\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n`mtcars` is a much-loved dataset from base R, giving information on various attributes of several car models. Let's make a quick plot showing the relationship between horsepower and fuel efficiency:\n\n<!--\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 × 12\n   model         mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Mazda RX4    21       6  160    225  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    225  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108    242  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    225  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    160  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    230  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360     90  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.   273  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.   240  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   212  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n```\n\n\n:::\n:::\n\n\n-->\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-mtcars-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAs expected, we see that as the horsepower increases, the fuel efficiency increases; a moderately strong, positive correlation.\n\nWait... did you catch that? Read that again.\n\nThat doesn't seem right. As the horsepower of cars increases, we would expect their corresponding fuel efficiency values to _decrease_. Yet, this plot clearly shows that the opposite seems to be the case. That's because, in a code chunk we didn't show you, we applied this secret transformation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars$hp <- max(mtcars$hp) - mtcars$hp\n```\n:::\n\n\nThis code replaces the true horsepower with a transformation of the horsepower that subtracts the car's actual horsepower from the maximum value of the dataset. As such, any relationships between this variable and other variables in the dataset are \"flipped\" from their usual values.\n\n## Tricking LLMs\n\nNow, if LLMs write some code to plot this data and were shown the results, would they \"see\" this contradiction? We assembled 11 samples like this one, in which we fundamentally changed the underlying data in classic, known datasets like `mtcars`, `diamonds`, and `iris`. Then, we ran them through three frontier LLMs, repeating each sample three times. \n\nHere are some example excerpts from flagship models reacting to this `mtcars` plot.\n\n> **Claude Sonnet 4.5**: _Here's what I observe from this plot: **Clear Negative Relationship:** There's a strong negative correlation between horsepower and fuel efficiency. As horsepower increases, miles per gallon decreases..._\n>\n> **GPT-5**: _Here’s what stands out from the plot: Clear negative association: as horsepower increases, miles per gallon generally decreases._\n>\n> **Gemini 2.5 Pro**: _...As `hp` increases, `mpg` tends to decrease. This indicates a negative correlation or inverse relationship._\n\nTo a surprising extent, **LLMs do not catch tricks like this one**, where we make secret transformations to known datasets and then ask the models to plot the data and tell us what they see. Instead, **when interpreting plots, LLMs appear to see what they expect to see.**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-bluff-eval-mocked-1.png){fig-align='center' fig-alt='A horizontal bar chart comparing AI models\\' performance on bluffbench. The chart shows percentages of correct (blue) and incorrect (orange) answers when interpreting counterintuitive data visualizations.' width=768}\n:::\n:::\n\n\nWe discovered this phenomenon when working on coding agents like [Databot](https://positron.posit.co/databot.html) and [Positron Assistant](https://positron.posit.co/assistant.html), where generating and interpreting plots is a core capability. These agents assist data scientists by running Python and R code and looking at the results. What does it mean if those agents can't truly \"see\" these plots to the extent that they'd look further if some plotted trend disagreed with their expectations?\n\n## In practice\n\nIn reality, it's not often the case that data scientists are secretly modifying base datasets to try and trick these models in the way that we do above. So, we also tested these models in more subtle situations, where we name datasets and variables in a way that suggests some intuitive relationship will likely be observed, but the actual relationship present in the data shows the opposite (or shows no relationship at all).\n\nFor example, we can generate some data weekly study time and exam scores that looks like this:\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudents\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 75 × 2\n   study_hours_weekly exam_score\n                <dbl>      <dbl>\n 1              20.2        85.6\n 2               8.31       62.1\n 3               8.09       60.1\n 4              24.4        92.9\n 5              32.1        63.9\n 6              29.1        60.6\n 7              23.9        89.7\n 8              34.2        61.7\n 9              14.8        58.8\n10              28.4        64.3\n# ℹ 65 more rows\n```\n\n\n:::\n:::\n\n\nBoth humans and LLMs likely expect study time to be correlated with exam time, but this dataset shows essentially no correlation between these two variables and a large discontinuity in one range of the data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(students, aes(x = study_hours_weekly, y = exam_score)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-students-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIf either of us came across such a plot as a data scientist, this would definitely raise an eyebrow. What's going on with those observations in the 20-25 bucket? Is this a data quality issue? Was there some sort of assisted study program that happened to run 20-25 hours a week? **This is the capability we're most concerned about** as we develop these agents; do Databot and Positron Assistant see these subtle anomalies that go against their expectations and investigate further? We assembled 13 samples like this one, running them through the same models, three times each. Here are some excerpts from this sample's responses:\n\n> **Claude Sonnet 4.5**: _...Interesting outliers: There's a cluster of high performers (scores 90-100) who study around 20-25 hours per week... While more study time is associated with slightly better scores on average, it's clearly not the only factor determining exam performance._\n>\n> **GPT-5**: _Possible subgroups/outliers: A distinct cluster around ~20–25 hours with very high scores (~90–100)._\n>\n> **Gemini 2.5 Pro**: _...The group that studied around 20-25 hours a week did significantly better than those who studied even more. This suggests that simply studying more doesn't always lead to a better score, and there might be other factors at play._\n\nThe results from this set of experiments are certainly more promising:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-bluff-eval-intuitive-1.png){fig-align='center' fig-alt='A horizontal bar chart comparing AI models\\' performance on bluffbench. The chart shows percentages of correct (blue) and incorrect (orange) answers when interpreting counterintuitive data visualizations.' width=768}\n:::\n:::\n\n\nThat said, we still don't find these percentages encouraging; a typical data scientist would not have only a 50/50 chance at spotting that discontinuity. This is partially why [Databot's interface](https://posit.co/blog/databot-is-not-a-flotation-device/) shows intermediate results to the user and yields control to them regularly. Working with LLMs to carry out data science still requires human skepticism and curiosity, especially as long as LLMs are still prone to these types of mistakes. \n\n## Looking forward\n\nWe're actively working on systems that result in more faithful interpretations of plots. The obvious tricks—changes to system prompts or tool descriptions, \"models-in-the-middle\" that have some subset of the real context, etc.—haven't made much of a difference. Once we've built systems that result in improvements here, those changes will make their way to our agents and we'll share more here.\n\nIf you want to learn more about these experiments, check out [bluffbench's homepage](https://simonpcouch.github.io/bluffbench/). bluffbench is an LLM evaluation implemented in R with [ellmer](https://ellmer.tidyverse.org/) and [vitals](http://vitals.tidyverse.org/). In addition to a write-up similar to the one here, the [evaluation logs](https://simonpcouch.github.io/bluffbench/logs/) are also available on the evaluation's [website](https://simonpcouch.github.io/bluffbench/), allowing you to explore the full transcripts of each LLM's response and the corresponding grading applied to each submission.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}