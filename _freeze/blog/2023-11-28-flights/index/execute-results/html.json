{
  "hash": "9f2c84fd3fd47cdfc572096606466610",
  "result": {
    "markdown": "---\ntitle: \"Predicting flight delays with tidymodelsðŸ›©\"\ndate: '2023-11-28'\ntags:\n  - rstats\n  - tidymodels\n  - tune\n  - workflowsets\nsubtitle: \"Trying out a variety of machine learning models to predict flight delays out of Madison, WI.\"\nimage: featured.png\nsummary: ''\n---\n\n\n\n\n\n> *Last week, I virtually dropped by University of Wisconsin-Madison for a webinar on tidymodels. Heading into the holidays, I thought a fun example problem might be to try and predict flight delays using flights data from Madison's airport. This is a very, very difficult modeling problem, and the results aren't very impressive, but it's a fun one nonetheless.*\n\nI've collected some data on all of the outbound flights from Madison, Wisconsin in 2022. In this blog post, we'll use predictors based on the weather, plane, airline, and flight duration to try to predict whether a flight will be delayed or not. To do so, we will split up the source data and then train models in two stages:\n\n[**Round 1)**](#round1) Try out a variety of models, from a logistic regression to a boosted tree to a neural network, using a grid search for each.\n\n[**Round 2)**](#round2) Try out more advanced search techniques for the model that seems the most performant in Round 1). \n\nOnce we've trained the [final model fit](#final-fit), we can assess the predictive performance on the test set and prepare the model to be [deployed](#deploy).\n\n## Setup\n\nFirst, loading the [tidyverse](https://www.tidyverse.org/) and [tidymodels](https://www.tidymodels.org/), along with a few additional tidymodels extension packages:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# for data analysis:\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# for modeling:\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(bonsai)\nlibrary(baguette)\n```\n:::\n\n\nThe [finetune package](https://finetune.tidymodels.org/) will give us additional tuning functionality, while the bonsai and baguette packages provide support for additional model types.\n\ntidymodels supports a number of R frameworks for [parallel computing](https://tune.tidymodels.org/articles/extras/optimizations.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# loading needed packages:\nlibrary(doMC)\nlibrary(parallelly)\n\n# check out how many cores we have:\navailableCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsystem \n    10 \n```\n:::\n\n```{.r .cell-code}\n# register those cores so that tidymodels can see them:\nregisterDoMC(cores = max(1, availableCores() - 1))\n```\n:::\n\n\nWith a multi-core setup registered, tidymodels will now make use of all of the cores on my computer for expensive computations.\n\n## Data Import\n\nWe'll make use of a dataset, msnflights22, containing data on all outbound flights from Madison, Wisconsin in 2022.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(\"data/msnflights22.rda\")\n\nmsnflights22\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,754 Ã— 14\n   delayed airline     flight origin destination date        hour plane distance\n   <fct>   <chr>       <fct>  <fct>  <fct>       <date>     <dbl> <fct>    <dbl>\n 1 No      Endeavor Aâ€¦ 51     MSN    ATL         2022-01-01     5 N901â€¦      707\n 2 No      PSA Airlinâ€¦ 59     MSN    CLT         2022-01-01     6 N570â€¦      708\n 3 No      Envoy Air   27     MSN    MIA         2022-01-01     6 N280â€¦     1300\n 4 No      American Aâ€¦ 3      MSN    PHX         2022-01-01     6 N662â€¦     1396\n 5 Yes     American Aâ€¦ 16     MSN    DFW         2022-01-01     7 N826â€¦      821\n 6 No      SkyWest Aiâ€¦ 26     MSN    MSP         2022-01-01     7 N282â€¦      228\n 7 No      United Airâ€¦ 1      MSN    EWR         2022-01-01     7 <NA>       799\n 8 No      United Airâ€¦ 6      MSN    DEN         2022-01-01     8 N830â€¦      826\n 9 No      Republic Aâ€¦ 29     MSN    ORD         2022-01-01     8 N654â€¦      109\n10 No      Endeavor Aâ€¦ 62     MSN    DTW         2022-01-01     9 N582â€¦      311\n# â„¹ 10,744 more rows\n# â„¹ 5 more variables: duration <dbl>, wind_speed <dbl>, precip <dbl>,\n#   visibility <dbl>, plane_year <int>\n```\n:::\n:::\n\n\n::: callout-note\nYou can make your own flights data using the [anyflights](https://simonpcouch.github.io/anyflights/) package! The [`query_data.R`](https://github.com/simonpcouch/tidymodels-uw-2023/blob/main/example/query_data.R) file contains the code used to generate this dataset.\n:::\n\nWe'd like to model `delayed`, a binary outcome variable giving whether a given flight was delayed by 10 or more minutes.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# summarize counts of the outcome variable\nggplot(msnflights22) +\n  aes(x = delayed) +\n  geom_bar(fill = \"#4A7862\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/count-delayed-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nPredicting flight delays seems quite difficult given the data we have access to. For example, plotting whether a flight is delayed based on precipitation and wind speed:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot 2 predictors, colored by the outcome\nmsnflights22 %>%\n  filter(precip != 0) %>%\n  ggplot() +\n  aes(x = wind_speed, y = precip, color = delayed) +\n  geom_jitter()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-predictors-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nLooks like there's a _bit_ of signal in the time of the day of the flight, but those higher-proportion-delayed hours also have quite a bit fewer flights (and thus more variation):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(\n  ggplot(msnflights22, aes(x = hour, fill = delayed)) +\n  geom_bar()\n) /\n(\n  ggplot(msnflights22, aes(x = hour, fill = delayed)) +\n  geom_bar(position = \"fill\") + labs(y = \"proportion\")\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-predictors-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nA machine learning model may be able to get some traction here, though.\n\n## Splitting up data\n\nWe split data into training and testing sets so that, once we've trained our final model, we can get an honest assessment of the model's performance. Since this data is a time series, we'll allot the first ~10 months to training and the remainder to testing:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# set the seed for random number generation\nset.seed(1)\n\n# split the flights data into...\nflights_split <- initial_time_split(msnflights22, prop = 5/6)\n# training [jan - oct]\nflights_train <- training(flights_split)\n# ...and testing  [nov - dec]\nflights_test <- testing(flights_split)\n```\n:::\n\n\nThen, we'll resample the training data using a sliding period. \n\n::: callout-note\nA [sliding period](https://rsample.tidymodels.org/reference/slide-resampling) is a cross-fold validation technique that takes times into account. The tidymodels packages support more basic resampling schemes like bootstrapping and v-fold cross-validation as well---see [the rsample package's website](https://rsample.tidymodels.org/).\n:::\n\nWe create 8 folds, where in each fold the analysis set is a 2-month sample of data and the assessment set is the single month following.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nflights_folds <- \n  sliding_period(\n    flights_train, \n    index = date, \n    \"month\", \n    lookback = 1, \n    assess_stop = 1\n  )\n\nflights_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Sliding period resampling \n# A tibble: 8 Ã— 2\n  splits             id    \n  <list>             <chr> \n1 <split [1783/988]> Slice1\n2 <split [1833/819]> Slice2\n3 <split [1807/930]> Slice3\n4 <split [1749/901]> Slice4\n5 <split [1831/906]> Slice5\n6 <split [1807/920]> Slice6\n7 <split [1826/888]> Slice7\n8 <split [1808/826]> Slice8\n```\n:::\n:::\n\n\nFor example, in the second split,\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# training: february, march, april\ntraining(flights_folds$splits[[2]]) %>% pull(date) %>% month() %>% unique()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 3\n```\n:::\n\n```{.r .cell-code}\n# testing: may\ntesting(flights_folds$splits[[2]])  %>% pull(date) %>% month() %>% unique()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n:::\n\n\n::: callout-note\nWhat months will be in the training and testing sets in the third fold?\n:::\n\n## Defining our modeling strategies\n\nOur basic strategy is to first try out a bunch of different modeling approaches, and once we have an initial sense for how they perform, delve further into the one that looks the most promising.\n\nWe first define a few [recipes](https://recipes.tidymodels.org/), which specify how to process the inputted data in such a way that machine learning models will know how to work with predictors:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe_basic <-\n  recipe(delayed ~ ., flights_train) %>%\n  step_nzv(all_predictors())\n\nrecipe_normalize <-\n  recipe_basic %>%\n  step_YeoJohnson(all_double_predictors()) %>%\n  step_normalize(all_double_predictors())\n\nrecipe_pca <- \n  recipe_normalize %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_impute_mode(all_nominal_predictors()) %>%\n  step_pca(all_numeric_predictors(), num_comp = tune())\n```\n:::\n\n\nThese recipes vary in complexity, from basic checks on the input data to advanced feature engineering techniques like principal component analysis.\n\n::: callout-note\nThese preprocessors make use of predictors based on weather. Given that prediction models are only well-defined when trained using variables that are [available at prediction time](https://link.springer.com/article/10.1007/s40520-023-02560-2), in what use cases would this model be useful?\n:::\n\nWe also define several [model specifications](https://parsnip.tidymodels.org/). tidymodels comes with support for all sorts of machine learning algorithms, from neural networks to LightGBM boosted trees to plain old logistic regression:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspec_lr <-\n  logistic_reg() %>%\n  set_mode(\"classification\")\n\nspec_bm <- \n  bag_mars(num_terms = tune(), prod_degree = tune()) %>%\n  set_engine(\"earth\") %>% \n  set_mode(\"classification\")\n\nspec_bt <- \n  bag_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\nspec_nn <- \n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%\n  set_engine(\"nnet\", MaxNWts = 15000) %>%\n  set_mode(\"classification\")\n\nspec_svm <- \n  svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %>%\n  set_mode(\"classification\")\n\nspec_lgb <-\n  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(),\n             learn_rate = tune(), stop_iter = 10) %>%\n  set_engine(\"lightgbm\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\nNote how similar the code for each of these model specifications looks! tidymodels takes care of the \"translation\" from our unified syntax to the code that these algorithms expect.\n\nIf typing all of these out seems cumbersome to you, or you're not sure how to define a model specification that makes sense for your data, the [usemodels](https://usemodels.tidymodels.org/) RStudio addin may help!\n\n## Evaluating models: round 1 {#round1}\n\nWe'll pair machine learning models with the recipes that make the most sense for them using [workflow sets](https://workflowsets.tidymodels.org/):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwf_set <-\n  # pair the basic recipe with a boosted tree and logistic regression\n  workflow_set(\n    preproc = list(basic = recipe_basic),\n    models = list(boost_tree = spec_lgb, logistic_reg = spec_lr)\n  ) %>%\n  # pair the recipe that centers and scales input variables\n  # with the bagged models, support vector machine, and neural network\n  bind_rows(\n    workflow_set(\n      preproc = list(normalize = recipe_normalize),\n      models = list(\n        bag_tree = spec_bt,\n        bag_mars = spec_bm,\n        svm_rbf = spec_svm,\n        mlp = spec_nn\n      )\n    )\n  ) %>%\n  # pair those same models with a more involved, principal component\n  # analysis preprocessor\n  bind_rows(\n    workflow_set(\n      preproc = list(pca = recipe_pca),\n      models = list(\n        bag_tree = spec_bt,\n        bag_mars = spec_bm,\n        svm_rbf = spec_svm,\n        mlp = spec_nn\n      )\n    )\n  )\n\nwf_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A workflow set/tibble: 10 Ã— 4\n   wflow_id           info             option    result    \n   <chr>              <list>           <list>    <list>    \n 1 basic_boost_tree   <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 2 basic_logistic_reg <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 3 normalize_bag_tree <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 4 normalize_bag_mars <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 5 normalize_svm_rbf  <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 6 normalize_mlp      <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 7 pca_bag_tree       <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 8 pca_bag_mars       <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n 9 pca_svm_rbf        <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n10 pca_mlp            <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n```\n:::\n:::\n\n\nNow that we've defined each of our modeling configurations, we'll fit them to the resamples we defined earlier. Here, `tune_grid()` is applied to each workflow in the workflow set, testing out a set of tuning parameter values for each workflow and assessing the resulting fit.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwf_set_fit <-\n  workflow_map(\n    wf_set, \n    fn = \"tune_grid\", \n    verbose = TRUE, \n    seed = 1,\n    resamples = flights_folds,\n    control = control_grid(parallel_over = \"everything\")\n  )\n```\n:::\n\n\n::: callout-note\n`workflow_map()` is calling `tune_grid()` on each modeling workflow we've created. You can read more about `tune_grid()` [here](https://www.tidymodels.org/start/tuning/).\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwf_set_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A workflow set/tibble: 8 Ã— 4\n  wflow_id           info             option    result   \n  <chr>              <list>           <list>    <list>   \n1 basic_boost_tree   <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n2 normalize_bag_tree <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n3 normalize_bag_mars <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n4 normalize_mlp      <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n5 pca_bag_tree       <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n6 pca_bag_mars       <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n7 pca_svm_rbf        <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n8 pca_mlp            <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n```\n:::\n:::\n\n\nNote that the `result` column that was previously empty in `wf_set` now contains various tuning results, denoted by `<tune[+]>`, in `wf_set_fit`.\n\n::: callout-note\nThere's a few rows missing here; I filtered out results that failed to tune.\n:::\n\nCollecting the information on performance from the resulting object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# first look at metrics:\nmetrics_wf_set <- collect_metrics(wf_set_fit)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# extract the top roc_auc values\nmetrics_wf_set %>%\n  filter(.metric == \"roc_auc\") %>%\n  arrange(desc(mean)) %>%\n  select(wflow_id, mean, n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 71 Ã— 3\n   wflow_id            mean     n\n   <chr>              <dbl> <int>\n 1 normalize_bag_mars 0.615     8\n 2 normalize_bag_mars 0.613     8\n 3 pca_svm_rbf        0.608     8\n 4 normalize_bag_mars 0.606     8\n 5 pca_bag_mars       0.604     8\n 6 normalize_bag_mars 0.603     8\n 7 pca_svm_rbf        0.603     8\n 8 pca_bag_mars       0.601     8\n 9 pca_bag_mars       0.598     8\n10 normalize_bag_mars 0.598     8\n# â„¹ 61 more rows\n```\n:::\n:::\n\n\n::: callout-note\nWe use one of the default metrics, [`roc_auc()`](https://yardstick.tidymodels.org/reference/roc_auc.html), to evaluate our models here. Any metric defined using the [yardstick](https://yardstick.tidymodels.org/) package is fair game here (including custom metrics)!\n:::\n\nAlternatively, we can use the `autoplot()` method for workflow sets to visualize the same results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(wf_set_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/autoplot-wf-set-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIn terms of `accuracy()`, several of the models we evaluated performed quite well (with values near the event rate). With respect to `roc_auc()`, though, we can see that the [bagged MARS models](https://parsnip.tidymodels.org/reference/bag_mars.html) were clear winners.\n\n## Evaluating models: round 2 {#round2}\n\nIt looks like a bagged MARS model with centered and scaled predictors was considerably more performant than the other proposed models. Let's work with those MARS results and see if we can make any further improvements to performance:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_res <- extract_workflow_set_result(wf_set_fit, \"normalize_bag_mars\")\n\nmars_wflow <-\n  workflow() %>%\n  add_recipe(recipe_normalize) %>%\n  add_model(spec_bm)\n\nmars_sim_anneal_fit <-\n  tune_sim_anneal(\n    object = mars_wflow,\n    resamples = flights_folds,\n    iter = 10,\n    initial = mars_res,\n    control = control_sim_anneal(verbose = TRUE, parallel_over = \"everything\")\n  )\n```\n:::\n\n\nLooks like we *did* make a small improvement, though the model still doesn't do much better than randomly guessing:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(mars_sim_anneal_fit) %>%\n  filter(.metric == \"roc_auc\") %>%\n  arrange(desc(mean))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 8\n   num_terms prod_degree .metric .estimator .config  mean     n std_err\n       <int>       <int> <chr>   <chr>      <chr>   <dbl> <int>   <dbl>\n 1         4           2 roc_auc binary     Iter1   0.619     8  0.0191\n 2         3           2 roc_auc binary     Iter6   0.612     8  0.0249\n 3         5           2 roc_auc binary     Iter8   0.610     8  0.0185\n 4         4           1 roc_auc binary     Iter9   0.606     8  0.0154\n 5         3           2 roc_auc binary     Iter4   0.605     8  0.0211\n 6         4           1 roc_auc binary     Iter7   0.595     8  0.0205\n 7         3           2 roc_auc binary     Iter10  0.594     8  0.0192\n 8         3           1 roc_auc binary     Iter2   0.591     8  0.0248\n 9         2           1 roc_auc binary     Iter5   0.559     8  0.0183\n10         2           2 roc_auc binary     Iter3   0.555     8  0.0175\n```\n:::\n:::\n\n\nJust like the workflow set result from before, the simulated annealing result also has an `autoplot()` method:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(mars_sim_anneal_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-sim-anneal-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can now train the model with the most optimal performance in cross-validation on the entire training set.\n\n## The final model fit {#final-fit}\n\nThe `last_fit()` function will take care of fitting the most performant model specification to the whole training dataset and evaluating it's performance with the test set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmars_final_fit <-\n  mars_sim_anneal_fit %>%\n  # extract the best hyperparameter configuration\n  select_best(\"roc_auc\") %>%\n  # attach it to the general workflow\n  finalize_workflow(mars_wflow, .) %>%\n  # evaluate the final workflow on the train/test split\n  last_fit(flights_split)\n\nmars_final_fit\n```\n:::\n\n\nThe test set `roc_auc()` for this model was 0.625. The final fitted workflow can be extracted from `mars_final_fit` and is ready to predict on new data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfinal_fit <- extract_workflow(mars_final_fit)\n```\n:::\n\n\n## Deploying to Connect {#deploy}\n\nFrom here, all we'd need to do to deploy our fitted model is pass it off to vetiver for deployment to Posit Connect:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfinal_fit_vetiver <- vetiver_model(final_fit, \"simon.couch/flights\")\n\nboard <- board_connect()\n\nvetiver_pin_write(board, final_fit_vetiver)\n\nvetiver_deploy_rsconnect(board, \"simon.couch/flights\")\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}